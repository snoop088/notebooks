{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0798a7-405a-46cb-a4dd-629588a097dc",
   "metadata": {},
   "source": [
    "# Gradio Experiment. Ollama ChatBot.\n",
    "\n",
    "Lets create a ChatBot using Ollama and Gradio. We can create a dropdown to choose among few Ollama models. \n",
    "\n",
    "The purpose is to get a local ChatBot to test for coding assistance.\n",
    "\n",
    "Let's install/ update gradio first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8f4da8-8613-4a3d-b733-3ae12b9728ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /opt/conda/lib/python3.12/site-packages (5.20.0)\n",
      "Collecting gradio\n",
      "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.6.2.post1)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.115.5)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.7.2 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.7.2)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.27.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.10.11)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from gradio) (24.1)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (11.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.9.6)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.41.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.13.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.32.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.7.2->gradio) (2024.9.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /opt/conda/lib/python3.12/site-packages (from gradio-client==1.7.2->gradio) (14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (3.16.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.2.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: gradio\n",
      "  Attempting uninstall: gradio\n",
      "    Found existing installation: gradio 5.20.0\n",
      "    Uninstalling gradio-5.20.0:\n",
      "      Successfully uninstalled gradio-5.20.0\n",
      "Successfully installed gradio-5.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ollama in /opt/conda/lib/python3.12/site-packages (0.4.7)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in /opt/conda/lib/python3.12/site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /opt/conda/lib/python3.12/site-packages (from ollama) (2.9.2)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.0.6)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->ollama) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U gradio\n",
    "%pip install -U ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1239779-8906-4d96-aa0b-8abab926f4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from ollama import chat\n",
    "from ollama import Client\n",
    "import gradio as gr\n",
    "from typing import Iterator, Optional, Any\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed16b7b-beb3-44ed-a596-0596f6901138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the custom client to target ollama docker service\n",
    "client = Client(\n",
    "  host='ollama',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93aba1d-4eaf-488a-bcd0-af82973f5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LangChain\n",
    "llm = ChatOllama(\n",
    "    model=\"llama3.1:8b-instruct-fp16\",\n",
    "    temperature=0.12,\n",
    "    base_url=\"ollama\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bd33b9e-4fce-4e48-b1e4-24b62279fc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* Running on public URL: https://aa2c687c02ec85dd91.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://aa2c687c02ec85dd91.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.load_chat(\"http://ollama:11434/v1/\", model=\"llama3.1:8b-instruct-fp16\", token=\"ollama\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a90e936-4eed-4e94-bc0a-b8b53d2e59c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing examples where possible.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d502a6f-f5bc-4f45-96c5-9ca352fcee05",
   "metadata": {},
   "source": [
    "### Lets Create a Class\n",
    "\n",
    "Here is some description of what it should do:\n",
    "\n",
    "- Takes in the constructor:\n",
    "    1. model name out of Literal of supported names that have been loaded in ollama\n",
    "    2. system_message\n",
    "    3. temperature\n",
    "    4. top_p\n",
    "- exposes get_stream function that takes in messages and returns the stream from the model defined above\n",
    "- parameters should be obtained from gradio chat interface with few more inputs.\n",
    "- model is a dropdown to limit what can be used\n",
    "- the rest of the inputs are text fields\n",
    "- potentially extend it with tool use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3d27ed-b2d5-4c76-b0b3-d0e6300a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal, List, TypedDict\n",
    "from ollama import Client\n",
    "\n",
    "class ChatWrapper:\n",
    "    client = Client(\n",
    "      host='ollama',\n",
    "    )\n",
    "    def __init__(self, \n",
    "                 model: Literal['llama3.2:latest', 'qwen2.5-coder:32b', 'qwen2.5:14b', 'llama3.1:8b-instruct-fp16', 'deepseek-r1:32b'],\n",
    "                 system_message: str,\n",
    "                 temperature: float = 0.25,\n",
    "                 top_p: float = 0.8\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.system_message = [{\"role\": \"system\", \"content\": system_message}]\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "\n",
    "    def get_stream(self, messages: List[TypedDict]):\n",
    "        return ChatWrapper.client.chat(\n",
    "            model=self.model,\n",
    "            messages=self.system_message + messages,\n",
    "            stream=True,\n",
    "            options={\"temperature\": self.temperature,\n",
    "                    \"top_p\": self.top_p\n",
    "                    }\n",
    "        )\n",
    "\n",
    "    def get_response(self, messages: List[TypedDict]) -> str:\n",
    "        return ChatWrapper.client.chat(\n",
    "            model=self.model,\n",
    "            messages=self.system_message + messages,\n",
    "            stream=False,\n",
    "            options={\"temperature\": self.temperature,\n",
    "                    \"top_p\": self.top_p\n",
    "                    }\n",
    "        )\n",
    "    def get_with_tools(self, messages: List[TypedDict], tools: List) -> Iterator[dict]:\n",
    " \n",
    "        return ChatWrapper.client.chat(\n",
    "            model=self.model,\n",
    "            messages=self.system_message + messages,\n",
    "            stream=False,\n",
    "            options={\"temperature\": self.temperature,\n",
    "                    \"top_p\": self.top_p\n",
    "                    },\n",
    "            tools=tools\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52934030-a7b6-4330-82ff-46209e8c9e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(query: str) -> Optional[list[dict[str, Any]]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Query search engine.\n",
    "    This function queirs the web to fetch comprehensive, accurate and trusted results. It's useful\n",
    "    for answering questions about current events.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The query to search\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of the results\n",
    "    \"\"\"\n",
    "    if query:\n",
    "        wrapped = TavilySearchResults(max_results=3)\n",
    "        result = wrapped.invoke({\"query\": query})\n",
    "        return result\n",
    "    else:\n",
    "        raise ValueError(\"No query was supplied by the LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "851f731f-8b29-4dc5-b6d9-3922bbcafabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing DeepSeek without Gradio\n",
    "tools_map = {\"web_search\": web_search}\n",
    "wrapper = ChatWrapper(model=\"deepseek-r1:32b\", \n",
    "                     system_message=\"You are a friendly, polite AI assistant that answers questions\",\n",
    "                    temperature=0.15)\n",
    "wrapper_qwen = ChatWrapper(model=\"qwen2.5-coder:32b\", \n",
    "                     system_message=\"You are a friendly, polite AI assistant that answers questions. \\\n",
    "                     User your vast knowledge to answer the questions posed by our human friend. \\\n",
    "                     You have 'web_search' tool at your disposal. Only use the tool for questions that \\\n",
    "                     are concerning current events or you have insufficient knowledge of. \\\n",
    "                     Otherwise answer the question directly.\",\n",
    "                    temperature=0.1)\n",
    "messages = [{\"role\": \"user\", \"content\": \"what was the command to clean up yay cache on an Arch based distro?\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9d59647f-13b8-4f97-a663-4c3bff368955",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = wrapper_qwen.get_with_tools(messages, [web_search])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9e49a26-c81c-4d7c-8c47-cf677d40b615",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 0\n",
    "while response.message.tool_calls and max_iter < 2:\n",
    "    for tool in response.message.tool_calls:\n",
    "        formatted_search_docs = ''\n",
    "        if function_to_call := tools_map.get(tool.function.name):\n",
    "            print('Calling function:', tool.function.name)\n",
    "            \n",
    "            output = function_to_call(**tool.function.arguments)\n",
    "            formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "                [\n",
    "                    f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "                    for doc in output\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "          print('Function', tool.function.name, 'not found')\n",
    "    messages.append({'role': 'tool', 'content': formatted_search_docs, 'name': tool.function.name})\n",
    "    response = wrapper_qwen.get_with_tools(messages, [web_search])\n",
    "    print(response.message.content)\n",
    "    max_iter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7412505e-f131-4228-9fe1-19ec82e039e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To clean up the Yay cache on an Arch-based distribution, you can use the following command:\n",
      "\n",
      "```bash\n",
      "yay -Sc\n",
      "```\n",
      "\n",
      "This command will remove unused packages from the cache. If you want to remove all cached packages, you can use:\n",
      "\n",
      "```bash\n",
      "yay -Scc\n",
      "```\n",
      "\n",
      "Please be cautious with `yay -Scc` as it will clear your entire package cache, which might require re-downloading packages in the future.\n"
     ]
    }
   ],
   "source": [
    "print(response.message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06920a04-7771-4000-a65c-14413fd9b349",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "            for doc in output\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09ad530e-6edf-4862-93f9-b6e738def3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out whether Elrond or Tom Bombadil is older in the Lord of the Rings universe. Hmm, both are characters from J.R.R. Tolkien's works, but they don't interact much, so it's not immediately clear which one is older.\n",
      "\n",
      "First, let me recall what I know about each character. Elrond is a prominent figure in Middle-earth, especially in Rivendell. He was present during the First Age and played a role in significant events like the War of the Ring. Tom Bombadil, on the other hand, is more of a mysterious figure who appears early in The Fellowship of the Ring when Frodo and his companions encounter him in the Old Forest.\n",
      "\n",
      "I think Elrond's age is mentioned somewhere. I remember that he was born during the First Age, specifically around the time of the creation of the Silmarils. That would make him very old by the time of the Third Age when The Lord of the Rings takes place. Tom Bombadil's origins are a bit more unclear. He seems to be an ancient being, perhaps even older than Elrond, but I'm not sure.\n",
      "\n",
      "Wait, in the appendices of The Return of the King, there's information about the ages and timelines of various characters. Let me try to remember that. Elrond was born in the First Age, around 3299 years before the end of the Third Age. Tom Bombadil is described as an \"Eldest,\" which suggests he might be one of the oldest beings in Middle-earth, possibly existing even before the Valar came into the world.\n",
      "\n",
      "So if Elrond was born in the First Age and Tom Bombadil is older than that, then Tom would be older. But I'm not entirely certain because sometimes characters can have different timelines or their origins aren't explicitly stated. Maybe there's a source that directly compares their ages.\n",
      "\n",
      "I should also consider their roles. Elrond is a leader of the Elves and has a significant impact on events throughout Middle-earth's history. Tom Bombadil, while powerful in his own right, seems to have a more localized influence, mainly around the Old Forest. His ancient nature might indicate that he was there long before other characters like Elrond.\n",
      "\n",
      "Another point is that Tom Bombadil is often associated with the land itself, almost as an embodiment of it, which could imply that he's been around since the creation of Middle-earth. If that's the case, then he would indeed be older than Elrond, who was born much later in the First Age.\n",
      "\n",
      "I think I've gathered enough information to conclude that Tom Bombadil is older than Elrond. Their roles and origins suggest that Tom has been around for a longer time, possibly from the very beginning of Middle-earth.\n",
      "</think>\n",
      "\n",
      "Tom Bombadil is older than Elrond. While Elrond was born in the First Age, Tom Bombadil's origins are more ancient, with some sources suggesting he existed even before the Valar arrived in Middle-earth. His association with the land and his role as an \"Eldest\" indicate that he has been present since the creation of Middle-earth, making him older than Elrond."
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in stream:\n",
    "    # chunks.append(chunk[\"message\"][\"content\"])\n",
    "    print( chunk[\"message\"][\"content\"], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d375c028-6c96-4c7b-9500-835f9d239c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_tool_response(response: dict, chat_wrapper: ChatWrapper, tools_map: dict, messages: list):\n",
    "    max_iter = 0\n",
    "    while response.message.tool_calls and max_iter < 2:\n",
    "        for tool in response.message.tool_calls:\n",
    "            formatted_search_docs = ''\n",
    "            if function_to_call := tools_map.get(tool.function.name):\n",
    "                print('Calling function:', tool.function.name)\n",
    "                \n",
    "                output = function_to_call(**tool.function.arguments)\n",
    "                formatted_search_docs = \"\\n\\n---\\n\\n\".join(\n",
    "                    [\n",
    "                        f'<Document href=\"{doc[\"url\"]}\"/>\\n{doc[\"content\"]}\\n</Document>'\n",
    "                        for doc in output\n",
    "                    ]\n",
    "                )\n",
    "                \n",
    "            else:\n",
    "              print('Function', tool.function.name, 'not found')\n",
    "        messages.append({'role': 'tool', 'content': formatted_search_docs, 'name': tool.function.name})\n",
    "        response = chat_wrapper.get_with_tools(messages, [web_search])\n",
    "        print(response.message.content)\n",
    "        max_iter += 1\n",
    "    return response\n",
    "    \n",
    "def chatter(message, history: list, model: str, tools: str, system_message: str, temperature: float):\n",
    "    tools_map = {\"web_search\": web_search}\n",
    "    chat_wrapper = ChatWrapper(model=model, \n",
    "                               system_message=system_message,\n",
    "                               temperature=float(temperature),\n",
    "                               top_p=0.85\n",
    "                              )\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "  \n",
    "    if tools in tools_map:\n",
    "        response = chat_wrapper.get_with_tools(history, [tools_map[tools]])\n",
    "        # lets catch the tool call\n",
    "        if response.message.tool_calls:\n",
    "            print(\"need tools\")\n",
    "            response = resolve_tool_response(response, chat_wrapper, tools_map, history)\n",
    "        \n",
    "        yield response[\"message\"][\"content\"]\n",
    "    else:\n",
    "        stream = chat_wrapper.get_stream(history)\n",
    "        chunks = []\n",
    "        for chunk in stream:\n",
    "            chunks.append(chunk[\"message\"][\"content\"])\n",
    "            yield \"\".join(chunks)\n",
    "    # print(type(response[\"message\"][\"content\"]))\n",
    "    \n",
    "    # yield response[\"message\"][\"content\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f09f9f-1302-4d27-aff6-f00d34958a89",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'system' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msystem\u001b[49m}]\n\u001b[1;32m      2\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      3\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite me a 1000 word story on elves and dwarves who are in alliance against an evil human king.\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# using chat from Ollama with custom client\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# llama3.1:8b-instruct-fp16\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# qwen2.5-coder:32b\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'system' is not defined"
     ]
    }
   ],
   "source": [
    "# messages = [{\"role\": \"system\", \"content\": system}]\n",
    "# messages.append({\"role\": \"user\", \n",
    "#                  \"content\": \"Write me a 1000 word story on elves and dwarves who are in alliance against an evil human king.\"})\n",
    "\n",
    "# # using chat from Ollama with custom client\n",
    "# # llama3.1:8b-instruct-fp16\n",
    "# # qwen2.5-coder:32b\n",
    "# stream = client.chat(\n",
    "#     model='qwen2.5-coder:32b',\n",
    "#     messages=messages,\n",
    "#     stream=True,\n",
    "    \n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858ee296-2037-404f-97fa-084a4daafe47",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing examples where possible.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Who is the oldest on Arda that has a role in the LOTR books?'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41aec86-8abc-4275-aeba-f060ee27b092",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of Eridoria, where the sun dipped into the horizon and painted the sky with hues of crimson and gold, two ancient enemies had put aside their differences to form an unlikely alliance against a common foe: King Malakai, the tyrannical ruler of the human kingdom.\n",
      "\n",
      "For centuries, elves and dwarves had clashed in battle, each side convinced that they were the superior beings. The elves, with their lithe bodies and agile limbs, were skilled archers and assassins, while the dwarves, sturdy and proud, excelled at mining and smithing. Their conflicts had shaped the history of Eridoria, leaving scars that would never fully heal.\n",
      "\n",
      "However, as King Malakai's power grew, his ambition consumed him. He sought to conquer all of Eridoria, enslaving its inhabitants and bending them to his will. His armies marched across the land, leaving destruction in their wake.\n",
      "\n",
      "It was amidst this darkness that the elves and dwarves found common ground. In a secret gathering deep within the heart of the ancient forest of Elvanar, the leaders of both nations met to forge an alliance against King Malakai.\n",
      "\n",
      "Ariniel, the elven queen, stood tall, her piercing emerald eyes scanning the assembly with a fierce determination. Beside her stood Grimgold Ironfist, the stout dwarf king, his beard bristling with a fiery passion.\n",
      "\n",
      "\"We are at war,\" Ariniel declared, her voice echoing through the forest glade. \"King Malakai's armies have brought destruction to our lands, and it is time we take action.\"\n",
      "\n",
      "Grimgold nodded, his gruff demeanor softening for an instant as he gazed upon the elven queen. \"We dwarves have long been wary of your kind, Ariniel. But I see now that our differences are of no consequence in the face of this great evil. We will stand with you against Malakai.\"\n",
      "\n",
      "Together, they forged a pact: to unite their forces and launch a series of daring raids upon King Malakai's strongholds. The elven archers would provide cover from afar, while the dwarves, masters of siegecraft, would breach the gates and lead the charge.\n",
      "\n",
      "As word of the alliance spread, other factions of Eridoria began to take notice. The wise wizards of the ancient tower of Eldrador, who had long watched from the shadows, decided to lend their aid. They crafted powerful magical artifacts, imbuing them with the essence of the land itself.\n",
      "\n",
      "Meanwhile, in the dark corners of the realm, whispers began to spread of a secret army forming. A band of rogue humans, tired of King Malakai's tyranny, pledged their loyalty to the elven-dwarven alliance.\n",
      "\n",
      "Among these rebels was a young woman named Eira. Born into a family of blacksmiths, she had long chafed against her father's loyalty to the crown. When Malakai's armies marched through their village, burning homes and enslaving their people, Eira knew that she could no longer stand idly by.\n",
      "\n",
      "With a small group of trusted companions, she slipped away into the night, bound for the gathering point of the alliance. As they rode across the rolling hills and verdant forests, Eira felt an excitement building within her – a sense that she was part of something greater than herself.\n",
      "\n",
      "The day of their first joint operation arrived, with the sun barely above the horizon. Ariniel's elven scouts reported the presence of King Malakai's forces in the nearby town of Oakhaven. A small garrison, led by one of Malakai's most trusted captains, had been left to guard the vital trade route.\n",
      "\n",
      "Grimgold Ironfist grinned, his face creasing with a mix of excitement and anticipation. \"Today, we take back our roads!\"\n",
      "\n",
      "As the elven archers took positions atop the surrounding hills, their bows singing as they loosed arrows at the garrison below. Meanwhile, Grimgold led his dwarves in a thunderous charge, their battle-axes swinging in perfect synchrony.\n",
      "\n",
      "Eira and her companions, armed with a mix of swords and shortbows, charged into the fray alongside the dwarves. Together, they carved a bloody path through the Malakai's men, clearing a path for the elven archers to advance.\n",
      "\n",
      "In the chaos of battle, Eira caught glimpses of Ariniel, her silver hair shining like moonlight as she directed the assault from atop a nearby outcropping. The elven queen wielded a majestic longbow, its string singing with each shot that found its mark among Malakai's soldiers.\n",
      "\n",
      "As the garrison crumbled, the dwarves and their human allies pressed on, determined to secure Oakhaven. Eira felt a surge of pride as she watched Grimgold Ironfist take up position before the town gates, his axe raised high in triumph.\n",
      "\n",
      "\"We will not be driven from our lands!\" he bellowed, his voice carrying across the battlefield.\n",
      "\n",
      "The humans and dwarves cheered in response, knowing that they stood together against an enemy that would stop at nothing to conquer all of Eridoria. The victory at Oakhaven marked only the beginning of their struggle, but with Ariniel's guidance and Grimgold's unwavering resolve, they knew that as long as they stood united, King Malakai's darkness could never prevail.\n",
      "\n",
      "As the battle faded into memory, Eira gazed out upon the newly liberated town. The people of Oakhaven emerged from their hiding places, tears streaming down their faces as they thanked the elves and dwarves for their bravery.\n",
      "\n",
      "In that moment, Eira knew that she had found her true calling – to stand alongside these ancient enemies-turned-allies in their fight against tyranny. Together, they would shape the future of Eridoria, forging a new era of peace and cooperation where none seemed possible before.\n",
      "\n",
      "The battle-hardened warriors, elves and dwarves alike, looked upon each other with newfound respect and admiration, knowing that their differences were no longer a liability but a strength in the face of overwhelming odds. As they rested among the people of Oakhaven, the first whispers of a new legend began to spread: one of friendship forged in fire and tempered by courage.\n",
      "\n",
      "In the shadows, the wizards of Eldrador watched with approval, their eyes twinkling with hope for a brighter future. For though the road ahead would be long and fraught with danger, they knew that as long as elves and dwarves stood together against King Malakai's darkness, the very fabric of Eridoria might yet be transformed into something greater than its sum.\n",
      "\n",
      "The alliance had begun, a beacon of hope in a world torn apart by conflict. And though the battles ahead would be fierce, Eira and her companions knew that they would not fight alone – for alongside them stood the unyielding determination of Ariniel and Grimgold, an unbreakable bond forged in fire and tempered by courage."
     ]
    }
   ],
   "source": [
    "\n",
    "for c in stream:\n",
    "    print(c['message']['content'], flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4992c183-451d-4b20-ade8-91825e32e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_system_prompt(selected_tool):\n",
    "    if selected_tool == \"web_search\":\n",
    "        return gr.update(value=\"You are a research assistant with access to a 'web_search' tool. Use it to find recent or unknown information.\")\n",
    "    return gr.update(value=\"You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing examples where possible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d97bee-63ba-4a8f-9427-2e466d9d2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_tools = \"You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing \\\n",
    "examples where possible. You have access to a 'web_search' tool to help you with questions that concern recent events or topics that \\\n",
    "you have limited knowledge of. If the question can be answered by using your own knowledge base, do not resort to the tool use.\".strip()\n",
    "\n",
    "system = \"You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing \\\n",
    "examples where possible.\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54afa900-30b4-4a50-ad29-3f1e2720e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://a1827ec920874bc6b5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a1827ec920874bc6b5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(\n",
    "    fn=chatter, \n",
    "    type=\"messages\",\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown(['llama3.1:8b-instruct-fp16', 'qwen2.5-coder:32b', 'llama3.2', 'qwen2.5:14b', 'deepseek-r1:32b'], value='qwen2.5:14b', label=\"Choose Downloaded Ollama Model\"),\n",
    "        gr.Dropdown(['No tools', 'web_search'], value='web_search', label=\"Choose Tools to Use\"),\n",
    "        gr.TextArea(system.strip(), label=\"System Prompt\"),\n",
    "        gr.Textbox(0.1, label=\"Temperature\")\n",
    "    ]).launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e700fa3-66d2-4f17-88b3-e3ccb205c5e4",
   "metadata": {},
   "source": [
    "> The below does not work since the `gr.ChatInterface().launch()` does not support `close()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22cb82b-0c19-4b6b-b1ad-f066548c345f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://9b06d4caad36a9abd5.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9b06d4caad36a9abd5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio app is running.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TupleNoPrint' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     history\u001b[38;5;241m.\u001b[39mappend((message, response))\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mChatInterfaceContext\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama3.1:8b-instruct-fp16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqwen2.5-coder:32b\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama3.2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllama3.1:8b-instruct-fp16\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mChoose Downloaded Ollama Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextArea\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing examples where possible.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSystem Prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSlider\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m demo:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradio app is running.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mChatInterfaceContext.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_value, traceback):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapp:\n\u001b[0;32m---> 22\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TupleNoPrint' object has no attribute 'close'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/blocks.py\", line 2042, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/blocks.py\", line 1587, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/utils.py\", line 850, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/chat_interface.py\", line 855, in _submit_fn\n",
      "    history = self._append_message_to_history(response, history, \"assistant\")\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/chat_interface.py\", line 786, in _append_message_to_history\n",
      "    message_dicts = self._message_as_message_dict(message, role)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.12/site-packages/gradio/chat_interface.py\", line 824, in _message_as_message_dict\n",
      "    for x in msg.get(\"files\", []):\n",
      "             ^^^^^^^\n",
      "AttributeError: 'tuple' object has no attribute 'get'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "class ChatInterfaceContext:\n",
    "    def __init__(self, fn, type=\"messages\", additional_inputs=None):\n",
    "        self.fn = fn\n",
    "        self.type = type\n",
    "        self.additional_inputs = additional_inputs if additional_inputs is not None else []\n",
    "        self.app = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        chatbot = gr.ChatInterface(\n",
    "                    fn=self.fn,\n",
    "                    type=self.type,\n",
    "                    additional_inputs=self.additional_inputs\n",
    "                )\n",
    "            \n",
    "        self.app = chatbot.launch(share=True, inbrowser=False)\n",
    "        return self.app\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        if self.app:\n",
    "            # not supported\n",
    "            self.app.close()\n",
    "\n",
    "def chatter(message, history, model, system_prompt, temperature):\n",
    "    # Your chat logic here\n",
    "    response = f\"Response from {model} with prompt: {system_prompt} and temperature: {temperature}\"\n",
    "    history.append((message, response))\n",
    "    return \"\", history\n",
    "\n",
    "with ChatInterfaceContext(\n",
    "    fn=chatter,\n",
    "    additional_inputs=[\n",
    "        gr.Dropdown(['llama3.1:8b-instruct-fp16', 'qwen2.5-coder:32b', 'llama3.2', 'deepseek-r1:32b'], value='deepseek-r1:32b', label=\"Choose Downloaded Ollama Model\"),\n",
    "        gr.TextArea(\"You are a friendly and helpful assistant. Your aim is to answer the questions posed by your human friends thoroughly, providing examples where possible.\", label=\"System Prompt\"),\n",
    "        gr.Slider(0.0, 1.0, value=0.1, step=0.01, label=\"Temperature\")\n",
    "    ]\n",
    ") as demo:\n",
    "    print(\"Gradio app is running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f763aa99-3f7e-40dd-a237-79f8fc6a9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_system_prompt(selected_tool):\n",
    "    if selected_tool == \"web_search\":\n",
    "        return gr.update(value=system_tools)\n",
    "    return gr.update(value=system)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68773271-2b36-49fa-afd5-456e7c70a6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://11d960ef5c040c3495.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://11d960ef5c040c3495.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(type=\"messages\")\n",
    "    model_dropdown = gr.Dropdown(\n",
    "        ['llama3.1:8b-instruct-fp16', 'qwen2.5-coder:32b', 'llama3.2', 'deepseek-r1:32b'],\n",
    "        value='qwen2.5-coder:32b',\n",
    "        label=\"Choose Downloaded Ollama Model\"\n",
    "    )\n",
    "    \n",
    "    tools_dropdown = gr.Dropdown(\n",
    "        ['No tools', 'web_search'],\n",
    "        value='No tools',\n",
    "        label=\"Choose Tools to Use\"\n",
    "    )\n",
    "\n",
    "    system_prompt = gr.TextArea(\n",
    "        system,\n",
    "        label=\"System Prompt\"\n",
    "    )\n",
    "\n",
    "    temperature = gr.Textbox(0.1, label=\"Temperature\")\n",
    "\n",
    "    tools_dropdown.change(update_system_prompt, tools_dropdown, system_prompt)\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7be6fa-f889-4f49-9405-dbc48161b030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.pull(\"qwen2.5-coder:32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b904caea-71ca-489e-a793-b80bb808f8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'success'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.pull('deepseek-r1:32b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af1f09-ff0e-4ec5-a031-b4c24621da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.pull(\"qwen2.5:14b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "351e2bc6-1fc6-4816-bac5-7e282e80245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59429c7-d168-4d5a-800f-1beefd6d2a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"model\":\"qwen2.5:14b\",\"modified_at\":\"2025-03-08T13:16:08.873753Z\",\"digest\":\"7cdf5a0187d5c58cc5d369b255592f7841d1c4696d45a8c8a9489440385b22f6\",\"size\":8988124069,\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"qwen2\",\"families\":[\"qwen2\"],\"parameter_size\":\"14.8B\",\"quantization_level\":\"Q4_K_M\"}}\n",
      "{\"model\":\"deepseek-r1:32b\",\"modified_at\":\"2025-02-03T07:45:31.124246Z\",\"digest\":\"38056bbcbb2d068501ecb2d5ea9cea9dd4847465f1ab88c4d4a412a9f7792717\",\"size\":19851337640,\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"qwen2\",\"families\":[\"qwen2\"],\"parameter_size\":\"32.8B\",\"quantization_level\":\"Q4_K_M\"}}\n",
      "{\"model\":\"qwen2.5-coder:32b\",\"modified_at\":\"2025-01-26T14:18:39.353482Z\",\"digest\":\"4bd6cbf2d094264457a17aab6bd6acd1ed7a72fb8f8be3cfb193f63c78dd56df\",\"size\":19851349856,\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"qwen2\",\"families\":[\"qwen2\"],\"parameter_size\":\"32.8B\",\"quantization_level\":\"Q4_K_M\"}}\n",
      "{\"model\":\"llama3.1:8b-instruct-fp16\",\"modified_at\":\"2024-11-20T15:44:02.680559Z\",\"digest\":\"4aacac4194543ff7f70dab3f2ebc169c132d5319bb36f7a7e99c4ff525ebcc09\",\"size\":16068910253,\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"8.0B\",\"quantization_level\":\"F16\"}}\n",
      "{\"model\":\"llama3.2:latest\",\"modified_at\":\"2024-11-15T15:18:17.022783Z\",\"digest\":\"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72\",\"size\":2019393189,\"details\":{\"parent_model\":\"\",\"format\":\"gguf\",\"family\":\"llama\",\"families\":[\"llama\"],\"parameter_size\":\"3.2B\",\"quantization_level\":\"Q4_K_M\"}}\n"
     ]
    }
   ],
   "source": [
    "for model in client.list().models:\n",
    "    print(model.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
