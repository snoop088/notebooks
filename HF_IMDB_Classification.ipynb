{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97247069-d782-46b0-b69f-0be2fe1409d7",
   "metadata": {},
   "source": [
    "## Trying to train MS Phy Model for Classification\n",
    "\n",
    "### Using the IMDB Dataset as a test case\n",
    "\n",
    "We will add classification head to the Phy model. We will try and train without QLora first. See if it fits in memory\n",
    "\n",
    "If we have problems we will need to use Peft but with regular trainer as it is a classification task instead of Text Gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95957d60-128a-473f-bc9c-83a91511f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.16.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting Click!=8.0.0,>=7.1 (from wandb)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from wandb) (5.9.5)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-1.39.1-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: PyYAML in /opt/conda/lib/python3.11/site-packages (from wandb) (6.0.1)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from wandb) (68.2.2)\n",
      "Collecting appdirs>=1.4.3 (from wandb)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb)\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wandb-0.16.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentry_sdk-1.39.1-py2.py3-none-any.whl (254 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setproctitle-1.3.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
      "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: appdirs, smmap, setproctitle, sentry-sdk, protobuf, einops, docker-pycreds, Click, gitdb, GitPython, wandb\n",
      "Successfully installed Click-8.1.7 GitPython-3.1.40 appdirs-1.4.4 docker-pycreds-0.4.0 einops-0.7.0 gitdb-4.0.11 protobuf-4.25.1 sentry-sdk-1.39.1 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c9fa46-a388-4200-a41d-99b4f96343c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    PreTrainedModel\n",
    ")\n",
    "from transformers.modeling_outputs import SequenceClassifierOutputWithPast\n",
    "from typing import List, Optional, Tuple, Union\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from peft import LoraConfig, peft_model, get_peft_model, AutoPeftModelForCausalLM\n",
    "from peft.tuners.lora import LoraLayer\n",
    "# from trl import SFTTrainer # this is only needed when we Tune\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers.utils import ( add_code_sample_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    is_flash_attn_2_available,\n",
    "    is_flash_attn_greater_or_equal_2_10,\n",
    "    logging,\n",
    "    replace_return_docstrings)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba30f6f0-1d71-42d9-bf7b-adb4b7170ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=\"./data/imdb_reviews/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a461d514-7876-4c86-8d87-e18670f7a8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 45000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset[\"train\"].shuffle(42).train_test_split(0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93fb0ade-6075-4291-bcbc-1f19c73c5833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['review', 'sentiment'],\n",
       "         num_rows: 36000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['review', 'sentiment'],\n",
       "         num_rows: 9000\n",
       "     })\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['review', 'sentiment'],\n",
       "         num_rows: 45000\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['review', 'sentiment'],\n",
       "         num_rows: 5000\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dataset[\"train\"].train_test_split(0.2)\n",
    "data, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a894d5-4a65-4abc-b57d-6f49e8a2237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"eval\"] = data[\"test\"]\n",
    "data[\"test\"] = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1f01b5-0ad1-4111-80be-67b9804ded0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 36000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['review', 'sentiment'],\n",
       "        num_rows: 9000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data\n",
    "data = None\n",
    "del data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e7d5bff-86ee-48a0-8933-d58ee3040100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(row):\n",
    "    return len(row['review'].split(' '))\n",
    "pd.set_option(\"expand_frame_repr\", False)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.DataFrame(dataset[\"train\"].shuffle()[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11981e28-3e75-49a6-bcd5-6862f8e2e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num'] = df.apply(lambda row: count(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ac99ac7-d2af-488b-9921-1aad09d85996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233.498"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['num'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19a97ad5-1b17-4916-92ea-5b2d73ba3eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some weights for unbalanced positive/ negative spread\n",
    "pos_weights = len(dataset['train'].to_pandas()) / (2 * dataset['train'].to_pandas().sentiment.value_counts()['positive'])\n",
    "neg_weights = len(dataset['train'].to_pandas()) / (2 * dataset['train'].to_pandas().sentiment.value_counts()['negative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5b9c0c8-3278-4d4d-ae75-bda8f844f294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As far as the Muppet line goes, however, this is not the best, nor the second best. This was marketed towards the kiddies, but has some dark, and emotionally upsetting adult moments, to which parents may not wish to expose their children. One of which showcases Miss Piggy going \"postal\" in a jealous rage, which lasts basically throughout the duration of this work.&lt;br /&gt;&lt;br /&gt;Beyond that, however, the story is progressive, and highly entertaining. One scene in which Joan Rivers and Miss PIggy go berserk in a department store is simply hilarious! And there are other parts of this work which contain the same level of levity and fun.&lt;br /&gt;&lt;br /&gt;I like this very much, and enjoy it still today.&lt;br /&gt;&lt;br /&gt;It rates a 7.6/10 from...&lt;br /&gt;&lt;br /&gt;the Fiend :.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by. Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever (who says the golden age of cinema is dead?) and Vartan holds his own.&lt;br /&gt;&lt;br /&gt;There is simmering chemistry between the two leads; the film is most alive when they share a scene - lots! It is done so well that you find yourself willing them to get together...&lt;br /&gt;&lt;br /&gt;Ignore the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much better. If you are already happy, then you will be euphoric.&lt;br /&gt;&lt;br /&gt;(PS: I am 33, Male, from the UK and a hopeless romantic still searching for his Princess...)</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Take a pinch of GOODFELLAS, mix it with THE GODFATHER, add some Roman mythology and plenty of lowbrow comedy, and you have THE SOPRANOS, about a mob clan operating out of northern New Jersey. It's almost as entertaining as pro wrestling. I am not the biggest fan of this show, but I do admire James Gandolfini's very complicated Tony Soprano, a psychopath with an occasional glimmer of conscience. I also have come to admire te contributions of folks like gravel-voiced Dom Chianese as the bewildered but murderous Uncle Junior, silver-haired Tony Sirico as the perpetually perplexed Paulie and the very beautiful Edie Falco as the duplicitous, tough-as-nails Carmela Soprano. The violence is sudden and graphic, the body count steadily climbs each season, but it is often the small moments that matter most here. Watch Paulie and Tony's nephew Christopher (Michael Imperioli late of LAW &amp; ORDER) as they get lost in the Pine Barrens and sit out a bitter cold night in an abandoned trruck, both convinced they've had it.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what ever you do do not waste your time on this pointless. movie. A remake that did not need to be retold. Everyone coming out of the theater had the same comments. Worst movie I ever saw. Save your time and money!!!&lt;br /&gt;&lt;br /&gt;Nicgolas Cage was biking down hills, swimming in murky water and rolling down hills while being attacked by bees but yet his suit was still perfectly pressed and shirt crisp white until the very last scene.&lt;br /&gt;&lt;br /&gt;Although a good cast with Ellen Bernstein and Cage the acting was just as unbelievable as the movie itself. It is amazing how good actors can do such bad movies. Don't they get a copy of the script first. If you still have any interest at all in seeing the movie at the very least wait for it to come out on DVD.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the first OVA's (\"original video animation\") I ever bought, this still has to be one of my favourite anime titles. A cyberpunk sci-fi action comedy set against an unlikely (for a comedy, that is) background of near-future pollution in a dystopian society.&lt;br /&gt;&lt;br /&gt;The \"heroes\" of Dominion are the Tank Police, formed with a \"if we can't beat crime, we'll get bigger guns\" philosophy, and who are, like the name suggests, patrolling the city in tanks instead of patrol cars, and who are actually far more dangerous than any criminals they are trying to catch. Most, if not all, of these cops are borderline(?) psychopaths and neurotics, giving new meaning to the phrase \"loose cannons\".&lt;br /&gt;&lt;br /&gt;Equally colourful and amusing are their adversaries, terrorist Buaku and his hench(wo)men, the Twin Cat Sisters, whose existence always seems to involve giving the Tank Police a hard time.&lt;br /&gt;&lt;br /&gt;The animation is not state of the art, but it's very nice otherwise; the colourful palette and cartoonish look of the characters and mecha fit nicely with the comedic atmosphere of Dominion.&lt;br /&gt;&lt;br /&gt;The English dubbing is, again, lots of fun. The soundtrack of the English version is also very good. I wonder if they ever made a soundtrack album of that...&lt;br /&gt;&lt;br /&gt;Anyway, Dominion Tank Police is great. It's Japanese cyberpunk SF with lots of comedy, filled with completely over-the-top characters and situations, making sure that it never takes itself seriously. Highly recommended.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Franco Rossi's 1985 six-hour Italian mini-series of Quo Vadis is a very curious beast, creating an absolutely convincing ancient Roman world shot in matter of fact fashion (very few long shots, no big cityscapes), but playing the drama down so much in favour of allusions to classical literature and history that the story constantly gets lost in the background.&lt;br /&gt;&lt;br /&gt;The shifting structure (much of episode one is played out via voice over letters) and lack of narrative urgency makes the full six-hour version simultaneously demanding and undemanding, and certainly far too often uninvolving, but it has something going for it. The two main strengths are the characterisation of Petronius (a thankfully dubbed Frederic Forrest, whose own voice would almost certainly flatten his dialogue) as a man whose spent so long looking for an astute angle to survive court life that he's become incapable of experiencing emotion, and Klaus Maria Brandauer's unique take on Nero as a wannabe actor whose every move and action is calculated on how his 'audience' will receive it. Elsewhere, Max Von Sydow briefly appears in a few episodes, being rewarded with the show's most impressive and genuinely moving scene here he encounters a child as he attempts to leave Rome. It's the kind of thing the show could do with more of, but it seems all too often to flatten every potentially emotional, inspiring or exciting moment under it's relentlessly low-key direction.&lt;br /&gt;&lt;br /&gt;Unfortunately Francesco Quinn makes a staggeringly anonymous hero, blending in with the walls and coming over less as a Roman officer than that quiet, slightly gormless but inoffensive guy who works in the same office as you who never says much at office parties - you know, the one who you think is called Dave or something like that. The budgetary limitations are very visible once its Meet the Lions time for the Christians and Ursus battle with the bull is so determinedly low key that it just passes over you before the show just abruptly loses interest and suddenly ends.&lt;br /&gt;&lt;br /&gt;Not a trip I can particularly recommend, I'm afraid, but if you do embark on it it's one not entirely without its small rewards.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          review sentiment\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         As far as the Muppet line goes, however, this is not the best, nor the second best. This was marketed towards the kiddies, but has some dark, and emotionally upsetting adult moments, to which parents may not wish to expose their children. One of which showcases Miss Piggy going \"postal\" in a jealous rage, which lasts basically throughout the duration of this work.<br /><br />Beyond that, however, the story is progressive, and highly entertaining. One scene in which Joan Rivers and Miss PIggy go berserk in a department store is simply hilarious! And there are other parts of this work which contain the same level of levity and fun.<br /><br />I like this very much, and enjoy it still today.<br /><br />It rates a 7.6/10 from...<br /><br />the Fiend :.  positive\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       I just can't understand the negative comments about this film. Yes it is a typical boy-meets-girl romance but it is done with such flair and polish that the time just flies by. Henstridge (talk about winning the gene-pool lottery!) is as magnetic and alluring as ever (who says the golden age of cinema is dead?) and Vartan holds his own.<br /><br />There is simmering chemistry between the two leads; the film is most alive when they share a scene - lots! It is done so well that you find yourself willing them to get together...<br /><br />Ignore the negative comments - if you are feeling a bit blue, watch this flick, you will feel so much better. If you are already happy, then you will be euphoric.<br /><br />(PS: I am 33, Male, from the UK and a hopeless romantic still searching for his Princess...)  positive\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Take a pinch of GOODFELLAS, mix it with THE GODFATHER, add some Roman mythology and plenty of lowbrow comedy, and you have THE SOPRANOS, about a mob clan operating out of northern New Jersey. It's almost as entertaining as pro wrestling. I am not the biggest fan of this show, but I do admire James Gandolfini's very complicated Tony Soprano, a psychopath with an occasional glimmer of conscience. I also have come to admire te contributions of folks like gravel-voiced Dom Chianese as the bewildered but murderous Uncle Junior, silver-haired Tony Sirico as the perpetually perplexed Paulie and the very beautiful Edie Falco as the duplicitous, tough-as-nails Carmela Soprano. The violence is sudden and graphic, the body count steadily climbs each season, but it is often the small moments that matter most here. Watch Paulie and Tony's nephew Christopher (Michael Imperioli late of LAW & ORDER) as they get lost in the Pine Barrens and sit out a bitter cold night in an abandoned trruck, both convinced they've had it.  positive\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         what ever you do do not waste your time on this pointless. movie. A remake that did not need to be retold. Everyone coming out of the theater had the same comments. Worst movie I ever saw. Save your time and money!!!<br /><br />Nicgolas Cage was biking down hills, swimming in murky water and rolling down hills while being attacked by bees but yet his suit was still perfectly pressed and shirt crisp white until the very last scene.<br /><br />Although a good cast with Ellen Bernstein and Cage the acting was just as unbelievable as the movie itself. It is amazing how good actors can do such bad movies. Don't they get a copy of the script first. If you still have any interest at all in seeing the movie at the very least wait for it to come out on DVD.  negative\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      One of the first OVA's (\"original video animation\") I ever bought, this still has to be one of my favourite anime titles. A cyberpunk sci-fi action comedy set against an unlikely (for a comedy, that is) background of near-future pollution in a dystopian society.<br /><br />The \"heroes\" of Dominion are the Tank Police, formed with a \"if we can't beat crime, we'll get bigger guns\" philosophy, and who are, like the name suggests, patrolling the city in tanks instead of patrol cars, and who are actually far more dangerous than any criminals they are trying to catch. Most, if not all, of these cops are borderline(?) psychopaths and neurotics, giving new meaning to the phrase \"loose cannons\".<br /><br />Equally colourful and amusing are their adversaries, terrorist Buaku and his hench(wo)men, the Twin Cat Sisters, whose existence always seems to involve giving the Tank Police a hard time.<br /><br />The animation is not state of the art, but it's very nice otherwise; the colourful palette and cartoonish look of the characters and mecha fit nicely with the comedic atmosphere of Dominion.<br /><br />The English dubbing is, again, lots of fun. The soundtrack of the English version is also very good. I wonder if they ever made a soundtrack album of that...<br /><br />Anyway, Dominion Tank Police is great. It's Japanese cyberpunk SF with lots of comedy, filled with completely over-the-top characters and situations, making sure that it never takes itself seriously. Highly recommended.  positive\n",
       "5  Franco Rossi's 1985 six-hour Italian mini-series of Quo Vadis is a very curious beast, creating an absolutely convincing ancient Roman world shot in matter of fact fashion (very few long shots, no big cityscapes), but playing the drama down so much in favour of allusions to classical literature and history that the story constantly gets lost in the background.<br /><br />The shifting structure (much of episode one is played out via voice over letters) and lack of narrative urgency makes the full six-hour version simultaneously demanding and undemanding, and certainly far too often uninvolving, but it has something going for it. The two main strengths are the characterisation of Petronius (a thankfully dubbed Frederic Forrest, whose own voice would almost certainly flatten his dialogue) as a man whose spent so long looking for an astute angle to survive court life that he's become incapable of experiencing emotion, and Klaus Maria Brandauer's unique take on Nero as a wannabe actor whose every move and action is calculated on how his 'audience' will receive it. Elsewhere, Max Von Sydow briefly appears in a few episodes, being rewarded with the show's most impressive and genuinely moving scene here he encounters a child as he attempts to leave Rome. It's the kind of thing the show could do with more of, but it seems all too often to flatten every potentially emotional, inspiring or exciting moment under it's relentlessly low-key direction.<br /><br />Unfortunately Francesco Quinn makes a staggeringly anonymous hero, blending in with the walls and coming over less as a Roman officer than that quiet, slightly gormless but inoffensive guy who works in the same office as you who never says much at office parties - you know, the one who you think is called Dave or something like that. The budgetary limitations are very visible once its Meet the Lions time for the Christians and Ursus battle with the bull is so determinedly low key that it just passes over you before the show just abruptly loses interest and suddenly ends.<br /><br />Not a trip I can particularly recommend, I'm afraid, but if you do embark on it it's one not entirely without its small rewards.  negative"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small = pd.DataFrame(dataset[\"test\"][444:450])\n",
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bd69efb-3160-43de-99c4-aacc8d0fbb8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9961217659246666, 1.003908550623762)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_weights, neg_weights # too evenly spread to bother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0208201e-4102-439d-864e-5a0241a57906",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.microsoft.phi-2.d3186761bf5c4409f7679359284066c25ab668ee.configuration_phi.PhiConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, T5Config, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:569\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    567\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    568\u001b[0m     )\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers_modules.microsoft.phi-2.d3186761bf5c4409f7679359284066c25ab668ee.configuration_phi.PhiConfig'> for this kind of AutoModel: AutoModelForSequenceClassification.\nModel type should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BloomConfig, CamembertConfig, CanineConfig, LlamaConfig, ConvBertConfig, CTRLConfig, Data2VecTextConfig, DebertaConfig, DebertaV2Config, DistilBertConfig, ElectraConfig, ErnieConfig, ErnieMConfig, EsmConfig, FalconConfig, FlaubertConfig, FNetConfig, FunnelConfig, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTJConfig, IBertConfig, LayoutLMConfig, LayoutLMv2Config, LayoutLMv3Config, LEDConfig, LiltConfig, LlamaConfig, LongformerConfig, LukeConfig, MarkupLMConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MobileBertConfig, MPNetConfig, MptConfig, MraConfig, MT5Config, MvpConfig, NezhaConfig, NystromformerConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PerceiverConfig, PersimmonConfig, PhiConfig, PLBartConfig, QDQBertConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, SqueezeBertConfig, T5Config, TapasConfig, TransfoXLConfig, UMT5Config, XLMConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, YosoConfig."
     ]
    }
   ],
   "source": [
    "model_name = 'microsoft/phi-2'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "da089463-3563-4302-8005-53c335932d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ecd0440-34dd-4cc7-ad0d-de968c9cce12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bigscience/bloom-1b1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb956f1-9720-4bf0-877a-7900d05c3d58",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_name\u001b[49m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, trust_remote_code=True, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4683f185-b270-4b12-9566-59a04ad61e9d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3677b6f1f04a28bfef60bae440e6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# lets load it as base model for CausalLM\n",
    "model_name = 'microsoft/phi-2'\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccb0c9e2-637f-42c6-a073-64b9a2c33657",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (transformer): PhiModel(\n",
       "    (embd): Embedding(\n",
       "      (wte): Embedding(51200, 2560)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x ParallelBlock(\n",
       "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (mixer): MHA(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): Linear4bit(in_features=2560, out_features=7680, bias=True)\n",
       "          (out_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (inner_attn): SelfAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): CrossAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): CausalLMHead(\n",
       "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bfb7f-c415-4297-8474-fe7f9b0beeac",
   "metadata": {},
   "source": [
    "### OK No API for Seq Classification.\n",
    "\n",
    "Lets override the Seq Classification Class from here.\n",
    "\n",
    "We will load the model for CausalLM, then we will use it as base_model and hardcode the number of labels + outputs, as shown [here](https://colab.research.google.com/drive/1y_CFog1i97Ctwre41kUnKuTGFWgzGWte?usp=sharing#scrollTo=MY3ksrAdyHiG)\n",
    "\n",
    "blog post [here](https://medium.com/mlearning-ai/microsoft-phi-2-for-classification-b83beaec2069)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b936c3f6-498e-4d91-9bc2-69ac5ba2c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some variables to use for the classification\n",
    "\n",
    "NUM_LABELS = 2\n",
    "\n",
    "class PhiPreTrainedModel(PreTrainedModel):\n",
    "    config_class = base_model.config_class\n",
    "    base_model_prefix = \"model\"\n",
    "    supports_gradient_checkpointing = True\n",
    "    _skip_keys_device_placement = \"past_key_values\"\n",
    "    _supports_flash_attn_2 = True\n",
    "    _supports_cache_class = True\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "                \n",
    "#custom class - modified from PhiForSequenceClassification\n",
    "# original class is here: \n",
    "# https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/models/phi/modeling_phi.py#L1165\n",
    "class PhiForSequenceClassificationModified(PhiPreTrainedModel):\n",
    "    def __init__(self, config, base_model, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels#changed\n",
    "        self.model = base_model.transformer#changed\n",
    "        self.score = nn.Linear(base_model.config.hidden_size, NUM_LABELS, bias=False)#changed\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.model.embd.wte#changed\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.model.embd.wte = value#changed\n",
    "\n",
    "    @add_start_docstrings_to_model_forward(\"PHI_INPUTS_DOCSTRING\")\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        model_outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "\n",
    "        )\n",
    "        hidden_states = model_outputs#changed\n",
    "        logits = self.score(hidden_states)\n",
    "        # print(logits)\n",
    "\n",
    "        if input_ids is not None:\n",
    "            batch_size = input_ids.shape[0]\n",
    "        else:\n",
    "            batch_size = inputs_embeds.shape[0]\n",
    "\n",
    "        if self.config.pad_token_id is None and batch_size != 1:\n",
    "            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n",
    "        if self.config.pad_token_id is None:\n",
    "            sequence_lengths = -1\n",
    "        else:\n",
    "            if input_ids is not None:\n",
    "                sequence_lengths = (torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1).to(\n",
    "                    logits.device\n",
    "                )\n",
    "            else:\n",
    "                sequence_lengths = -1\n",
    "\n",
    "        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(pooled_logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(pooled_logits, labels)\n",
    "        if not return_dict:\n",
    "            output = (pooled_logits,) + model_outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            past_key_values=None,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "        )#changed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18de9ff-f877-4633-a55c-8fb58766d02c",
   "metadata": {},
   "source": [
    "Problems - doesnt seem to work with few errors occuring. Lets try Bloom instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f13c1c2a-5eac-4fca-bf9f-f8aea3fc0865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = PhiForSequenceClassificationModified(base_model.config, base_model, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae6517be-631f-410a-90ca-bf12aadd5433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(BloomForSequenceClassification(\n",
       "   (transformer): BloomModel(\n",
       "     (word_embeddings): Embedding(250880, 1536)\n",
       "     (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "     (h): ModuleList(\n",
       "       (0-23): 24 x BloomBlock(\n",
       "         (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "         (self_attention): BloomAttention(\n",
       "           (query_key_value): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "           (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "           (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "         )\n",
       "         (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "         (mlp): BloomMLP(\n",
       "           (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "           (gelu_impl): BloomGelu()\n",
       "           (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "   )\n",
       "   (score): Linear(in_features=1536, out_features=2, bias=False)\n",
       " ),\n",
       " 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b251c3-7703-4e87-9d9f-1b03ce421947",
   "metadata": {},
   "source": [
    "### Try Mistral Also\n",
    "\n",
    "Reading another blog article [here](https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral#mistral), it seems that for classification smaller models perform better than LLM. So we can do e test case with Mistral which seems to have a proper Head for SeqClass on HF.\n",
    "\n",
    "We may need to use LORA and Quantization, however as it seems it will not fit.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f221952-b3d7-4b3c-8761-81787e3ddddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
    "model.config.label2id = {'NEGATIVE': 0, 'POSITIVE': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7fa05bc-7b7a-473f-9e9d-667239fb981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(example):\n",
    "    item = tokenizer(example[\"review\"], truncation=True, max_length=320) # see if this is OK for dyn padding\n",
    "    item[\"labels\"] = [ 1 if sent == 'positive' else 0 for sent in example[\"sentiment\"]]\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bdaef48e-73c4-45e5-a894-8bc25f7367a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7671f6cd8d6c43889560320477bb0a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/36000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fc4cb0b6ed42bc8ae080907124b868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68eafa689e6e46aab6cc30be0c5adfe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenised_data = dataset.map(process_data, batched=True, num_proc=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "14bfa867-cdc8-4a32-9d03-4bb94ccb5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenised_data = tokenised_data.remove_columns([\"review\", \"sentiment\"])\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "097d457c-d7c4-4a6a-a06e-d3fcd42c0d93",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44,\n",
       " 47697,\n",
       " 1119,\n",
       " 51651,\n",
       " 1620,\n",
       " 661,\n",
       " 4936,\n",
       " 3638,\n",
       " 7384,\n",
       " 16401,\n",
       " 1185,\n",
       " 661,\n",
       " 16916,\n",
       " 4384,\n",
       " 15,\n",
       " 14600,\n",
       " 55052,\n",
       " 2194,\n",
       " 791,\n",
       " 1130,\n",
       " 17,\n",
       " 5361,\n",
       " 1306,\n",
       " 3466,\n",
       " 14998,\n",
       " 4143,\n",
       " 140434,\n",
       " 530,\n",
       " 179501,\n",
       " 2376,\n",
       " 718,\n",
       " 632,\n",
       " 613,\n",
       " 14216,\n",
       " 1130,\n",
       " 427,\n",
       " 722,\n",
       " 25739,\n",
       " 427,\n",
       " 5067,\n",
       " 267,\n",
       " 11940,\n",
       " 2194,\n",
       " 7496,\n",
       " 5963,\n",
       " 15,\n",
       " 10118,\n",
       " 26676,\n",
       " 50827,\n",
       " 4978,\n",
       " 368,\n",
       " 1230,\n",
       " 2233,\n",
       " 461,\n",
       " 5553,\n",
       " 59859,\n",
       " 567,\n",
       " 179034,\n",
       " 5,\n",
       " 47060,\n",
       " 17,\n",
       " 473,\n",
       " 11602,\n",
       " 1485,\n",
       " 23346,\n",
       " 15,\n",
       " 4618,\n",
       " 14216,\n",
       " 71096,\n",
       " 8610,\n",
       " 368,\n",
       " 4548,\n",
       " 427,\n",
       " 21525,\n",
       " 6834,\n",
       " 8621,\n",
       " 25754,\n",
       " 15,\n",
       " 530,\n",
       " 44556,\n",
       " 36874,\n",
       " 165132,\n",
       " 1306,\n",
       " 267,\n",
       " 15422,\n",
       " 86995,\n",
       " 17,\n",
       " 426,\n",
       " 68136,\n",
       " 361,\n",
       " 35581,\n",
       " 3121,\n",
       " 14275,\n",
       " 1728,\n",
       " 3760,\n",
       " 1881,\n",
       " 15,\n",
       " 368,\n",
       " 18210,\n",
       " 8876,\n",
       " 1620,\n",
       " 71429,\n",
       " 5276,\n",
       " 217597,\n",
       " 2292,\n",
       " 613,\n",
       " 14216,\n",
       " 361,\n",
       " 24955,\n",
       " 14779,\n",
       " 17303,\n",
       " 38319,\n",
       " 15,\n",
       " 530,\n",
       " 14216,\n",
       " 361,\n",
       " 9897,\n",
       " 3866,\n",
       " 3808,\n",
       " 36830,\n",
       " 16622,\n",
       " 1427,\n",
       " 3291,\n",
       " 45240,\n",
       " 6364,\n",
       " 36830,\n",
       " 16045,\n",
       " 17123,\n",
       " 427,\n",
       " 42488,\n",
       " 3808,\n",
       " 35076,\n",
       " 69,\n",
       " 16489,\n",
       " 17,\n",
       " 21998,\n",
       " 632,\n",
       " 66818,\n",
       " 999,\n",
       " 7963,\n",
       " 6147,\n",
       " 1119,\n",
       " 25979,\n",
       " 361,\n",
       " 5553,\n",
       " 3509,\n",
       " 15,\n",
       " 530,\n",
       " 1701,\n",
       " 2213,\n",
       " 267,\n",
       " 7220,\n",
       " 5382,\n",
       " 919,\n",
       " 4143,\n",
       " 210406,\n",
       " 1701,\n",
       " 1542,\n",
       " 1728,\n",
       " 37468,\n",
       " 919,\n",
       " 2592,\n",
       " 3509,\n",
       " 791,\n",
       " 8512,\n",
       " 17,\n",
       " 37864,\n",
       " 427,\n",
       " 368,\n",
       " 12617,\n",
       " 461,\n",
       " 16401,\n",
       " 1185,\n",
       " 15,\n",
       " 1701,\n",
       " 1306,\n",
       " 7086,\n",
       " 427,\n",
       " 39390,\n",
       " 3595,\n",
       " 718,\n",
       " 2742,\n",
       " 722,\n",
       " 162525,\n",
       " 8920,\n",
       " 3638,\n",
       " 14216,\n",
       " 861,\n",
       " 5219,\n",
       " 4054,\n",
       " 447,\n",
       " 18090,\n",
       " 613,\n",
       " 267,\n",
       " 169585,\n",
       " 14775,\n",
       " 375,\n",
       " 5984,\n",
       " 28226,\n",
       " 13729,\n",
       " 632,\n",
       " 861,\n",
       " 368,\n",
       " 854,\n",
       " 1306,\n",
       " 7836,\n",
       " 1216,\n",
       " 216442,\n",
       " 5067,\n",
       " 21380,\n",
       " 1776,\n",
       " 44181,\n",
       " 15,\n",
       " 267,\n",
       " 29180,\n",
       " 1427,\n",
       " 4936,\n",
       " 1427,\n",
       " 861,\n",
       " 1152,\n",
       " 6582,\n",
       " 2213,\n",
       " 368,\n",
       " 54397,\n",
       " 861,\n",
       " 202905,\n",
       " 6325,\n",
       " 632,\n",
       " 664,\n",
       " 368,\n",
       " 25169,\n",
       " 661,\n",
       " 6355,\n",
       " 17,\n",
       " 473,\n",
       " 6482,\n",
       " 2592,\n",
       " 3853,\n",
       " 632,\n",
       " 861,\n",
       " 1130,\n",
       " 1728,\n",
       " 14216,\n",
       " 1306,\n",
       " 368,\n",
       " 5025,\n",
       " 15,\n",
       " 530,\n",
       " 368,\n",
       " 14216,\n",
       " 368,\n",
       " 12330,\n",
       " 1306,\n",
       " 10343,\n",
       " 427,\n",
       " 567,\n",
       " 180444,\n",
       " 5,\n",
       " 1306,\n",
       " 1130,\n",
       " 368,\n",
       " 26696,\n",
       " 5268,\n",
       " 3276,\n",
       " 8265,\n",
       " 427,\n",
       " 2213,\n",
       " 361,\n",
       " 361,\n",
       " 368,\n",
       " 3968,\n",
       " 6507,\n",
       " 17,\n",
       " 1387,\n",
       " 38026,\n",
       " 125347,\n",
       " 461,\n",
       " 368,\n",
       " 13420,\n",
       " 427,\n",
       " 368,\n",
       " 144795,\n",
       " 16909,\n",
       " 1152,\n",
       " 70654,\n",
       " 368,\n",
       " 8950,\n",
       " 461,\n",
       " 368,\n",
       " 20500,\n",
       " 14216,\n",
       " 10343,\n",
       " 427,\n",
       " 2213,\n",
       " 361,\n",
       " 2703,\n",
       " 530,\n",
       " 5546,\n",
       " 2592,\n",
       " 81884,\n",
       " 2703,\n",
       " 919,\n",
       " 1728,\n",
       " 17,\n",
       " 3162,\n",
       " 632,\n",
       " 267,\n",
       " 36213,\n",
       " 1776,\n",
       " 115335,\n",
       " 8876,\n",
       " 17,\n",
       " 26293,\n",
       " 22790,\n",
       " 98402,\n",
       " 17123,\n",
       " 427,\n",
       " 8265,\n",
       " 427,\n",
       " 1955,\n",
       " 3403,\n",
       " 722,\n",
       " 25739,\n",
       " 4,\n",
       " 1387,\n",
       " 8876,\n",
       " 461,\n",
       " 16401,\n",
       " 1185,\n",
       " 632,\n",
       " 1130,\n",
       " 2592,\n",
       " 42278,\n",
       " 8744,\n",
       " 2292,\n",
       " 121930,\n",
       " 17,\n",
       " 27069,\n",
       " 154878,\n",
       " 10398,\n",
       " 117124,\n",
       " 26676,\n",
       " 15,\n",
       " 368,\n",
       " 203403,\n",
       " 10968,\n",
       " 26099,\n",
       " 44181,\n",
       " 36645,\n",
       " 14652,\n",
       " 368,\n",
       " 5857,\n",
       " 15960,\n",
       " 10278,\n",
       " 2449,\n",
       " 49316,\n",
       " 386,\n",
       " 3808,\n",
       " 24520,\n",
       " 529,\n",
       " 427,\n",
       " 5268,\n",
       " 9513,\n",
       " 81,\n",
       " 4192,\n",
       " 16,\n",
       " 12192,\n",
       " 15,\n",
       " 530,\n",
       " 361,\n",
       " 267,\n",
       " 4936,\n",
       " 3172,\n",
       " 177826,\n",
       " 4676,\n",
       " 15,\n",
       " 15576,\n",
       " 427,\n",
       " 36645,\n",
       " 1002,\n",
       " 368,\n",
       " 141771,\n",
       " 58790,\n",
       " 24985,\n",
       " 3478,\n",
       " 4054,\n",
       " 17,\n",
       " 16883,\n",
       " 3866,\n",
       " 49297,\n",
       " 23151,\n",
       " 3638,\n",
       " 14216,\n",
       " 530,\n",
       " 14583,\n",
       " 63923,\n",
       " 15,\n",
       " 2131,\n",
       " 3121,\n",
       " 6610,\n",
       " 189112,\n",
       " 10082,\n",
       " 24829,\n",
       " 17,\n",
       " 1387,\n",
       " 7458,\n",
       " 361,\n",
       " 64766,\n",
       " 632,\n",
       " 247269,\n",
       " 15,\n",
       " 1965,\n",
       " 27826,\n",
       " 17,\n",
       " 27069,\n",
       " 154878,\n",
       " 10398,\n",
       " 40859,\n",
       " 368,\n",
       " 51651,\n",
       " 632,\n",
       " 30052,\n",
       " 21998,\n",
       " 1485,\n",
       " 368,\n",
       " 3968,\n",
       " 4345,\n",
       " 15,\n",
       " 718,\n",
       " 18456,\n",
       " 267,\n",
       " 10512,\n",
       " 10974,\n",
       " 84502,\n",
       " 13246,\n",
       " 613,\n",
       " 368,\n",
       " 15659,\n",
       " 28608,\n",
       " 15,\n",
       " 1965,\n",
       " 368,\n",
       " 26702,\n",
       " 1542,\n",
       " 267,\n",
       " 14488,\n",
       " 4676,\n",
       " 461,\n",
       " 52492,\n",
       " 16834,\n",
       " 28084,\n",
       " 427,\n",
       " 1152,\n",
       " 15,\n",
       " 530,\n",
       " 1152,\n",
       " 3520,\n",
       " 2256,\n",
       " 15980,\n",
       " 368,\n",
       " 15032,\n",
       " 33777,\n",
       " 15,\n",
       " 530,\n",
       " 6582,\n",
       " 32391,\n",
       " 368,\n",
       " 210195,\n",
       " 4994,\n",
       " 248567,\n",
       " 3164,\n",
       " 530,\n",
       " 10665,\n",
       " 4054,\n",
       " 6497,\n",
       " 1402,\n",
       " 17,\n",
       " 1387,\n",
       " 37362,\n",
       " 16237,\n",
       " 789,\n",
       " 632,\n",
       " 368,\n",
       " 447,\n",
       " 143054,\n",
       " 43842,\n",
       " 60420,\n",
       " 461,\n",
       " 368,\n",
       " 53484,\n",
       " 123056,\n",
       " 86,\n",
       " 15,\n",
       " 2131,\n",
       " 17398,\n",
       " 447,\n",
       " 143054,\n",
       " 530,\n",
       " 447,\n",
       " 118594,\n",
       " 15,\n",
       " 530,\n",
       " 21894,\n",
       " 447,\n",
       " 152322,\n",
       " 376,\n",
       " 17,\n",
       " 27069,\n",
       " 154878,\n",
       " 10398,\n",
       " 200100,\n",
       " 386,\n",
       " 861,\n",
       " 1119,\n",
       " 51651,\n",
       " 1620,\n",
       " 2634,\n",
       " 5845,\n",
       " 9411,\n",
       " 368,\n",
       " 6869,\n",
       " 5963,\n",
       " 461,\n",
       " 368,\n",
       " 21998,\n",
       " 7672,\n",
       " 510,\n",
       " 7508,\n",
       " 11927,\n",
       " 19502,\n",
       " 1074,\n",
       " 267,\n",
       " 54397,\n",
       " 461,\n",
       " 189508,\n",
       " 613,\n",
       " 1728,\n",
       " 27707,\n",
       " 17]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenised_data[\"train\"][3]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96629688-49f5-4249-ac99-cc1f68a2fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./data/finetuned_classifier_bloom_wEval\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    logging_steps=5,\n",
    "    learning_rate=1e-5,\n",
    "    max_grad_norm = 0.3,\n",
    "    eval_steps=0.2,\n",
    "    num_train_epochs=2,\n",
    "    warmup_ratio= 0.1,\n",
    "    # group_by_length=True,\n",
    "    fp16=False,\n",
    "    weight_decay=0.001,\n",
    "    lr_scheduler_type=\"constant\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2cba6edc-aa71-42d2-a177-c5927983d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,542,016 || all params: 1,068,859,392 || trainable%: 0.3313827830405592\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, LoraConfig(\n",
    "                            task_type=\"SEQ_CLS\",\n",
    "                            r=16,\n",
    "                            lora_alpha=16,\n",
    "                            target_modules=[\n",
    "                                'query_key_value',\n",
    "                                'dense'\n",
    "                            ],\n",
    "                            bias=\"none\",\n",
    "                            lora_dropout=0.05, # Conventional\n",
    "                        ))\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f297561e-cb18-47e2-8af6-daee4564d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForSequenceClassification(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 1536)\n",
       "        (word_embeddings_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-23): 24 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=4608, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4608, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=1536, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=1536, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b8d5dfa-d4b3-49bb-b187-0b9eba0f5ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedCELossTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # Get model's predictions\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # Compute custom loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9575c8b-8e8a-4e70-9c61-725b064397c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "02731a1e-ab0c-4d68-97fb-d0a316c5fc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    peft_model,\n",
    "    training_arguments,\n",
    "    train_dataset=tokenised_data[\"train\"],\n",
    "    eval_dataset=tokenised_data[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4929409-cc1a-471d-8f57-15257aa4567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, module in trainer.model.named_modules():\n",
    "#     if \"norm\" in name:\n",
    "#         module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b5f67bd-37d3-4921-a58e-751445bf4c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/work/wandb/run-20240103_131130-vbcv36ln</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/snoop088/huggingface/runs/vbcv36ln' target=\"_blank\">distinctive-bush-31</a></strong> to <a href='https://wandb.ai/snoop088/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/snoop088/huggingface' target=\"_blank\">https://wandb.ai/snoop088/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/snoop088/huggingface/runs/vbcv36ln' target=\"_blank\">https://wandb.ai/snoop088/huggingface/runs/vbcv36ln</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 52:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.784300</td>\n",
       "      <td>0.617776</td>\n",
       "      <td>0.875613</td>\n",
       "      <td>0.876394</td>\n",
       "      <td>0.876004</td>\n",
       "      <td>0.876444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.335400</td>\n",
       "      <td>0.378954</td>\n",
       "      <td>0.949876</td>\n",
       "      <td>0.854083</td>\n",
       "      <td>0.899436</td>\n",
       "      <td>0.904889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.119500</td>\n",
       "      <td>0.231434</td>\n",
       "      <td>0.928794</td>\n",
       "      <td>0.931281</td>\n",
       "      <td>0.930036</td>\n",
       "      <td>0.930222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.216521</td>\n",
       "      <td>0.933064</td>\n",
       "      <td>0.926818</td>\n",
       "      <td>0.929931</td>\n",
       "      <td>0.930444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.200700</td>\n",
       "      <td>0.204484</td>\n",
       "      <td>0.929732</td>\n",
       "      <td>0.944668</td>\n",
       "      <td>0.937140</td>\n",
       "      <td>0.936889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4500, training_loss=0.5262089767704408, metrics={'train_runtime': 3158.4242, 'train_samples_per_second': 22.796, 'train_steps_per_second': 1.425, 'total_flos': 8.953551858696192e+16, 'train_loss': 0.5262089767704408, 'epoch': 2.0})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "369c9eeb-447e-49a7-8408-857c28f997ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 02:34]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21896573901176453,\n",
       " 'eval_runtime': 154.7007,\n",
       " 'eval_samples_per_second': 80.801,\n",
       " 'eval_steps_per_second': 20.2,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3e7b041a-fdf1-4a14-b925-76d0fe84b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./data/finetuned_classifier_bloom_wEval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7571f560-017b-49e5-b926-c331209d3fc3",
   "metadata": {},
   "source": [
    "### Checking the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd6dbf7b-76c5-40b0-a75d-8126adfa826b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.12/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.12/site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a0766e4-c412-4a22-a050-ca103262ee41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloom-1b1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = './data/finetuned_classifier_bloom_wEval/'\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(model_name, \n",
    "                                                                  trust_remote_code=True, \n",
    "                                                                  num_labels=2,\n",
    "                                                                  device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0092178f-c456-4efe-9a71-4b1c6ab88626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a297e05a2124f7cbea8b91d6d75c688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/14.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/snoop088/imdb_tuned-bloom1b1-sentiment-classifier/commit/bedf168fdaf0a3e30c37c91012b0b0792b3f8525', commit_message='Upload BloomForSequenceClassification', commit_description='', oid='bedf168fdaf0a3e30c37c91012b0b0792b3f8525', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.push_to_hub('imdb_tuned-bloom1b1-sentiment-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa67dbec-d879-4463-9b61-afea5f43b7be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae1876f1c4e4d869fb0a57b9cbfbee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/snoop088/imdb_tuned-bloom1b1-sentiment-classifier/commit/91020a3819bd9b053ccaebec0bf88d34bfe56f38', commit_message='Upload tokenizer', commit_description='', oid='91020a3819bd9b053ccaebec0bf88d34bfe56f38', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('imdb_tuned-bloom1b1-sentiment-classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038f287c-644e-4a8a-bf62-330427a21d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26db5da3-7ed4-45c7-a060-e50be9ec2ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 1068856320 || trainable%: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c246f-793f-4a69-92c4-80d8bda8e9ad",
   "metadata": {},
   "source": [
    "#### Manually Creating a Simple DataFrame of Reviews\n",
    "\n",
    "I can later try and create an app that will take in dates, sort method and try and scrape reviews from IMDB to create movie recommendation lists.\n",
    "\n",
    "But lets test the model first with manual list of 3-5 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4256b75-9388-4ecf-a398-5c740cea7190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# load data using Python JSON module\n",
    "with open('./data/manual_movies.json','r') as f:\n",
    "    data = json.loads(f.read())\n",
    "df_manual = pd.json_normalize(data, record_path=['movies'])\n",
    "df_manual.to_csv('./data/df_manual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c214102-71db-42c5-b95a-389313d2a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_set = pd.read_csv(\"./data/df_manual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c3ff385-72d0-4f87-97c0-bb49453b17f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# input = tokenizer(df_small[\"review\"], return_tensors=\"pt\")\n",
    "# output = loaded_model(**input)\n",
    "inputs = tokenizer(list(df_manual[\"review\"]), truncation=True, padding=\"max_length\", max_length=256,  return_tensors=\"pt\")\n",
    "outputs = loaded_model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f17d03e3-cfb9-4369-9eb5-cfee339930cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(outputs.logits, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af66aa25-e3d0-4d75-bf48-ae4f183068d2",
   "metadata": {},
   "source": [
    "### Time to load scraped data from IMDB\n",
    "\n",
    "Lets create a test with real IMDB data. We should load the data and extract the reviews together with the movie title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40986bd0-bed2-41ac-b6be-5c4b43f877ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('./data/scraped_01-03_01-11.complete.json','r') as f:\n",
    "    movie_data = json.loads(f.read())\n",
    "loaded_movies = movie_data[\"movies\"]\n",
    "# for movie in loaded_movies:\n",
    "#     review_texts = []\n",
    "#     for review in movie[\"reviews\"]:\n",
    "#         review_texts.append(review[\"copy\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877208f6-4f86-4f5d-93ff-fcf3596b5707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "      <th>link</th>\n",
       "      <th>meta</th>\n",
       "      <th>votes</th>\n",
       "      <th>type</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/reviews</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>{'copy': 'There seems to be a trend these days...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/reviews</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>{'copy': 'I tried hard not to just compare #2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/reviews</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>{'copy': 'The film offers a thrilling experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/reviews</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>{'copy': 'Now as i watched the movie i truly t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/reviews</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>{'copy': 'Didnt get the same feeling I got bac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  stars                                          link meta  \\\n",
       "0  Gladiator II    6.9  https://www.imdb.com/title/tt9218128/reviews   64   \n",
       "0  Gladiator II    6.9  https://www.imdb.com/title/tt9218128/reviews   64   \n",
       "0  Gladiator II    6.9  https://www.imdb.com/title/tt9218128/reviews   64   \n",
       "0  Gladiator II    6.9  https://www.imdb.com/title/tt9218128/reviews   64   \n",
       "0  Gladiator II    6.9  https://www.imdb.com/title/tt9218128/reviews   64   \n",
       "\n",
       "  votes   type                                            reviews  \n",
       "0   78K  Movie  {'copy': 'There seems to be a trend these days...  \n",
       "0   78K  Movie  {'copy': 'I tried hard not to just compare #2 ...  \n",
       "0   78K  Movie  {'copy': 'The film offers a thrilling experien...  \n",
       "0   78K  Movie  {'copy': 'Now as i watched the movie i truly t...  \n",
       "0   78K  Movie  {'copy': 'Didnt get the same feeling I got bac...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(loaded_movies)\n",
    "df_exploded = df.explode([\"reviews\"])\n",
    "df_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd1539ef-f447-4f61-b37e-53dff047cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(movie):\n",
    "    reviews = movie[\"reviews\"]\n",
    "    review_copy, review_stars = zip(*map(lambda review: (review[\"copy\"], review[\"stars\"]), reviews))\n",
    "    return {\n",
    "        \"title\": movie[\"title\"],\n",
    "        \"review_copy\": list(review_copy),\n",
    "        \"review_stars\": list(review_stars)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9b94181-0fe1-46e9-a43b-cc9044d8d2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating(movie, model, tokenizer, device):\n",
    "    outputs = []\n",
    "    reviews = [review[\"copy\"] for review in movie[\"reviews\"]]\n",
    "    stars = [review[\"stars\"] for review in movie[\"reviews\"]]\n",
    "    total = len(reviews)\n",
    "    inputs = tokenizer(reviews, \n",
    "                       truncation=True, \n",
    "                       padding=\"max_length\", \n",
    "                       max_length=256,  \n",
    "                       return_tensors=\"pt\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs)\n",
    "    total_sentiment = np.argmax(torch.Tensor.cpu(outputs.logits), axis=-1).sum()\n",
    "    return {\"total_sentiment\": f\"{total_sentiment.item()} / {total}\", \n",
    "            \"total_stars\": f\"{np.array(stars).sum()} / {total * 10}\"\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b07fd13-9ca0-445d-8b49-d513ac84489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "222cd233-b4dc-48b6-9fcd-6921e77a6e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 23.54 GiB of which 1.41 GiB is free. Process 1693599 has 21.06 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 28.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(processed_reviews, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,  return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mloaded_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m outcome \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m.\u001b[39mcpu(outputs\u001b[38;5;241m.\u001b[39mlogits), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m sentiment \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m outcome]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:1102\u001b[0m, in \u001b[0;36mBloomForSequenceClassification.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, **deprecated_arguments)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unexpected arguments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdeprecated_arguments\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1100\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1102\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1115\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:693\u001b[0m, in \u001b[0;36mBloomModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, head_mask, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **deprecated_arguments)\u001b[0m\n\u001b[1;32m    681\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    682\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    683\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m         cache_position,\n\u001b[1;32m    691\u001b[0m     )\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 693\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43malibi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malibi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:426\u001b[0m, in \u001b[0;36mBloomBlock.forward\u001b[0;34m(self, hidden_states, alibi, attention_mask, layer_past, head_mask, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    423\u001b[0m     residual \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# MLP.\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayernorm_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n\u001b[1;32m    429\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (output,) \u001b[38;5;241m+\u001b[39m outputs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:345\u001b[0m, in \u001b[0;36mBloomMLP.forward\u001b[0;34m(self, hidden_states, residual)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, residual: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 345\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense_h_to_4h\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_but_exact:\n\u001b[1;32m    348\u001b[0m         intermediate_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(residual)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:171\u001b[0m, in \u001b[0;36mBloomGelu.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeLUFunction\u001b[38;5;241m.\u001b[39mapply(x)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbloom_gelu_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/transformers/models/bloom/modeling_bloom.py:121\u001b[0m, in \u001b[0;36mbloom_gelu_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbloom_gelu_forward\u001b[39m(x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;03m    Custom bias GELU function. Adapted from Megatron-DeepSpeed code. Here we use a simple implementation (inference) to\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    make the model jitable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m            input hidden states\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;241m0.79788456\u001b[39m \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241;43m0.044715\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 23.54 GiB of which 1.41 GiB is free. Process 1693599 has 21.06 GiB memory in use. Of the allocated memory 20.58 GiB is allocated by PyTorch, and 28.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "processed = list(map(process, loaded_movies))\n",
    "processed_reviews = []\n",
    "movies_with_sentiment = []\n",
    "for processed_movie in processed:\n",
    "    processed_reviews.extend(processed_movie[\"review_copy\"])\n",
    "\n",
    "inputs = tokenizer(processed_reviews, truncation=True, padding=\"max_length\", max_length=256,  return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.inference_mode():\n",
    "    outputs = loaded_model(**inputs)\n",
    "outcome = np.argmax(torch.Tensor.cpu(outputs.logits), axis=-1)\n",
    "sentiment = ['positive' if out == 1 else 'negative' for out in outcome]\n",
    "# for (i,item) in enumerate(sentiment):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ae247dc-7843-4814-b75d-89fbccc6d134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_sentiment': '3 / 15', 'total_stars': '60 / 150'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rating(loaded_movies[2], loaded_model, tokenizer, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "243ac59b-c2ba-4d71-812b-e834eb0dd843",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = [{\"title\": movie[\"title\"],\n",
    "           \"stars\": movie[\"stars\"], \n",
    "           \"meta\": movie[\"meta\"],\n",
    "           \"votes\": movie[\"votes\"],\n",
    "           \"type\": movie[\"type\"],\n",
    "           \"link\": movie[\"link\"].split(\"review\")[0],\n",
    "           **get_rating(movie, loaded_model, tokenizer, DEVICE)} for movie in loaded_movies]\n",
    "movies_df = pd.DataFrame(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51de7b6a-b18c-4f5c-92b1-11d494cb9788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "      <th>meta</th>\n",
       "      <th>votes</th>\n",
       "      <th>type</th>\n",
       "      <th>link</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>total_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gladiator II</td>\n",
       "      <td>6.9</td>\n",
       "      <td>64</td>\n",
       "      <td>78K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt9218128/</td>\n",
       "      <td>3 / 10</td>\n",
       "      <td>58 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dune: Prophecy</td>\n",
       "      <td>7.4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>9.6K</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt10466872/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>65 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Penguin</td>\n",
       "      <td>8.7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>123K</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt15435876/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>95 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deadpool &amp; Wolverine</td>\n",
       "      <td>7.7</td>\n",
       "      <td>56</td>\n",
       "      <td>400K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt6263850/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>79 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Substance</td>\n",
       "      <td>7.4</td>\n",
       "      <td>78</td>\n",
       "      <td>148K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt17526714/</td>\n",
       "      <td>4 / 10</td>\n",
       "      <td>71 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Twisters</td>\n",
       "      <td>6.5</td>\n",
       "      <td>65</td>\n",
       "      <td>130K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt12584954/</td>\n",
       "      <td>5 / 10</td>\n",
       "      <td>57 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Heretic</td>\n",
       "      <td>7.2</td>\n",
       "      <td>71</td>\n",
       "      <td>19K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt28015403/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>76 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Smile 2</td>\n",
       "      <td>6.9</td>\n",
       "      <td>66</td>\n",
       "      <td>52K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt29268110/</td>\n",
       "      <td>6 / 10</td>\n",
       "      <td>66 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Anora</td>\n",
       "      <td>8.2</td>\n",
       "      <td>91</td>\n",
       "      <td>23K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt28607951/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>85 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Disclaimer</td>\n",
       "      <td>7.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>18K</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt16294384/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>68 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alien: Romulus</td>\n",
       "      <td>7.2</td>\n",
       "      <td>64</td>\n",
       "      <td>175K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt18412256/</td>\n",
       "      <td>4 / 10</td>\n",
       "      <td>59 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dandadan</td>\n",
       "      <td>8.7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>12K</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt30217403/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>100 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Rivals</td>\n",
       "      <td>8.0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>9.1K</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt21906238/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>88 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Emilia Pérez</td>\n",
       "      <td>6.9</td>\n",
       "      <td>71</td>\n",
       "      <td>11K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt20221436/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>75 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Venom: The Last Dance</td>\n",
       "      <td>6.2</td>\n",
       "      <td>41</td>\n",
       "      <td>45K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt16366836/</td>\n",
       "      <td>6 / 10</td>\n",
       "      <td>60 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Terrifier 3</td>\n",
       "      <td>6.4</td>\n",
       "      <td>61</td>\n",
       "      <td>35K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27911000/</td>\n",
       "      <td>5 / 10</td>\n",
       "      <td>62 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Wild Robot</td>\n",
       "      <td>8.3</td>\n",
       "      <td>85</td>\n",
       "      <td>88K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt29623480/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>93 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Megalopolis</td>\n",
       "      <td>4.9</td>\n",
       "      <td>55</td>\n",
       "      <td>25K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt10128846/</td>\n",
       "      <td>3 / 10</td>\n",
       "      <td>46 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>High Potential</td>\n",
       "      <td>7.7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>9.9K</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt26748649/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>81 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Conclave</td>\n",
       "      <td>7.5</td>\n",
       "      <td>79</td>\n",
       "      <td>17K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt20215234/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>81 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Blitz</td>\n",
       "      <td>6.3</td>\n",
       "      <td>71</td>\n",
       "      <td>6.7K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt15939198/</td>\n",
       "      <td>3 / 10</td>\n",
       "      <td>50 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saturday Night</td>\n",
       "      <td>7.1</td>\n",
       "      <td>63</td>\n",
       "      <td>11K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27657135/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>64 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Doctor Odyssey</td>\n",
       "      <td>5.9</td>\n",
       "      <td>N/A</td>\n",
       "      <td>4K</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt31849826/</td>\n",
       "      <td>5 / 10</td>\n",
       "      <td>50 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Agatha All Along</td>\n",
       "      <td>7.2</td>\n",
       "      <td>N/A</td>\n",
       "      <td>62K</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt15571732/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>74 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>We Live in Time</td>\n",
       "      <td>7.1</td>\n",
       "      <td>58</td>\n",
       "      <td>14K</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27131358/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>72 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Babygirl</td>\n",
       "      <td>6.0</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt30057084/</td>\n",
       "      <td>2 / 5</td>\n",
       "      <td>25 / 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Transformers One</td>\n",
       "      <td>7.6</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt8864596/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>86 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Moonflower Murders</td>\n",
       "      <td>7.4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt26314987/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>80 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The Last Showgirl</td>\n",
       "      <td>5.9</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt31193791/</td>\n",
       "      <td>2 / 3</td>\n",
       "      <td>22 / 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Joker: Folie à Deux</td>\n",
       "      <td>5.2</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt11315808/</td>\n",
       "      <td>1 / 10</td>\n",
       "      <td>39 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Nobody Wants This</td>\n",
       "      <td>7.9</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt26933824/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>86 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Matlock</td>\n",
       "      <td>7.7</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt26591147/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>81 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Absolution</td>\n",
       "      <td>5.2</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt8337290/</td>\n",
       "      <td>4 / 10</td>\n",
       "      <td>44 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Apocalypse Z: The Beginning of the End</td>\n",
       "      <td>6.1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27599851/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>61 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Piano Lesson</td>\n",
       "      <td>6.2</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt15507512/</td>\n",
       "      <td>7 / 10</td>\n",
       "      <td>61 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Trap</td>\n",
       "      <td>5.9</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt26753003/</td>\n",
       "      <td>1 / 10</td>\n",
       "      <td>44 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Apprentice</td>\n",
       "      <td>7.1</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt8368368/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>73 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Fallout</td>\n",
       "      <td>8.4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt12637874/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>87 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Longlegs</td>\n",
       "      <td>6.7</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt23468450/</td>\n",
       "      <td>3 / 10</td>\n",
       "      <td>56 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Juror #2</td>\n",
       "      <td>7.3</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27403986/</td>\n",
       "      <td>10 / 10</td>\n",
       "      <td>77 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>The Perfect Couple</td>\n",
       "      <td>6.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt11514868/</td>\n",
       "      <td>4 / 10</td>\n",
       "      <td>62 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Beetlejuice Beetlejuice</td>\n",
       "      <td>6.8</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt2049403/</td>\n",
       "      <td>3 / 10</td>\n",
       "      <td>57 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>The Count of Monte-Cristo</td>\n",
       "      <td>7.7</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt26446278/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>78 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Blink Twice</td>\n",
       "      <td>6.5</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt14858658/</td>\n",
       "      <td>6 / 10</td>\n",
       "      <td>56 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Speak No Evil</td>\n",
       "      <td>6.9</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27534307/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>62 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>It Ends with Us</td>\n",
       "      <td>6.5</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt10655524/</td>\n",
       "      <td>4 / 10</td>\n",
       "      <td>55 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Murder in a Small Town</td>\n",
       "      <td>6.8</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt12408718/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>72 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Armor</td>\n",
       "      <td>3.5</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt29252358/</td>\n",
       "      <td>1 / 10</td>\n",
       "      <td>25 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Abigail</td>\n",
       "      <td>6.6</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt27489557/</td>\n",
       "      <td>8 / 10</td>\n",
       "      <td>63 / 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Until I Kill You</td>\n",
       "      <td>7.4</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>TV Series</td>\n",
       "      <td>https://www.imdb.com/title/tt26738706/</td>\n",
       "      <td>9 / 10</td>\n",
       "      <td>84 / 100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  stars meta votes            type  \\\n",
       "0                             Gladiator II    6.9   64   78K           Movie   \n",
       "1                           Dune: Prophecy    7.4  N/A  9.6K       TV Series   \n",
       "2                              The Penguin    8.7  N/A  123K  TV Mini Series   \n",
       "3                     Deadpool & Wolverine    7.7   56  400K           Movie   \n",
       "4                            The Substance    7.4   78  148K           Movie   \n",
       "5                                 Twisters    6.5   65  130K           Movie   \n",
       "6                                  Heretic    7.2   71   19K           Movie   \n",
       "7                                  Smile 2    6.9   66   52K           Movie   \n",
       "8                                    Anora    8.2   91   23K           Movie   \n",
       "9                               Disclaimer    7.5  N/A   18K  TV Mini Series   \n",
       "10                          Alien: Romulus    7.2   64  175K           Movie   \n",
       "11                                Dandadan    8.7  N/A   12K       TV Series   \n",
       "12                                  Rivals    8.0  N/A  9.1K       TV Series   \n",
       "13                            Emilia Pérez    6.9   71   11K           Movie   \n",
       "14                   Venom: The Last Dance    6.2   41   45K           Movie   \n",
       "15                             Terrifier 3    6.4   61   35K           Movie   \n",
       "16                          The Wild Robot    8.3   85   88K           Movie   \n",
       "17                             Megalopolis    4.9   55   25K           Movie   \n",
       "18                          High Potential    7.7  N/A  9.9K       TV Series   \n",
       "19                                Conclave    7.5   79   17K           Movie   \n",
       "20                                   Blitz    6.3   71  6.7K           Movie   \n",
       "21                          Saturday Night    7.1   63   11K           Movie   \n",
       "22                          Doctor Odyssey    5.9  N/A    4K       TV Series   \n",
       "23                        Agatha All Along    7.2  N/A   62K  TV Mini Series   \n",
       "24                         We Live in Time    7.1   58   14K           Movie   \n",
       "25                                Babygirl    6.0   82     0           Movie   \n",
       "26                        Transformers One    7.6   64     0           Movie   \n",
       "27                      Moonflower Murders    7.4  N/A     0       TV Series   \n",
       "28                       The Last Showgirl    5.9   71     0           Movie   \n",
       "29                     Joker: Folie à Deux    5.2   45     0           Movie   \n",
       "30                       Nobody Wants This    7.9  N/A     0       TV Series   \n",
       "31                                 Matlock    7.7  N/A     0       TV Series   \n",
       "32                              Absolution    5.2   51     0           Movie   \n",
       "33  Apocalypse Z: The Beginning of the End    6.1  N/A     0           Movie   \n",
       "34                        The Piano Lesson    6.2   69     0           Movie   \n",
       "35                                    Trap    5.9   52     0           Movie   \n",
       "36                          The Apprentice    7.1   64     0           Movie   \n",
       "37                                 Fallout    8.4  N/A     0       TV Series   \n",
       "38                                Longlegs    6.7   77     0           Movie   \n",
       "39                                Juror #2    7.3   73     0           Movie   \n",
       "40                      The Perfect Couple    6.5  N/A     0  TV Mini Series   \n",
       "41                 Beetlejuice Beetlejuice    6.8   62     0           Movie   \n",
       "42               The Count of Monte-Cristo    7.7   66     0           Movie   \n",
       "43                             Blink Twice    6.5   66     0           Movie   \n",
       "44                           Speak No Evil    6.9   66     0           Movie   \n",
       "45                         It Ends with Us    6.5   53     0           Movie   \n",
       "46                  Murder in a Small Town    6.8  N/A     0       TV Series   \n",
       "47                                   Armor    3.5  N/A     0           Movie   \n",
       "48                                 Abigail    6.6   62     0           Movie   \n",
       "49                        Until I Kill You    7.4  N/A     0       TV Series   \n",
       "\n",
       "                                      link total_sentiment total_stars  \n",
       "0    https://www.imdb.com/title/tt9218128/          3 / 10    58 / 100  \n",
       "1   https://www.imdb.com/title/tt10466872/          7 / 10    65 / 100  \n",
       "2   https://www.imdb.com/title/tt15435876/         10 / 10    95 / 100  \n",
       "3    https://www.imdb.com/title/tt6263850/          7 / 10    79 / 100  \n",
       "4   https://www.imdb.com/title/tt17526714/          4 / 10    71 / 100  \n",
       "5   https://www.imdb.com/title/tt12584954/          5 / 10    57 / 100  \n",
       "6   https://www.imdb.com/title/tt28015403/         10 / 10    76 / 100  \n",
       "7   https://www.imdb.com/title/tt29268110/          6 / 10    66 / 100  \n",
       "8   https://www.imdb.com/title/tt28607951/          9 / 10    85 / 100  \n",
       "9   https://www.imdb.com/title/tt16294384/          7 / 10    68 / 100  \n",
       "10  https://www.imdb.com/title/tt18412256/          4 / 10    59 / 100  \n",
       "11  https://www.imdb.com/title/tt30217403/         10 / 10   100 / 100  \n",
       "12  https://www.imdb.com/title/tt21906238/          9 / 10    88 / 100  \n",
       "13  https://www.imdb.com/title/tt20221436/          9 / 10    75 / 100  \n",
       "14  https://www.imdb.com/title/tt16366836/          6 / 10    60 / 100  \n",
       "15  https://www.imdb.com/title/tt27911000/          5 / 10    62 / 100  \n",
       "16  https://www.imdb.com/title/tt29623480/         10 / 10    93 / 100  \n",
       "17  https://www.imdb.com/title/tt10128846/          3 / 10    46 / 100  \n",
       "18  https://www.imdb.com/title/tt26748649/          9 / 10    81 / 100  \n",
       "19  https://www.imdb.com/title/tt20215234/          8 / 10    81 / 100  \n",
       "20  https://www.imdb.com/title/tt15939198/          3 / 10    50 / 100  \n",
       "21  https://www.imdb.com/title/tt27657135/          8 / 10    64 / 100  \n",
       "22  https://www.imdb.com/title/tt31849826/          5 / 10    50 / 100  \n",
       "23  https://www.imdb.com/title/tt15571732/          8 / 10    74 / 100  \n",
       "24  https://www.imdb.com/title/tt27131358/          7 / 10    72 / 100  \n",
       "25  https://www.imdb.com/title/tt30057084/           2 / 5     25 / 50  \n",
       "26   https://www.imdb.com/title/tt8864596/         10 / 10    86 / 100  \n",
       "27  https://www.imdb.com/title/tt26314987/          7 / 10    80 / 100  \n",
       "28  https://www.imdb.com/title/tt31193791/           2 / 3     22 / 30  \n",
       "29  https://www.imdb.com/title/tt11315808/          1 / 10    39 / 100  \n",
       "30  https://www.imdb.com/title/tt26933824/          9 / 10    86 / 100  \n",
       "31  https://www.imdb.com/title/tt26591147/          9 / 10    81 / 100  \n",
       "32   https://www.imdb.com/title/tt8337290/          4 / 10    44 / 100  \n",
       "33  https://www.imdb.com/title/tt27599851/          7 / 10    61 / 100  \n",
       "34  https://www.imdb.com/title/tt15507512/          7 / 10    61 / 100  \n",
       "35  https://www.imdb.com/title/tt26753003/          1 / 10    44 / 100  \n",
       "36   https://www.imdb.com/title/tt8368368/         10 / 10    73 / 100  \n",
       "37  https://www.imdb.com/title/tt12637874/         10 / 10    87 / 100  \n",
       "38  https://www.imdb.com/title/tt23468450/          3 / 10    56 / 100  \n",
       "39  https://www.imdb.com/title/tt27403986/         10 / 10    77 / 100  \n",
       "40  https://www.imdb.com/title/tt11514868/          4 / 10    62 / 100  \n",
       "41   https://www.imdb.com/title/tt2049403/          3 / 10    57 / 100  \n",
       "42  https://www.imdb.com/title/tt26446278/          9 / 10    78 / 100  \n",
       "43  https://www.imdb.com/title/tt14858658/          6 / 10    56 / 100  \n",
       "44  https://www.imdb.com/title/tt27534307/          8 / 10    62 / 100  \n",
       "45  https://www.imdb.com/title/tt10655524/          4 / 10    55 / 100  \n",
       "46  https://www.imdb.com/title/tt12408718/          8 / 10    72 / 100  \n",
       "47  https://www.imdb.com/title/tt29252358/          1 / 10    25 / 100  \n",
       "48  https://www.imdb.com/title/tt27489557/          8 / 10    63 / 100  \n",
       "49  https://www.imdb.com/title/tt26738706/          9 / 10    84 / 100  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce28ca41-ac61-4cfd-9ee2-a517f2174213",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df.to_csv('./data/movies_sentiment_01.03-01.11.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
