{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c864c9e2-fd9b-4c5f-8e55-bdb819bf4054",
   "metadata": {},
   "source": [
    "# Exploring the capabilities of Langchain's Agents\n",
    "\n",
    "## The Search Agents\n",
    "\n",
    "Trying out what we can achieve with Search agents and RAG combined. Lets try follwing the Langchain Agents [Quickstart](https://python.langchain.com/docs/modules/agents/quick_start/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12830b13-7c2f-4700-9a55-b73e5325930a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.126.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /opt/conda/lib/python3.11/site-packages (from google-api-python-client) (2.28.2)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.23.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Downloading google_api_python_client-2.126.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, proto-plus, httplib2, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.18.0 google-api-python-client-2.126.0 google-auth-httplib2-0.2.0 httplib2-0.22.0 proto-plus-1.23.0 uritemplate-4.1.1\n",
      "Collecting langchain-openai\n",
      "  Downloading langchain_openai-0.1.3-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.42 (from langchain-openai)\n",
      "  Downloading langchain_core-0.1.43-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (1.14.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in /opt/conda/lib/python3.11/site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (0.1.27)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.1.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai) (3.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.42->langchain-openai) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.42->langchain-openai) (2.16.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai) (1.26.18)\n",
      "Downloading langchain_openai-0.1.3-py3-none-any.whl (33 kB)\n",
      "Downloading langchain_core-0.1.43-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.1/289.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.1.32\n",
      "    Uninstalling langchain-core-0.1.32:\n",
      "      Successfully uninstalled langchain-core-0.1.32\n",
      "Successfully installed langchain-core-0.1.43 langchain-openai-0.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -U google-api-python-client\n",
    "!pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c63413-ecdc-49d1-92d7-5a770d37837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e348b04-9c3d-457e-a608-89a4a7d3f67a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import load_tools, initialize_agent, AgentType, AgentExecutor\n",
    "from langchain import hub\n",
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbab48-4403-40ef-bd90-1b42c8dd7e8b",
   "metadata": {},
   "source": [
    "### Creating the Google search agent\n",
    "\n",
    "- using the OpenAI ChatGPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31ee1588-ef70-417e-9276-b8990b91653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "google_search = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results. Use this tool to get up to date information on any topic.\",\n",
    "    func=search.run,\n",
    ")\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f47c1bd2-f9c6-4ea2-b271-3bfdb55bd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the prompt to use - you can modify this!\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcc6cfaf-420f-46ed-af84-1a7c73aeec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, [google_search], prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1182e00d-c903-45f9-b61c-dc95d99a6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=[google_search], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf0cf493-c7e9-4975-adf9-6a01ae059a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `google_search` with `next flight from Sofia to Amsterdam`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mIt's easy to use Google Flights to find the cheapest days to fly from Sofia to Amsterdam. Just click the Departure label near the top of the page to open the ... Find flights to Amsterdam from $70. Fly from Sofia on Air Serbia, Austrian Airlines, Bulgaria Air and more. Search for Amsterdam flights on KAYAK now to ... Flights from Amsterdam to Sofia. Use Google Flights to plan your next trip and find cheap one way or round trip flights from Amsterdam to Sofia. Find flights to Sofia from $117. Fly from Amsterdam on Air Serbia, Bulgaria Air, Norwegian and more. Search for Sofia flights on KAYAK now to find the best ... Flights between Sofia, Bulgaria and Amsterdam, Netherlands starting at £61. Choose between Wizz Air, easyJet, or KLM Royal Dutch Airlines to find the best ... Book one-way or return flights from Sofia to Amsterdam with no change fee on selected flights. Earn your airline miles on top of our rewards! Round-trip flight with Wizz Air and easyJet.Outbound indirect flight with Wizz Air, departing from Sofia on Wed, May 22, arriving in Amsterdam Schiphol.Inbound ... Useful information for your next trip. Travel ID. Unlimited access to Lufthansa Group Airlines and Miles & More ... Return flight with Wizz Air and easyJet.Outbound indirect flight with Wizz Air, departs from Sofia on Tue, 21 May, arriving in Amsterdam Schiphol.Inbound ... Cheap Flights from Amsterdam to Sofia (AMS-SOF). Prices were available within the past 7 days and start at $134 for one-way flights and $229 for round trip, for ...\u001b[0m\u001b[32;1m\u001b[1;3mThe next flight from Sofia to Amsterdam is available on airlines such as Air Serbia, Austrian Airlines, and Bulgaria Air. You can find more details and book your flight on platforms like KAYAK.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = agent_executor.invoke({\"input\": \"What time is the next flight from Sofia to Amsterdam and which airlines?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7522ab18-a5e6-4f75-a8ec-7b5c1989e6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next flight from Sofia to Amsterdam is available on airlines such as Air Serbia, Austrian Airlines, and Bulgaria Air. You can find more details and book your flight on platforms like KAYAK.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c259e86a-5432-41f5-bfd8-cb21f67b105c",
   "metadata": {},
   "source": [
    "- using an open LLM in this case Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9060a8-b06b-45f0-896f-0cdde06105a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "import torch\n",
    "model_name = \"../ext_models/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ac6a0f-aca1-4399-94b7-2b50e96b1070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b8909aa198d438b97df0bcce4881bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3bae522-e816-4d7f-8e31-de6ec7638b24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.01\n",
    "generation_config.top_p = 0.9\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "open_llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d14639f4-5e0f-4ef1-99d6-58c7a1256260",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = GoogleSearchAPIWrapper()\n",
    "\n",
    "google_search = Tool(\n",
    "    name=\"google_search\",\n",
    "    description=\"Search Google for recent results. Use this tool to get up to date information on any topic.\",\n",
    "    func=search.run,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc50375a-8d35-4350-9e05-79118be54617",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "            [tool], \n",
    "            open_llm, \n",
    "            agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            handle_parsing_errors=True,\n",
    "            verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64cf2959-62f6-4a26-bab9-edbc5713738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I should search Google to find information on how to add a custom prompt template to the Hugging Face Transformer pipeline.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"google_search\",\n",
      "  \"action_input\": \"How to add custom prompt template to Hugging Face Transformer pipeline\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOct 18, 2023 ... ... Hugging Face with the transformers.pipeline ... prompt to instruct the model to use ... prompt format into the code, and change some parameters in ... Hugging Face's logo Hugging Face ... How to add a pipeline to Transformers? Testing ... examples of the custom chat prompt template also make use of this format. Nov 14, 2023 ... We are going be using Hugging Face to load our quantized Mistral-7B model. ... text_generation_pipeline = transformers.pipeline( ... # Create prompt ... Hugging Face's logo Hugging Face ... examples Community resources Custom Tools and Prompts ... pipeline, let's explore how you can use prompts to solve NLP tasks. Jun 10, 2023 ... Please post the full stacktrace of the error Pipeline cannot infer suitable model classes from when trying to use it along with with langchain ... Hugging Face's logo Hugging Face ... examples Community resources Custom Tools and Prompts Troubleshoot Contribute new quantization method ... How to add a pipeline ... Sep 29, 2023 ... answer_template = \"\"\"{response}\"\"\" # creating function to add keys in the dictionary for prompt, answer and whole text def _add_text(rec): When using chat templates as input for model generation, it's also a good idea to use add_generation_prompt=True to add a generation prompt. Here's an example ... Jul 27, 2023 ... ... Hugging Face hub come with an implicit specific prompt template. ... add prompt templates, I ... transformers import pipeline, Conversation ... Jul 18, 2023 ... Why Llama 2? Demo; Inference. With Transformers; With Inference Endpoints. Fine-tuning with PEFT; How to Prompt Llama 2; Additional Resources ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: I have found information on how to add a custom prompt template to the Hugging Face Transformer pipeline.\n",
      "Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will need to refine my search query to find a more specific and actionable answer on how to add a custom prompt template to the Hugging Face Transformer pipeline.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"google_search\",\n",
      "  \"action_input\": \"How to add custom prompt template to Hugging Face Transformer pipeline site:huggingface.co\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mOct 18, 2023 ... ... Hugging Face with the transformers.pipeline ... prompt to instruct the model to use ... prompt format into the code, and change some parameters in ... Hugging Face's logo Hugging Face ... How to add a pipeline to Transformers? Testing ... examples of the custom chat prompt template also make use of this format. Hugging Face's logo Hugging Face ... examples Community resources Custom Tools and Prompts ... pipeline, let's explore how you can use prompts to solve NLP tasks. >>> from transformers import pipeline >>> transcriber = pipeline(model=\"openai/whisper-base\") >>> transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/ ... Chat templates are part of the tokenizer. They specify how to convert conversations, represented as lists of messages, into a single tokenizable string in the ... Jul 26, 2023 ... Context: I am trying to query Llama-2 7B, taken from HuggingFace (meta-llama/Llama-2-7b-hf). · Note about the “context”: · Example Problem: Aug 30, 2023 ... I wanted to use a Llama 2 model in my project and the thing that made it better than ChatGpt for me was that you could change the model's ... Jul 5, 2022 ... ... prompt template and add 1 so you don't hardcode it. from transformers import StoppingCriteriaList stopping_criteria = StoppingCriteriaList ... Jul 18, 2023 ... Why Llama 2? Demo; Inference. With Transformers; With Inference Endpoints. Fine-tuning with PEFT; How to Prompt Llama 2; Additional Resources ... Hugging Face's logo Hugging Face ... examples Community resources Custom Tools and Prompts Troubleshoot Contribute new quantization method ... How to add a pipeline ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: I could not find a specific and actionable answer on how to add a custom prompt template to the Hugging Face Transformer pipeline.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I could not find a specific and actionable answer on how to add a custom prompt template to the Hugging Face Transformer pipeline.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"<s> [INST] How to add custom prompt template to huggingface transformer pipeline? [/INST]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da31c37-635f-46a0-9164-3b911b5b526b",
   "metadata": {},
   "source": [
    "> So it seems that the open LLM does not search correctly for some reason. We need to research futher..\n",
    "- After some additional research, it seems we need to somehow import the Mistral Template for it to work properly\n",
    "  - Quickhack is to just include it in the prompt\n",
    "- pipeline does not allow us to customise the prompt\n",
    "- we need to extend LLM by creating CustomLLM class as specified [here](https://medium.com/@jorgepardoserrano/building-a-langchain-agent-with-a-self-hosted-mistral-7b-a-step-by-step-guide-85eda2fbf6c2):\n",
    "- OR what if we try and create the agent via create_tool_calling_agent(llm, tools, prompt), which has a prompt variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e85fea8-ea30-4bc8-b21d-c32d72dd3a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchainhub\n",
      "  Downloading langchainhub-0.1.15-py3-none-any.whl.metadata (621 bytes)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchainhub) (2.31.0)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Downloading types_requests-2.31.0.20240406-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchainhub)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Downloading langchainhub-0.1.15-py3-none-any.whl (4.6 kB)\n",
      "Downloading types_requests-2.31.0.20240406-py3-none-any.whl (15 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: urllib3, types-requests, langchainhub\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unstructured 0.12.6 requires urllib3==1.26.18, but you have urllib3 2.2.1 which is incompatible.\n",
      "botocore 1.34.51 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchainhub-0.1.15 types-requests-2.31.0.20240406 urllib3-2.2.1\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (0.1.12)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.33)\n",
      "Collecting langchain-community<0.1,>=0.0.32 (from langchain)\n",
      "  Downloading langchain_community-0.0.33-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.42 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.43)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.42->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Downloading langchain-0.1.16-py3-none-any.whl (817 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading langchain_community-0.0.33-py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-community, langchain\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.28\n",
      "    Uninstalling langchain-community-0.0.28:\n",
      "      Successfully uninstalled langchain-community-0.0.28\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.12\n",
      "    Uninstalling langchain-0.1.12:\n",
      "      Successfully uninstalled langchain-0.1.12\n",
      "Successfully installed langchain-0.1.16 langchain-community-0.0.33\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.11/site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.3/378.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.0\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=731ed8709526b2ccc1d9d9a844630f8b32c071fa1ea6e41ab4aa57e003641ad4\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchainhub\n",
    "!pip install -U langchain\n",
    "!pip install numexpr\n",
    "!pip install -U wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598a9e39-ea55-4104-894e-d68654c696d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent, create_json_chat_agent, load_tools\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.llms.base import LLM\n",
    "from transformers.models.mistral.modeling_mistral import MistralForCausalLM\n",
    "from transformers.models.llama.tokenization_llama_fast import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99bff497-1a19-40ba-9e8f-f847e5fd4605",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "open_llm_template = \"\"\"<s>[INST]You are designed to solve tasks using tools.\n",
    "\n",
    "These are the tools you can use: {tool_names}.\n",
    "\n",
    "These are the tools descriptions:\n",
    "\n",
    "{tools}\n",
    "\n",
    "If you have enough information to answer the query use the tool \"Final Answer\". Its parameters is the solution.\n",
    "If there is not enough information, keep trying.\n",
    "\n",
    "This is my query=\"{input}\".\n",
    "{agent_scratchpad}\n",
    "[/INST]\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=open_llm_template, \n",
    "                               input_variables=[\"input\", \"tools\", \"tool_names\", \"agent_scratchpad\"]\n",
    "                       )\n",
    "# from langchain import hub\n",
    "# prompt = hub.pull(\"hwchase17/react-chat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa97a86-0cbf-4aa1-b790-28dffb19ec97",
   "metadata": {},
   "source": [
    "#### Lets try and define a CustomLLM\n",
    "\n",
    "> Note: Using the pipeline as llm is not sufficient to work with the custom prompts below.\n",
    "> Taken from this [notebook](https://colab.research.google.com/drive/1e5gJaUtGVvzJP_Nr-sBEL4J2JubC5YrE) from the tutorial listed above for creating CustomLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b0f003-0d59-4808-ad9a-3ab9713ae6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLMMistral(LLM):\n",
    "    model: MistralForCausalLM\n",
    "    tokenizer: LlamaTokenizerFast\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "    def _call(self, prompt: str, stop = None,\n",
    "        run_manager = None) -> str:\n",
    "        messages = [\n",
    "         {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "\n",
    "        encodeds = self.tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "        model_inputs = encodeds.to(self.model.device)\n",
    "\n",
    "        generated_ids = self.model.generate(model_inputs,\n",
    "                                            max_new_tokens=512,\n",
    "                                            do_sample=True,\n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            top_k=3, temperature=0.01, top_p=0.9)\n",
    "        decoded = self.tokenizer.batch_decode(generated_ids)\n",
    "\n",
    "        output = decoded[0].split(\"[/INST]\")[1].replace(\"</s>\", \"\").strip()\n",
    "\n",
    "        if stop is not None:\n",
    "          for word in stop:\n",
    "            output = output.split(word)[0].strip()\n",
    "\n",
    "        while not output.endswith(\"```\"):\n",
    "          output += \"`\"\n",
    "\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"model\": self.model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d55d385c-b6d2-432f-9ad4-cc6412f028ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm = CustomLLMMistral(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb2fc7f-5d65-47cc-9db5-9b67d87cb034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional tools\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], open_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5aa010-b21b-40eb-b613-eac18159781e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system = '''Assistant is a large language model.\n",
    "\n",
    "Assistant is designed to be able to assist with a wide range of tasks, from answering\n",
    "simple questions to providing in-depth explanations and discussions on a wide range of\n",
    "topics. As a language model, Assistant is able to generate human-like text based on\n",
    "the input it receives, allowing it to engage in natural-sounding conversations and\n",
    "provide responses that are coherent and relevant to the topic at hand.\n",
    "'''\n",
    "\n",
    "human = '''TOOLS\n",
    "------\n",
    "Assistant can ask the user to use tools to look up information that may be helpful in answering the users original question.\n",
    "The tools the human can use are:\n",
    "\n",
    "{tools}\n",
    "\n",
    "RESPONSE FORMAT INSTRUCTIONS\n",
    "----------------------------\n",
    "\n",
    "When responding to me, please output a response in one of two formats:\n",
    "\n",
    "**Option 1:**\n",
    "Use this if you want the human to use a tool.\n",
    "Markdown code snippet formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"action\": string, \\ The action to take. Must be one of {tool_names}\n",
    "    \"action_input\": string \\ The input to the action\n",
    "}}\n",
    "```\n",
    "\n",
    "**Option #2:**\n",
    "Use this if you want to respond directly to the human. Markdown code snippet formatted in the following schema:\n",
    "\n",
    "```json\n",
    "{{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": string \\ You should put what you want to return to use here\n",
    "}}\n",
    "```\n",
    "\n",
    "USER'S INPUT\n",
    "--------------------\n",
    "Here is the user's input (remember to respond with a markdown code snippet of a json blob with a single action, and NOTHING else):\n",
    "\n",
    "User: {input}'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbe9110-e8cf-40aa-a290-9e3f5c0a52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_json_chat_agent(\n",
    "    tools = tools + [google_search],\n",
    "    llm = open_llm,\n",
    "    prompt = prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2517b53b-f37f-4a07-b228-fef854cfdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools + [google_search], verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0757c2d8-3bf3-4013-906a-2fa04b366765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"wikipedia\",\n",
      "    \"action_input\": \"Particle physics, also known as high-energy physics, is a branch of physics that studies the nature of particles that are the constituents of what is often called matter and radiation. It is the study of particles that are the composites of quarks and leptons, as well as the force carriers and fields. Is it the same as quantum physics? No, particle physics is a subfield of quantum physics. Quantum physics, also known as quantum mechanics, is a fundamental theory in physics which describes nature at the smallest scales of energy levels of atoms and subatomic particles.\"\n",
      "}\n",
      "```\u001b[0m\u001b[33;1m\u001b[1;3mPage: Particle physics\n",
      "Summary: Particle physics or high-energy physics is the study of fundamental particles and forces that constitute matter and radiation. The field also studies combinations of elementary particles up to the scale of protons and neutrons, while the study of combination of protons and neutrons is called nuclear physics.\n",
      "The fundamental particles in the universe are classified in the Standard Model as fermions (matter particles) and bosons (force-carrying particles). There are three generations of fermions,  although ordinary matter is made only from the first fermion generation. The first generation consists of up and down quarks which form  protons and neutrons,  and electrons and electron neutrinos. The three fundamental interactions known to be mediated by bosons are electromagnetism, the weak interaction, and the strong interaction.\n",
      "Quarks cannot exist on their own but form hadrons.  Hadrons that contain an odd number of quarks are called baryons and those that contain an even number are called mesons. Two baryons, the proton and the neutron, make up most of the mass of ordinary matter. Mesons are unstable and the longest-lived last for only a few hundredths of a microsecond. They occur after collisions between particles made of quarks, such as fast-moving protons and neutrons in cosmic rays. Mesons are also produced in cyclotrons or other particle accelerators.\n",
      "Particles have corresponding  antiparticles with the same mass but with opposite electric charges. For example, the antiparticle of the electron is the positron. The electron has a negative electric charge, the positron has a positive charge. These antiparticles can theoretically form a corresponding form of matter called antimatter. Some particles, such as the photon, are their own antiparticle.\n",
      "These elementary particles are excitations of the quantum fields that also govern their interactions. The dominant theory explaining these fundamental particles and fields, along with their dynamics, is called the Standard Model. The reconciliation of gravity to the current particle physics theory is not solved; many theories have addressed this problem, such as loop quantum gravity, string theory and supersymmetry theory.\n",
      "Practical particle physics is the study of these particles in radioactive processes and in particle accelerators such as the Large Hadron Collider. Theoretical particle physics is the study of these particles in the context of cosmology and quantum theory. The two are closely interrelated: the Higgs boson was postulated by theoretical particle physicists and its presence confirmed by practical experiments.\n",
      "\n",
      "\n",
      "\n",
      "Page: Higgs boson\n",
      "Summary: The Higgs boson, sometimes called the Higgs particle, is an elementary particle in the Standard Model of particle physics produced by the quantum excitation of the Higgs field, one of the fields in particle physics theory. In the Standard Model, the Higgs particle is a massive scalar boson with zero spin, even (positive) parity, no electric charge, and no colour charge that couples to (interacts with) mass. It is also very unstable, decaying into other particles almost immediately upon generation.\n",
      "The Higgs field is a scalar field with two neutral and two electrically charged components that form a complex doublet of the weak isospin SU(2) symmetry. Its \"Sombrero potential\" leads it to take a nonzero value everywhere (including otherwise empty space), which breaks the weak isospin symmetry of the electroweak interaction and, via the Higgs mechanism, gives a rest mass to all massive elementary particles of the Standard Model, including the Higgs boson itself.\n",
      "Both the field and the boson are named after physicist Peter Higgs, who in 1964, along with five other scientists in three teams, proposed the Higgs mechanism, a way for some particles to acquire mass. (All fundamental particles known at the time should be massless at very high energies, but fully explaining how some particles gain mass at lower ene\u001b[0m\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"Particle physics and quantum physics are related fields in physics. Particle physics is the study of particles that make up matter and radiation, while quantum physics is a fundamental theory that describes nature at the smallest scales. The Higgs boson is an elementary particle in the Standard Model of particle physics that gives mass to other particles. It was proposed by Peter Higgs and other scientists in the 1960s.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is particle physics? Is it the same as quantum physics?',\n",
       " 'output': 'Particle physics and quantum physics are related fields in physics. Particle physics is the study of particles that make up matter and radiation, while quantum physics is a fundamental theory that describes nature at the smallest scales. The Higgs boson is an elementary particle in the Standard Model of particle physics that gives mass to other particles. It was proposed by Peter Higgs and other scientists in the 1960s.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.debug = False\n",
    "# agent_executor.invoke({\"input\": \"Calculate the hypotenuse of an isosceles right angle triangle if the side is 2\"})\n",
    "agent_executor.invoke({\"input\": \"What is particle physics? Is it the same as quantum physics?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
