{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7493ed-591f-4478-99bb-6abfb93560d0",
   "metadata": {},
   "source": [
    "## Testing Images from HF\n",
    "\n",
    "The idea of this notebook is to design a quick interface that we can continuously improve to quickly test various images from HF. We can add various options such as:\n",
    "- loading in FP\n",
    "- loading in 8 or 4 bit with bits and bytes\n",
    "- loading with other quantinisation methods\n",
    "- different types of Memory if possible.\n",
    "\n",
    "The models will be used for inference and supplied via Gradio.\n",
    "\n",
    "Lets start with a new model for Coding Assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7992c4b6-ccf6-4b63-8d44-05830ad92563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.1.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (2.6.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad04a421-4d79-44f7-a820-9b9bf2834a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import os\n",
    "import openai\n",
    "import torch\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ecb9764-5ac1-4f19-be3a-f68d0433bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d201976-1091-4e56-b46e-40590985c87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e40019a8a247a9842b6496dad2ff4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", \n",
    "                                             trust_remote_code=True, load_in_8bit=True\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fe20cba-5a04-4207-85b5-022e41c119f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('', 0)])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "device_map = infer_auto_device_map(model)\n",
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d195e6e-2ba4-46fa-9592-93e472f5178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.01\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "343a03ee-f0d1-4e2e-8531-229c112b83da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "963a4800-a617-4f97-ab02-2253d52796ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = text_pipeline(\"What PC game would you recommend that is similar to The Witcher 3 - The Wild Hunt?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b93af7-bb10-4d26-afd0-1f4067b39c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What PC game would you recommend that is similar to The Witcher 3 - The Wild Hunt?\n",
      "User 1: I'd say Skyrim, but with mods. It's not as good of a story, but the open world and freedom is very similar.\n",
      "\n",
      "If you want something more like TW3, then maybe Dragon Age Inquisition or Mass Effect Andromeda. Both have great stories and are open world RPGs.\n",
      "User 2: I've played both Skyrim and DA:I, and while they were fun, they didn't quite scratch the itch for me. I'll give ME:A another try though, since I only got about halfway through it before getting bored.\n",
      "User 1: Yeah, I'm not a huge fan of the ME series either. But I've heard good things about the new one.\n",
      "\n",
      "Another suggestion could be the Divinity Original Sin games. They're turn based RPGs, but they have a lot of depth and a great story.\n",
      "User 2: I've heard good things about DOS2, but haven't tried it yet. I'll definitely check it out! Thanks for the suggestions!\n"
     ]
    }
   ],
   "source": [
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "079df399-9c97-4dc1-9b12-24d52b99f15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "1 1\n",
      "1 2\n",
      "2 3\n",
      "3 5\n",
      "5 8\n",
      "8 13\n",
      "13 21\n",
      "21 34\n",
      "34 55\n",
      "55 89\n",
      "89 144\n",
      "144 233\n",
      "233 377\n",
      "377 610\n",
      "610 987\n",
      "987 1597\n",
      "1597 2584\n",
      "2584 4181\n",
      "4181 6765 10946\n"
     ]
    }
   ],
   "source": [
    "sum = 9000\n",
    "def fib(n, m):\n",
    "    if n + m >= sum:\n",
    "        print (n, m, n+m)\n",
    "    else:\n",
    "        print(n, m)\n",
    "        fib(m, n + m)\n",
    "\n",
    "fib(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d32fa-18a6-4369-9e71-c78ebd8141b6",
   "metadata": {},
   "source": [
    "### Defining the structure in Gradio and LangChain\n",
    "\n",
    "The idea is to parametrise the selection of model and other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88f03060-f8dc-4fbe-a6ea-82dbd72150c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-4.19.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Downloading altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.11/site-packages (from gradio) (0.109.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio-client==0.10.1 (from gradio)\n",
      "  Downloading gradio_client-0.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.11/site-packages (from gradio) (0.26.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.11/site-packages (from gradio) (0.20.3)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.11/site-packages (from gradio) (6.1.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (3.8.2)\n",
      "Requirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (1.26.3)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.9.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m898.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (2.2.0)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (10.2.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (2.6.0)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting python-multipart>=0.0.9 (from gradio)\n",
      "  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (6.0.1)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.9 in /opt/conda/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.11/site-packages (from gradio) (0.27.0.post1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from gradio-client==0.10.1->gradio) (2023.10.0)\n",
      "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.1->gradio)\n",
      "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.11/site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\n",
      "Collecting toolz (from altair<6.0,>=4.2.0->gradio)\n",
      "  Downloading toolz-0.12.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (4.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (3.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx>=0.24.1->gradio) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0->gradio) (2.16.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.11/site-packages (from typer<1.0,>=0.9->typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
      "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich<14.0.0,>=10.11.0 (from typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /opt/conda/lib/python3.11/site-packages (from fastapi->gradio) (0.35.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.11.2)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.13.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.17.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.1.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading gradio-4.19.2-py3-none-any.whl (16.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading gradio_client-0.10.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
      "Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading toolz-0.12.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m855.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=c6d4e4cfd203552280aed88fea0124bbab3a3e530a42d11f96115e4650b5929c\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/55/3c/f2/f6e34046bac0d57c13c7d08123b85872423b89c8f59bafda51\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: pydub, ffmpy, websockets, toolz, tomlkit, shellingham, semantic-version, ruff, python-multipart, orjson, mdurl, aiofiles, markdown-it-py, rich, gradio-client, altair, gradio\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 12.0\n",
      "    Uninstalling websockets-12.0:\n",
      "      Successfully uninstalled websockets-12.0\n",
      "Successfully installed aiofiles-23.2.1 altair-5.2.0 ffmpy-0.3.2 gradio-4.19.2 gradio-client-0.10.1 markdown-it-py-3.0.0 mdurl-0.1.2 orjson-3.9.15 pydub-0.25.1 python-multipart-0.0.9 rich-13.7.0 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 tomlkit-0.12.0 toolz-0.12.1 websockets-11.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d21a22fd-4737-4611-a09a-3ef272a27e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app includes and inference test with the function\n",
    "import gradio as gr\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain import HuggingFacePipeline, ConversationChain, LLMChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "def getConfigFromModel(model, tokenizer):\n",
    "    generation_config = GenerationConfig.from_pretrained(model)\n",
    "    generation_config.max_new_tokens = 512\n",
    "    generation_config.temperature = 0.1\n",
    "    generation_config.top_p = 0.85\n",
    "    generation_config.do_sample = True\n",
    "    generation_config.repetition_penalty = 1.15\n",
    "    generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "    return generation_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Template for mistralai/Mistral-7B-Instruct-v0.2\n",
    "# template = \"\"\"\n",
    "# <s>[INST]Answer the question below as good as you can. Do not repeat yourself.\n",
    "# Current conversation:\n",
    "# {history}\n",
    "# Question: {input}[/INST]\n",
    "# \"\"\"\n",
    "\n",
    "# Template for MS Phi-2\n",
    "TEMPLATE = \"\"\"\n",
    "Current conversation:\n",
    "{history}\n",
    "Instruct:\n",
    "{input}.\n",
    "Output:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "710087bc-e614-4165-970a-74321494c7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593adac1e3b34b61813e737a3776fb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name=\"/home/jovyan/ext_models/phi-2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             trust_remote_code=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "config = getConfigFromModel(model_name, tokenizer)\n",
    "text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=True,\n",
    "        generation_config=config\n",
    "    )\n",
    "open_llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "prompt = PromptTemplate(template=TEMPLATE.replace('{history}', \"\"), input_variables=[\"input\"])\n",
    "chain = LLMChain(llm=open_llm, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc2a8a8f-02a8-4c7a-b373-08be0a96ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Current conversation:\n",
      "\n",
      "Instruct:\n",
      "What is the population of India and China?.\n",
      "Output:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "r = chain.invoke({\"input\": \"What is the population of India and China?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3378949e-3955-44ee-8e8f-33df505a36b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The current population of India is approximately 1.366 billion, while China has a population of around 1.439 billion people.\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad4823e-5517-41d7-93ad-28bbf1502b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: \n",
      "<s>[INST]Answer the question below as good as you can. Do not repeat yourself.\n",
      "Current conversation:\n",
      "Human: Hi there who is this?\n",
      "AI: Response: Hello! I'm an artificial intelligence designed to assist with various tasks and answer questions to the best of my ability. How may I help you today?\n",
      "Question: nice to meet you my name is Snoop. I am a famous rapper[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Snoop, it's nice to meet you! I know that you are a renowned rapper. Is there something specific you would like assistance or information on related to your music career or any particular topic? I'll do my best to provide accurate and helpful responses.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.predict(input=\"nice to meet you my name is Snoop. I am a famous rapper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca70f4b-10a1-4222-ad29-7748ed467a68",
   "metadata": {},
   "source": [
    "### The App Definition.\n",
    "\n",
    "We will use Blocks so that we can define different data flows. We need one data flow to help us set up the model and an another flow to provide the chatbot.\n",
    "\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d136c15-9219-453a-a2d0-4218b607efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_runner(llm_obj, model_name, quant, device, llm_type):\n",
    "    model_args = dict()\n",
    "    if quant == '8bit':\n",
    "        model_args.update({\"load_in_8bit\": True})\n",
    "    if device == 'gpu' and \"cuda\" in DEVICE:\n",
    "        model_args.update({\"device_map\": DEVICE})\n",
    "    else:\n",
    "        model_args.update({\"device_map\": 'auto'})\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             trust_remote_code=True, **model_args)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # total_steps = list(model.parameters())\n",
    "    \n",
    "    config = getConfigFromModel(model_name, tokenizer)\n",
    "    text_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=True,\n",
    "        generation_config=config\n",
    "    )\n",
    "    open_llm = HuggingFacePipeline(pipeline=text_pipeline)\n",
    "    \n",
    "    llm_obj = dict({\"open_llm\": open_llm, \n",
    "                    \"config\": config, \n",
    "                    \"model\": model, \n",
    "                    \"tokenizer\": tokenizer})\n",
    "    return llm_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efd707b9-55aa-4dac-8b8f-1882f10d56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(message, history, llm_obj, llm_type, template=None):\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    history_str = \"\".join([\"\".join([\"\\n<question>\"+item[0], \"\\n<answer>\"+item[1]])  #curr_system_message +\n",
    "                for item in history_transformer_format])\n",
    "    result_template = template if template is not None and template != \"\" else TEMPLATE.strip()\n",
    "    prompt = PromptTemplate(template=result_template.replace('{history}', history_str), input_variables=[\"input\"])\n",
    "    if (llm_type == 'gpt_3'):\n",
    "        gpt3_llm = ChatOpenAI(temperature=0.2, model=\"gpt-3.5-turbo\")\n",
    "        chain = LLMChain(llm=gpt3_llm, prompt=prompt, verbose=True)\n",
    "    else:\n",
    "        chain = LLMChain(llm=llm_obj[\"open_llm\"], prompt=prompt, verbose=True)\n",
    "    response = chain.invoke({\"input\": message})\n",
    "    history.append((message, str(response[\"text\"])))\n",
    "    \n",
    "    # response = chain.predict(input=text)\n",
    "    return \"\", history\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c964620-ceff-411c-a280-56aa6d2b8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(llm_obj):\n",
    "    for key in llm_obj.keys():\n",
    "        llm_obj[key] = None\n",
    "    del llm_obj\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac3124c-9547-469b-91f2-16b0e51ba095",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Blocks()\n",
    "with demo:\n",
    "    placeholder_template = TEMPLATE.strip()\n",
    "    llm_obj = gr.State()\n",
    "    model_name = gr.Textbox(label='Model')\n",
    "    with gr.Row():\n",
    "        quant = gr.Dropdown(['full', '8bit'], label='Quant')\n",
    "        device = gr.Dropdown(['auto', 'gpu'], label='Device')\n",
    "    b1 = gr.Button('Load Model')\n",
    "    # progress = gr.Textbox()\n",
    "    llm_type = gr.Radio(choices=['open_llm', 'gpt_3', 'gpt_4'], value='open_llm')\n",
    "    message = gr.Textbox(label='Message')\n",
    "    template = gr.TextArea(value=None, label='Custom Template', placeholder=placeholder_template)\n",
    "    bot = gr.Chatbot(label='Response')\n",
    "    b2 = gr.Button('Submit')\n",
    "    b3 = gr.Button('Unload Model')\n",
    "    b1.click(model_runner, inputs=[llm_obj, model_name, quant, device, llm_type], outputs=[llm_obj])\n",
    "    b2.click(inference, inputs=[message, bot, llm_obj, llm_type, template], outputs=[message, bot])\n",
    "    b3.click(cleanup, inputs=[llm_obj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8309a3c4-c719-4bbe-8547-cebead5d1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCurrent conversation:\n",
      "\n",
      "<question>Can you suggest a simple python app structure. The app will be used to launch a gradio application for a chatbot\n",
      "<answer>\n",
      "Instruct:\n",
      "Can you suggest a simple python app structure. The app will be used to launch a gradio application for a chatbot.\n",
      "Output:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mCurrent conversation:\n",
      "\n",
      "<question>Can you suggest a simple python app structure. The app will be used to launch a gradio application for a chatbot\n",
      "<answer>Sure! Here is a simple Python app structure for launching a Gradio application for a chatbot:\n",
      "\n",
      "1. Create a Python file for your chatbot logic, such as chatbot.py.\n",
      "2. Define your chatbot functionality in the chatbot.py file, including any necessary functions or classes.\n",
      "3. Create a separate Python file for your Gradio application, such as app.py.\n",
      "4. In the app.py file, import the necessary libraries (such as Gradio) and your chatbot logic from chatbot.py.\n",
      "5. Define the Gradio interface for your chatbot, including input and output components.\n",
      "6. Finally, launch the Gradio application using the gr.Interface() function.\n",
      "\n",
      "This structure will help keep your code organized and make it easier to maintain and update your chatbot application. Let me know if you need any more help or clarification!\n",
      "<question>what is the basic script for the main.py file? how to start the app?\n",
      "<answer>\n",
      "Instruct:\n",
      "what is the basic script for the main.py file? how to start the app?.\n",
      "Output:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa4bd64e-14b6-48d0-bc1b-0b35b7733676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa48769-43c0-4b39-85e9-f33933fb2a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
