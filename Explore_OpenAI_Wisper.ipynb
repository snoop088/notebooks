{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78a6e474-a52b-4f57-80d4-65da33d73fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pydub # requires ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009900b3-9816-4b2e-a9c9-e385d86597ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting audio_extract\n",
      "  Downloading audio_extract-0.7.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting ffmpeg-python==0.2.0 (from audio_extract)\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting imageio-ffmpeg==0.4.8 (from audio_extract)\n",
      "  Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting mutagen==1.46.0 (from audio_extract)\n",
      "  Downloading mutagen-1.46.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting future (from ffmpeg-python==0.2.0->audio_extract)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Downloading audio_extract-0.7.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Downloading imageio_ffmpeg-0.4.8-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mutagen-1.46.0-py3-none-any.whl (193 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Installing collected packages: mutagen, imageio-ffmpeg, future, ffmpeg-python, audio_extract\n",
      "Successfully installed audio_extract-0.7.0 ffmpeg-python-0.2.0 future-1.0.0 imageio-ffmpeg-0.4.8 mutagen-1.46.0\n"
     ]
    }
   ],
   "source": [
    "!pip install audio_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af4dd73-991e-4966-809d-f19f2a1745aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "import os\n",
    "import math\n",
    "from audio_extract import extract_audio "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1ce82-d6c6-4490-92b7-d091326f156b",
   "metadata": {},
   "source": [
    "## Split the Audio in 10 Minute Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4147b3e-8fbe-4aca-8d19-5166ef119d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MP3 file\n",
    "audio_file = AudioSegment.from_mp3(\"./data/sound/Building_Multimodal_Models.mp3\")\n",
    "\n",
    "# Calculate the number of 10-minute segments\n",
    "num_segments = math.ceil(len(audio_file) / (60 * 1000 * 10))  # 60 seconds per minute, 1000 milliseconds per second * 10 for 10 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e8be583-8f45-4355-87d2-4ddbd5a5c16f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cb357de-f64f-403b-ac04-deb354e31029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory to store the output files\n",
    "os.makedirs('./data/sound/output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d741bd1-cda8-4203-9e08-03253934ca37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "/opt/conda/lib/python3.11/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffprobe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AudioSegment\n\u001b[0;32m----> 2\u001b[0m next_js_conf \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_mp3\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data/sound/nextjs.conf/nextjs.conf.14.mp3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pydub/audio_segment.py:796\u001b[0m, in \u001b[0;36mAudioSegment.from_mp3\u001b[0;34m(cls, file, parameters)\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_mp3\u001b[39m(\u001b[38;5;28mcls\u001b[39m, file, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmp3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pydub/audio_segment.py:728\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 728\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[43mmediainfo_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43morig_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_ahead_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_ahead_limit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[1;32m    730\u001b[0m     audio_streams \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    731\u001b[0m                      \u001b[38;5;28;01mif\u001b[39;00m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodec_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pydub/utils.py:274\u001b[0m, in \u001b[0;36mmediainfo_json\u001b[0;34m(filepath, read_ahead_limit)\u001b[0m\n\u001b[1;32m    271\u001b[0m         file\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    273\u001b[0m command \u001b[38;5;241m=\u001b[39m [prober, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-of\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m command_args\n\u001b[0;32m--> 274\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstdin_parameter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m output, stderr \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mstdin_data)\n\u001b[1;32m    276\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/subprocess.py:1950\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1949\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1950\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1951\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffprobe'"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "next_js_conf = AudioSegment.from_mp3(\"./data/sound/nextjs.conf/nextjs.conf.14.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28389d7d-9df7-416f-a8bf-19581972f537",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_segments):\n",
    "    start_time = i * 60000 * 10  # 60 seconds per minute, 1000 milliseconds per second\n",
    "    end_time = (i + 1) * 60000 * 10\n",
    "    segment = audio_file[start_time:min(end_time, len(audio_file))]\n",
    "    segment.export(f\"./data/sound/output/segment_{i+1}.mp3\", format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8754f9bb-e7b0-4ef0-9165-330328493a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6470008"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aa142b-8e23-4861-8d85-e2a3498d6c7f",
   "metadata": {},
   "source": [
    "## Initialise Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0db72b-882e-45e3-a7eb-3087f6506ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b5e2ee-e64c-4ddb-9d36-d177a0bc464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    chunk_length_s=25,\n",
    "    batch_size=16,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs={\"language\": \"en\", \"task\": \"transcribe\", \"max_new_tokens\": 128},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e426b8a-599e-4982-9d49-8c63b8ce9516",
   "metadata": {},
   "source": [
    "## Process the Audio Segments and Store Documents for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4574e247-3444-43f3-b323-080c853c8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/sound/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "447dc52a-a885-4d02-9a21-39ea6e911c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sound/output/segment_10.mp3\n",
      "./data/sound/output/segment_5.mp3\n",
      "./data/sound/output/segment_11.mp3\n",
      "./data/sound/output/segment_1.mp3\n",
      "./data/sound/output/segment_2.mp3\n",
      "./data/sound/output/segment_4.mp3\n",
      "./data/sound/output/segment_6.mp3\n",
      "./data/sound/output/segment_3.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sound/output/segment_7.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sound/output/segment_8.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/sound/output/segment_9.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for file_name in os.listdir(path):\n",
    "    if file_name.endswith(\".mp3\"):\n",
    "        \n",
    "        audio_file_path = f\"{path}/{file_name}\"\n",
    "        print(audio_file_path)\n",
    "        result = pipe(audio_file_path)\n",
    "        with open(f\"./data/sound/output/{file_name[:-4]}.txt\", \"w\") as f:\n",
    "             f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272720-7aaa-434e-a36d-24dfc0e06150",
   "metadata": {},
   "source": [
    "## Utilise RAG to Query the Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a7979c7-0871-4d1e-ad40-8a85435bbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\n",
    "path = './data/sound/output'\n",
    "files = os.listdir(path)\n",
    "files.sort(key=lambda x: int(x.split(\"_\")[1][:-4])) # sort by segment number\n",
    "\n",
    "for file_name in files:\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(f\"{path}/{file_name}\", \"r\") as f:\n",
    "            text += f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae78c548-4ccc-4762-a8e1-64a489943f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17743"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd5d605-fefe-46c6-8dfe-eeb8cd3f4d35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpath\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/text_full.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "with open(f\"{path}/text_full.txt\", \"w\") as f:\n",
    "    f.write(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd3a03-4c28-4ad5-b278-f369b17436f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e92e2310-5647-4c92-ba57-906811bccd50",
   "metadata": {},
   "source": [
    "## Process Without Splitting .mp3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080d89ab-b60b-419b-bdf6-6eb2b3d2094d",
   "metadata": {},
   "source": [
    "Extract audio from the MOVs that we record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d52d36e9-f794-43db-a18a-e7ee87014d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success : audio file has been saved to \"/home/jovyan/work/data/transcribe_se6_vid/audio3.mp3\".\n"
     ]
    }
   ],
   "source": [
    "extract_audio(input_path=\"./data/transcribe_se6_vid/Screen Recording 2024-11-22 at 15.59.15.mov\", output_path=\"./data/transcribe_se6_vid/audio3.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464e7cdc-2582-4db1-8220-8b15789da30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "result = pipe('./data/transcribe_se6_vid/audio3.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8276538-d3be-4c02-b67c-c53d23604521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bb71f5e-2017-4c3d-82de-8dbd9a790474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \" Everything in the new Stream i6 is very similar to what you already know and have used in Stream i5.5. There are a few subtle differences that make the interface much more streamlined. First, on the right-hand side, you can see the feed tree. Instead of having buttons for context actions, we have placed them in a context menu that you can view by clicking on an item with the right click. Here you can edit, rename, archive, clone and other actions on the particular feed. Selecting an item from your dashboard you can go and quickly edit and create new feed from this item. Here is one major difference that now the completion of each task or the production of each feed in New Stream i has been broken down into steps and visualized by this wizard on top. You can select different options and then go next to select to proceed to a new step. Enter a feed name here to save your progress. Select size and click next. Here you can review preview what the banner would look like or you could go back and edit any of the steps that you have taken. In a similar fashion working with HTML5s you can select an item to edit, select offer layout from a portfolio of offer screens on the left and then click next. You can either click this next or use the next on the wizard steps itself. Type in your feed, select the sizes that you would like to add to the feed and click next. On this screen, all the sizes will be visualized. What's even better now is that you don't have to go back to edit a few options. You can right away edit them here. Clicking update will update the banner set that you preview the new copy.\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ba18e-73c8-4d57-985b-7f208a2506f5",
   "metadata": {},
   "source": [
    "We will split this in 10 equal segments. Lets write it down first so we can load it later if needed from a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5656ec53-c380-4c12-a739-d5c3a848b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the text\n",
    "with open(\"./data/sound/output/test-encode.txt\", \"w\") as f:\n",
    "    f.write(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02a2bf93-3473-4272-8102-618e8dd37ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the text so we dont process every time\n",
    "with open(\"./data/sound/nextjs.conf/text.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ca8f96-8554-4d45-a1ba-fb9db13b0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/sound/output/test-encode.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a107a87-6902-4ba0-9aac-85a33cec912e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3058.5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = text.split()\n",
    "segments = 2\n",
    "text_segments = []\n",
    "segment_length = len(arr) // segments\n",
    "for i in range(segments):\n",
    "    text_segments.append(\" \".join(arr[i*segment_length:(i+1)*segment_length]))\n",
    "\n",
    "len(text_segments[1].split()) * 1.5 # appx tokens / segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077492be-2c04-4f17-a590-754ce23b8d9a",
   "metadata": {},
   "source": [
    "#### Can We Create a Function to Adjust Segments from Context Window\n",
    "\n",
    "The function called gen_seg will take a context window, calculate with some margin of error number of words for the tokens in the context window and then split the text into segments of these many words, making sure not to finish a sentence abruptly. So, it will search inside the word allowance the last sentence and push it into the total segments.\n",
    "\n",
    "lets get to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8d2eee-80ae-4786-95ef-a4904ebafb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_seg(text, context_window, ratio=1.4):\n",
    "    \"\"\"\n",
    "    Splits text into segments that should fit into a context window tokens\n",
    "    Args:\n",
    "        text: string, the text to split, \n",
    "        context_window: int, the number of tokens in the context window\n",
    "        ratio: int, the ratio between tokens and words - normally each word converts into 1.4 tokens? adjustable.\n",
    "    Returns:\n",
    "        array of text_segments to use for generating summary of text\n",
    "    \"\"\"\n",
    "    text_segments = []\n",
    "    words_per_seg = int(context_window // ratio)\n",
    "    remaining_words = text.split()\n",
    "    # current_segment_num = 0\n",
    "\n",
    "    while len(remaining_words) > words_per_seg:\n",
    "        current_seg_arr = remaining_words[:words_per_seg]\n",
    "        current_seg = \" \".join(current_seg_arr)\n",
    "        current_seg_sent = \".\".join(current_seg.split('.')[:-1]) + \".\"\n",
    "        remaining_words = text.split(current_seg_sent)[1].split()\n",
    "        text_segments.append(current_seg_sent)\n",
    "        \n",
    "    text_segments.append(\" \".join(remaining_words))        \n",
    "    return text_segments\n",
    "\n",
    "segs = gen_seg(text, context_window=3500)\n",
    "\n",
    "# function to get all elements but the last in array\n",
    "# def all_but_last(arr):\n",
    "#     return arr[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f085081e-c0cd-41c9-a092-437b2aace03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(segs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af65bae-b39c-4e1c-8fa6-ed349f111fa0",
   "metadata": {},
   "source": [
    "## The Cumulative Summary Idea\n",
    "Now that we have this in segments, lets design a prompt that would generate an accumulative summary tally for each segment.\n",
    "Maybe the prompt can be something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74ae7475-2422-4040-af26-80f3bcbbafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see this in langsmith\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e67777-f4f6-47d6-886e-1f8368244036",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_instruction = \"\"\"You are a professional writer. Your job here is to produce great, detailed and coherent summary of a document.\n",
    "In order for the document to fit into your context window it is split into {number_of_parts} parts. Each part will be supplied to you as \n",
    "user prompt, together with the current key points up to this stage.\n",
    "\n",
    "Do not repeat yourself in the summary, but rather add to it or rephrase it as you see fit, enhancing the key take away\n",
    "points with new information. Only take out information that you do not think contributes to the main theme discussed in the document.\n",
    "\n",
    "Supply updated key points list that you have so far, based on existing points and the current step copy.\n",
    "\n",
    "When you receive copy for step - {number_of_parts} - please rework and rephrase what you have currently to produce a final list of up to 25 key points, \n",
    "followed by a short paragraph summarising the main theme of the entire document.\n",
    "\"\"\"\n",
    "\n",
    "# prompt_instruction_initial = \"\"\"You are a professional writer. Your job here is to produce great, detailed and coherent summary of a document.\n",
    "# In order for the document to fit into your context window it is split into {number_of_parts} parts. Each part will be supplied to you as \n",
    "# user prompt, together with the current key points up to this stage.\n",
    "\n",
    "# Add a summary of the user prompt to the current summary and produce a new list.\n",
    "\n",
    "# When you receive copy for step - {number_of_parts} - please rework and rephrase what you have currently to produce a final list of up to 25 key points.\n",
    "# \"\"\"\n",
    "\n",
    "prompt_instruction_summary = \"\"\"You are a professional writer. Your job here is to produce great, detailed and coherent summary of a document as a list\n",
    "of key points. In order for the document to fit into your context window it is split into {number_of_parts} parts. Each part will be supplied to you as \n",
    "user prompt, together with the current key points up to this stage.\n",
    "\n",
    "You are to summarise the following user prompt and add it in form of key points to the current list provided under the tag <key_points>. The new list\n",
    "should look like this:\n",
    "<key_points>\n",
    "1. Some example key point\n",
    "2. Another key point\n",
    "3. Key point from the latest user prompt\n",
    "4. More points from the latest prompt\n",
    "5. ...\n",
    "6. and so on...\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt_user = \"\"\"The key points thus far are:\n",
    "{{summary}}\n",
    "The copy of step number {current} follows below:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_user_key_points = \"\"\"The key points thus far are:\n",
    "<key_points>\n",
    "{{summary}}\n",
    "The copy to summarise into key points and add to the above list is:\n",
    "{input}\n",
    "Please when supplying the new list of total key points, remove any repeating ideas or themes and rephrase them for clarity.\n",
    "<key_points>\n",
    "\"\"\"\n",
    "\n",
    "initial_user = \"\"\"The key points thus far are:\n",
    "None. This is the initial input.\n",
    "The copy of step number 1 follows below:\n",
    "{input}\n",
    "\"\"\"\n",
    "prompt_initial_user_new = \"\"\"The key points thus far are:\n",
    "None. This is the initial input.\n",
    "The copy to summarise as 10 key points follows below:\n",
    "{input}\n",
    "<key_points>\n",
    "\"\"\"\n",
    "\n",
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>{instruction}\n",
    "<|eot_id|><|start_header_id|>user <|end_header_id|>{user}\n",
    "<|eot_id|><|start_header_id|>assistant <|end_header_id|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac7df391-7056-4574-be17-7550b099fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig,\n",
    "    pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce294d7a-6456-4631-9b3c-9d7cc309eac9",
   "metadata": {},
   "source": [
    "### Initial Step\n",
    "\n",
    "We need this in order to supply the first input to the chain of chains. For each step we will generate the relative instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce9ef8b-f664-4df5-8925-ea2acee24a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_instruction = prompt_instruction_summary.format(number_of_parts=len(segs))\n",
    "template_with_instr = template.format(instruction=initial_instruction, user=prompt_initial_user_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896f1eca-2dee-40e3-8c9e-5510b4774d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], template='\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a professional writer. Your job here is to produce great, detailed and coherent summary of a document as a list\\nof key points. In order for the document to fit into your context window it is split into 2 parts. Each part will be supplied to you as \\nuser prompt, together with the current key points up to this stage.\\n\\nYou are to summarise the following user prompt and add it in form of key points to the current list provided under the tag <key_points>. The new list\\nshould look like this:\\n<key_points>\\n1. Some example key point\\n2. Another key point\\n3. Key point from the latest user prompt\\n4. More points from the latest prompt\\n5. ...\\n6. and so on...\\n\\n\\n<|eot_id|><|start_header_id|>user <|end_header_id|>The key points thus far are:\\nNone. This is the initial input.\\nThe copy to summarise as 10 key points follows below:\\n{input}\\n<key_points>\\n\\n<|eot_id|><|start_header_id|>assistant <|end_header_id|>\\n')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# template_initial = template_with_instr.format(current=1)\n",
    "prompt = PromptTemplate.from_template(template_with_instr)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b60c239-c48d-4a50-9811-9f93d4a66091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial = prompt.partial(input=text_segments[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e228f7b-7978-4577-884b-9429b008e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"../ext_models/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22658eb8-131d-4498-b4b4-7302ec6d5f49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d755b0d729254afca1e5496886d1675c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=\"auto\")\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "496f6560-2a72-4b77-87d4-26488d977b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token = \"<|eot_id|>\"  \n",
    "stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "begin_token = \"<|begin_of_text|>\"\n",
    "begin_token_id = tokenizer.encode(begin_token)[0]\n",
    "generation_config.eos_token_id = stop_token_id\n",
    "generation_config.begin_token_id = begin_token_id\n",
    "generation_config.max_new_tokens = 1200\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_p = 0.90\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e670a0cb-1a88-465e-92f7-881ac66bc5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_huggingface in /opt/conda/lib/python3.11/site-packages (0.0.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from langchain_huggingface) (0.24.0)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /opt/conda/lib/python3.11/site-packages (from langchain_huggingface) (0.2.21)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in /opt/conda/lib/python3.11/site-packages (from langchain_huggingface) (3.0.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in /opt/conda/lib/python3.11/site-packages (from langchain_huggingface) (0.19.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in /opt/conda/lib/python3.11/site-packages (from langchain_huggingface) (4.42.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->langchain_huggingface) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.1.92)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.8.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.1.52->langchain_huggingface) (8.5.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.5.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (1.14.0)\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.11/site-packages (from sentence-transformers>=2.6.0->langchain_huggingface) (10.4.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.39.0->langchain_huggingface) (2024.5.15)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=4.39.0->langchain_huggingface) (0.4.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_huggingface) (3.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_huggingface) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.23.0->langchain_huggingface) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (12.5.82)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain_huggingface) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f42b75-a355-46ed-a252-74c7b8a4991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain.chains import SimpleSequentialChain, SequentialChain, LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e94541-18dc-4fd0-8179-763626a6ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        generation_config=generation_config, \n",
    "                        return_full_text=False) \n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68a6e0d0-d2bd-4af7-b317-ef9df845d17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "initial_chain = LLMChain(llm=llm, prompt=prompt, output_parser=StrOutputParser())\n",
    "# initial_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbaf80dd-5604-4e4d-b14a-c3ee48567da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = [initial_chain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b362a0d-f6eb-4145-a8ce-2b3d3ffb915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(segs)-1):\n",
    "    template_with_instr = template.format(instruction=prompt_instruction_summary.format(number_of_parts=len(segs)), user=prompt_user_key_points)\n",
    "    template_internal = template_with_instr.format(input=segs[i+1], current=i+2)\n",
    "    prompt = PromptTemplate.from_template(template_internal)\n",
    "    internal_chain = LLMChain(llm=llm, prompt=prompt, output_parser=StrOutputParser())\n",
    "    chains.append(internal_chain)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98a20808-d7ca-4d08-8215-15aa7ee247cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LLMChain(prompt=PromptTemplate(input_variables=['summary'], template=\"\\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a professional writer. Your job here is to produce great, detailed and coherent summary of a document as a list\\nof key points. In order for the document to fit into your context window it is split into 2 parts. Each part will be supplied to you as \\nuser prompt, together with the current key points up to this stage.\\n\\nYou are to summarise the following user prompt and add it in form of key points to the current list provided under the tag <key_points>. The new list\\nshould look like this:\\n<key_points>\\n1. Some example key point\\n2. Another key point\\n3. Key point from the latest user prompt\\n4. More points from the latest prompt\\n5. ...\\n6. and so on...\\n\\n\\n<|eot_id|><|start_header_id|>user <|end_header_id|>The key points thus far are:\\n<key_points>\\n{summary}\\nThe copy to summarise into key points and add to the above list is:\\nIt's not a deal breaker, it's just more of an annoyance, but if my mobile devices have a much better range, Apple should be able to figure out a way to make it much better on the MacBook. All in all, there's very few things that I don't like about the M3 Pro MacBook, and I honestly don't regret moving to it from the Mac Studio at all. It's super performant, battery life is great, and it's just an all-around great machine. I do think that people get a little bit carried away with how much RAM they think they need. 18 in my case has worked quite well, and especially if you stick with things like productivity, graphic design, coding, that kind of thing, 18 is more than enough. If you're doing more resource-heavy creative stuff with video or motion graphics, you can still get pretty far with just 18, but if you're getting into beefier projects with SFX, 3D, or you just want to have a thousand things open at once. It's probably in your best interest to bump things up a little bit specs wise. For me, I'm going to continue using this. I'm assuming until maybe the next iteration of the studio is released. And if you guys would like to see any other Macs reviewed on the channel that I haven't covered yet, let me know in the comments down below. That's it for me today. I hope you enjoyed this video or you found it useful. If you did, feel free to hit that like button. If you want to see more tech related content or organize a competitive eSport league for racing snails and augmented reality, please subscribe. Thank you so much for watching and I will see you in the next upload.\\nPlease when supplying the new list of total key points, remove any repeating ideas or themes and rephrase them for clarity.\\n<key_points>\\n\\n<|eot_id|><|start_header_id|>assistant <|end_header_id|>\\n\"), llm=HuggingFacePipeline(pipeline=<transformers.pipelines.text_generation.TextGenerationPipeline object at 0x73a0fa4871d0>)),\n",
       " 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chains[1], len(chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51a160be-0ea5-4dd7-a457-dbead7d3f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_chains = SimpleSequentialChain(chains=chains, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dca98b4c-e87b-4f16-b7e3-3c15ecc864e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mHere are the key points summarized:\n",
      "\n",
      "1. The author purchased the M3 Pro MacBook Pro and has been using it daily for three months, sharing his experiences and observations.\n",
      "2. The M3 Pro has a unique set of upgrades and downgrades compared to the previous generation, making it difficult to understand its capabilities.\n",
      "3. The author emphasizes the importance of considering real-world usage rather than focusing solely on specifications and features.\n",
      "4. He notes that the M3 Pro's 11-core CPU, 14-core GPU, 512GB storage, and 18GB RAM provide excellent performance for everyday tasks.\n",
      "5. Battery life has improved significantly, with the author achieving up to 16 hours of use on a single charge, a 20-30% increase over the M2 Pro.\n",
      "6. The author addresses common concerns about leaving the MacBook plugged in continuously, assuring that it won't degrade the battery.\n",
      "7. External storage issues, such as disconnections when the Mac goes to sleep, can be resolved by using a powered hub or dock instead of connecting directly to the Mac.\n",
      "8. The M3 Pro's internal drive is faster than the M2 Pro's, but the difference may not be noticeable in real-world usage.\n",
      "9. Memory bandwidth has decreased slightly, but the author finds that it doesn't impact performance noticeably.\n",
      "10. The author notes that the M3 Pro's 18GB RAM may cause issues with certain applications, particularly Lightroom, especially when combined with other resource-intensive programs.\n",
      "11. Video rendering takes longer due to the single encoding engine, but the author finds it acceptable for casual use.\n",
      "12. The CPU is suitable for most users, except for those who rely heavily on audio processing, who may notice a slight performance dip.\n",
      "13. The GPU performs exceptionally well, thanks to hardware-enabled ray tracing, and is suitable for basic 3D modeling and video editing.\n",
      "14. The speakers are slightly quieter than the M2 Pro, but the difference is negligible; however, Bluetooth connectivity range is poor.\n",
      "15. Overall, the author concludes that the M3 Pro is an excellent choice for everyday use, offering impressive performance, battery life, and value.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mHere is the updated list of key points:\n",
      "\n",
      "<key_points>\n",
      "1. The author purchased the M3 Pro MacBook Pro and has been using it daily for three months, sharing his experiences and observations.\n",
      "2. The M3 Pro has a unique set of upgrades and downgrades compared to the previous generation, making it difficult to understand its capabilities.\n",
      "3. The author emphasizes the importance of considering real-world usage rather than focusing solely on specifications and features.\n",
      "4. He notes that the M3 Pro's 11-core CPU, 14-core GPU, 512GB storage, and 18GB RAM provide excellent performance for everyday tasks.\n",
      "5. Battery life has improved significantly, with the author achieving up to 16 hours of use on a single charge, a 20-30% increase over the M2 Pro.\n",
      "6. The author addresses common concerns about leaving the MacBook plugged in continuously, assuring that it won't degrade the battery.\n",
      "7. External storage issues, such as disconnections when the Mac goes to sleep, can be resolved by using a powered hub or dock instead of connecting directly to the Mac.\n",
      "8. The M3 Pro's internal drive is faster than the M2 Pro's, but the difference may not be noticeable in real-world usage.\n",
      "9. Memory bandwidth has decreased slightly, but the author finds that it doesn't impact performance noticeably.\n",
      "10. The author notes that the M3 Pro's 18GB RAM may cause issues with certain applications, particularly Lightroom, especially when combined with other resource-intensive programs.\n",
      "11. Video rendering takes longer due to the single encoding engine, but the author finds it acceptable for casual use.\n",
      "12. The CPU is suitable for most users, except for those who rely heavily on audio processing, who may notice a slight performance dip.\n",
      "13. The GPU performs exceptionally well, thanks to hardware-enabled ray tracing, and is suitable for basic 3D modeling and video editing.\n",
      "14. The speakers are slightly quieter than the M2 Pro, but the difference is negligible; however, Bluetooth connectivity range is poor.\n",
      "15. Overall, the author concludes that the M3 Pro is an excellent choice for everyday use, offering impressive performance, battery life, and value.\n",
      "16. The author suggests that Apple could improve the MacBook's Bluetooth connectivity range, which is currently inferior to their mobile devices.\n",
      "17. Despite some minor drawbacks, the author highly recommends the M3 Pro, citing its overall performance, battery life, and value.\n",
      "18. The author believes that 18GB RAM is sufficient for most users, including those engaged in productivity, graphic design, coding, and similar activities.\n",
      "19. However, users requiring more demanding creative workloads (e.g., video production, 3D modeling) may benefit from upgrading to more RAM.\n",
      "20. The author plans to continue using the M3 Pro until the next iteration of the Mac Studio is released.\n",
      "\n",
      "Let me know if you'd like me to revise anything!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = chain_of_chains.run(input=segs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71d2bac1-1542-4ce6-90c8-b42d610e64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the updated list of key points:\n",
      "\n",
      "<key_points>\n",
      "1. The author purchased the M3 Pro MacBook Pro and has been using it daily for three months, sharing his experiences and observations.\n",
      "2. The M3 Pro has a unique set of upgrades and downgrades compared to the previous generation, making it difficult to understand its capabilities.\n",
      "3. The author emphasizes the importance of considering real-world usage rather than focusing solely on specifications and features.\n",
      "4. He notes that the M3 Pro's 11-core CPU, 14-core GPU, 512GB storage, and 18GB RAM provide excellent performance for everyday tasks.\n",
      "5. Battery life has improved significantly, with the author achieving up to 16 hours of use on a single charge, a 20-30% increase over the M2 Pro.\n",
      "6. The author addresses common concerns about leaving the MacBook plugged in continuously, assuring that it won't degrade the battery.\n",
      "7. External storage issues, such as disconnections when the Mac goes to sleep, can be resolved by using a powered hub or dock instead of connecting directly to the Mac.\n",
      "8. The M3 Pro's internal drive is faster than the M2 Pro's, but the difference may not be noticeable in real-world usage.\n",
      "9. Memory bandwidth has decreased slightly, but the author finds that it doesn't impact performance noticeably.\n",
      "10. The author notes that the M3 Pro's 18GB RAM may cause issues with certain applications, particularly Lightroom, especially when combined with other resource-intensive programs.\n",
      "11. Video rendering takes longer due to the single encoding engine, but the author finds it acceptable for casual use.\n",
      "12. The CPU is suitable for most users, except for those who rely heavily on audio processing, who may notice a slight performance dip.\n",
      "13. The GPU performs exceptionally well, thanks to hardware-enabled ray tracing, and is suitable for basic 3D modeling and video editing.\n",
      "14. The speakers are slightly quieter than the M2 Pro, but the difference is negligible; however, Bluetooth connectivity range is poor.\n",
      "15. Overall, the author concludes that the M3 Pro is an excellent choice for everyday use, offering impressive performance, battery life, and value.\n",
      "16. The author suggests that Apple could improve the MacBook's Bluetooth connectivity range, which is currently inferior to their mobile devices.\n",
      "17. Despite some minor drawbacks, the author highly recommends the M3 Pro, citing its overall performance, battery life, and value.\n",
      "18. The author believes that 18GB RAM is sufficient for most users, including those engaged in productivity, graphic design, coding, and similar activities.\n",
      "19. However, users requiring more demanding creative workloads (e.g., video production, 3D modeling) may benefit from upgrading to more RAM.\n",
      "20. The author plans to continue using the M3 Pro until the next iteration of the Mac Studio is released.\n",
      "\n",
      "Let me know if you'd like me to revise anything!\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8210af1c-8498-46ad-9303-dd4c1da78936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
