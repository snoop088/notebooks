{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca1e67b-6263-493a-b5c7-ab785985ccbc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.11/site-packages (0.1.12)\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.2.3-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.2.5)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
      "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain) (0.1.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
      "Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: langchain-text-splitters, langchain\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.0.1\n",
      "    Uninstalling langchain-text-splitters-0.0.1:\n",
      "      Successfully uninstalled langchain-text-splitters-0.0.1\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.1.12\n",
      "    Uninstalling langchain-0.1.12:\n",
      "      Successfully uninstalled langchain-0.1.12\n",
      "Successfully installed langchain-0.2.3 langchain-text-splitters-0.2.1\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.11/site-packages (0.0.28)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.2.4-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.6.4)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.2.5)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (0.1.75)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-community) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.9.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.16.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langchain-community\n",
      "  Attempting uninstall: langchain-community\n",
      "    Found existing installation: langchain-community 0.0.28\n",
      "    Uninstalling langchain-community-0.0.28:\n",
      "      Successfully uninstalled langchain-community-0.0.28\n",
      "Successfully installed langchain-community-0.2.4\n",
      "Requirement already satisfied: langchain-text-splitters in /opt/conda/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from langchain-text-splitters) (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (0.1.75)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (3.9.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters) (2024.2.2)\n",
      "Collecting langgraph\n",
      "  Downloading langgraph-0.0.66-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /opt/conda/lib/python3.11/site-packages (from langgraph) (0.2.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.75)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.6.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.9.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (2024.2.2)\n",
      "Downloading langgraph-0.0.66-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.6/88.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: langgraph\n",
      "Successfully installed langgraph-0.0.66\n"
     ]
    }
   ],
   "source": [
    "!pip install google-api-python-client>=2.10\n",
    "!pip install -U langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install -U langchain-text-splitters\n",
    "!pip install langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a9a77e-3890-4ab3-86f0-919dbbd89456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tavily-python\n",
      "  Downloading tavily_python-0.3.5-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from tavily-python) (2.32.3)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from tavily-python) (0.7.0)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from tavily-python) (0.27.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken>=0.5.1->tavily-python) (2024.5.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->tavily-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->tavily-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->tavily-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->tavily-python) (2024.7.4)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx->tavily-python) (4.4.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->tavily-python) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->tavily-python) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->tavily-python) (0.14.0)\n",
      "Downloading tavily_python-0.3.5-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: tavily-python\n",
      "Successfully installed tavily-python-0.3.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tavily-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e445064c-1592-43c2-b3da-460a7b4e4603",
   "metadata": {},
   "source": [
    "# Evaluating Workflow with Graph Agents\n",
    "\n",
    "Exploring a tutorial as detailed in this [video](https://www.youtube.com/watch?v=-ROS6gfYIts) and corresponding source files notebook [here](https://github.com/langchain-ai/langgraph/blob/main/examples/rag/langgraph_rag_agent_llama3_local.ipynb).\n",
    "\n",
    "> Node: I will be changing the RAG from local md files of the Streameye blog, utilising HuggingFaceEmbeddings to load ./data/embeddings/gte-large/ as embeddings model instead of the nomic. For search I will be using the Google API. Llama3 will be loaded with the normal transformers API of HF\n",
    "\n",
    "With the release of LLaMA3, we're seeing great interest in agents that can run reliably and locally (e.g., on your laptop). Here, we show to how build reliable local agents using LangGraph and LLaMA3-8b from scratch. We combine ideas from 3 advanced RAG papers (Adaptive RAG, Corrective RAG, and Self-RAG) into a single control flow. We run this locally w/ a local vectorstore c/o @nomic_ai & @trychroma, @tavilyai for web search, and LLaMA3-8b via @ollama.\n",
    "\n",
    "\n",
    "## Setting Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cb15fee-6018-451e-8ed0-7ad765005669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10261e5e-f8b1-418a-8e48-3dd7a62b011e",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd5d0a09-d4b8-471c-8037-359e5ad64780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"../ext_models/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "709422e8-8ef0-4eef-9144-64dfce42fdc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f027507b1e1b4c54a2e74f751b23c40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=\"auto\")\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3695cf5b-58fb-4f47-87fe-a23a13e0585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token = \"<|eot_id|>\"  \n",
    "stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "begin_token = \"<|begin_of_text|>\"\n",
    "begin_token_id = tokenizer.encode(begin_token)[0]\n",
    "generation_config.eos_token_id = stop_token_id\n",
    "generation_config.begin_token_id = begin_token_id\n",
    "generation_config.max_new_tokens = 1024\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_p = 0.9\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83989b90-674b-45e5-9667-0c950be19120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm_pipeline = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        generation_config=generation_config, \n",
    "                        return_full_text=False) \n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171b4b2-9143-4f8e-a8be-7af8e2aac7bb",
   "metadata": {},
   "source": [
    "## Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cda308-3176-4220-868c-954f32e58af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "gte_large = HuggingFaceEmbeddings(model_name=\"./data/embeddings/gte-large/\", \n",
    "                                       model_kwargs={\"device\": DEVICE}, \n",
    "                                       encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbab806-8a08-4875-9747-5fb1ed499ef9",
   "metadata": {},
   "source": [
    "## Prepare the Retreiver\n",
    "\n",
    "Load previously created Chroma DB of the Streameye Blog articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28f1347-d5f1-4deb-98cb-a747ac1042d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=\"./data/sound/db\", embedding_function=gte_large) # lets try the multimodal podcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3669a468-9b6b-4f4c-ae88-dba3003257b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6b5c4b1-af9a-42f0-b93e-39934254908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs, confidence = 0.85):\n",
    "    return \"\\n\\n\".join(doc[0].page_content for doc in docs if doc[1] > confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bcc407-9c2b-44bd-93df-00d36957057d",
   "metadata": {},
   "source": [
    "### Lets Create Another DB\n",
    "\n",
    "We will use the text taken from OpenAI Whisper to create a new Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd85ef7b-6474-41a2-b5ff-9e64b972c465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 documents into 29 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# loader = TextLoader(\"./data/sound/text_full.txt\")\n",
    "loader = TextLoader(\"./data/sound/nextjs.conf/text.txt\")\n",
    "text = loader.load()\n",
    "\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text)\n",
    "\n",
    "## Saving to Chroma\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    db_path = \"./data/sound/nextjs.conf/db\";\n",
    "    if os.path.exists(db_path):\n",
    "        shutil.rmtree(db_path) ## Recursive delete\n",
    "    db = Chroma.from_documents(chunks, gte_large, persist_directory=db_path)\n",
    "    db.persist()\n",
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d9a101-88e9-438d-9e2c-fc54e59c6852",
   "metadata": {},
   "source": [
    "### DB From Single Text From Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4856e201-2037-4118-a47e-baf36b29aeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=\"./data/sound/nextjs.conf/db\", embedding_function=gte_large)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54386c5a-efe1-4f33-99d1-dd46d0955495",
   "metadata": {},
   "source": [
    "## The Different Nodes of the Graph\n",
    "\n",
    "1. Retreiver Grader - grades the response by the retreiver as yes/ no\n",
    "2. The Answer Generator - answers a question using context from the docs\n",
    "3. Answer Grader - grades if the answer is relevant\n",
    "4. Hallucination Grader - grades if the model has hallucinated\n",
    "5. Web Search - falls back to web search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99b589c-6a35-48d0-a0ee-de3fbd23d468",
   "metadata": {},
   "source": [
    "### 1. Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8eece820-e2db-4f85-886a-2f1f63d55237",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing relevance \n",
    "    of a retrieved document to a user question. If the document contains keywords related to the user question, \n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals.\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\n",
    "    Provide the binary score as a JSON with a single key 'score' and no premable or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document: \\n\\n {document} \\n\\n\n",
    "    Here is the user question: {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1f939bd-c625-4a4b-bd72-b8d38d17e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What can you tell me about Chameleon model\"\n",
    "context = db.similarity_search_with_relevance_scores(question, k=3)\n",
    "docs = format_docs(context, confidence = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2806b0de-ddf3-4003-b6b5-35ca5d8008a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': './data/sound/text_full.txt', 'start_index': 4800}, page_content=\"is pretty important because now what this means is that this is the new state-of-the-art vision language model, multimodal model, right? I guess GPT-4-0 is supposedly better than GPT-4, but is it better than GPT-4-V? So if Chameleon is better than GPT-4-V, it might even be better than GPT-4-0, but we don't really know yet. So we're going to say potentially state-of-the-art multimodal model. Okay. The interesting thing about Chameleon is that it's going to be trained from scratch on an end-to-end fashion on an interleaved mixture of all modalities, right? And they need to train this from scratch because they're going to be using this early fusion approach where all modalities are projected into a shared representation space from the start, allowing for seamless reasoning and generation across modalities. Okay, so let's take a pause there to think about what does this actually mean. So what this means is that this model is taking these different modalities. Here you have the modality of\"),\n",
       "  0.8341793252902094),\n",
       " (Document(metadata={'source': './data/sound/text_full.txt', 'start_index': 3206}, page_content=\"two papers here, that's going to be text and images. So it's doing text and images at the same time okay let's start with the abstract here chameleon is a I'm gonna actually skim through it a little bit too because I realized that reading these abstracts entirely is a kind of a bit of a waste of time so chameleon is a family of early fusion token based mixed modal models so that's a a lot of different adjectives. But we're going to learn what all of those mean in the paper below. It does a bunch of things such as visual question answering image captioning, text generation, image generation and long form mixed modal generation mixed modal generation is going to be the kind of cool new functionality that this has, right? Where in visual question answering, you get maybe a picture, a question, and then you answer with text. Image captioning, you're answering with text. Text generation, you're answering with text. Image generation, you're generating images. But this mixed modal\"),\n",
       "  0.8237267488088258),\n",
       " (Document(metadata={'source': './data/sound/text_full.txt', 'start_index': 23206}, page_content=\"model to a better one. Okay. So when you're doing this type of gluing vision language model where you're gluing a vision encoder to a language model, the most important thing is that this language model here, which has been pre-trained, is very strong. Right? You also want the vision encoder to be pretty strong, but what they discovered is that if you switch out this language model from a Lama 1.7b to a Mistral 7b, you're going to get a 5% improvement in score. So the most important thing when you're doing this kind of gluing approach is to make this language model big, but you don't need to give a shit about that if you're training them from scratch. So you don't have that problem here because you're not using a pre-trained model. Let's look at the pre-training for the chameleon model. Okay. The chameleon model is pre-trained in two separate stages. In the first stage, you use a data mixture consisting of the following very large data sets. So you have a combination of the\"),\n",
       "  0.8214708176688559)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b0d44b-f60b-4c85-be3b-d3436dcd4d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"is pretty important because now what this means is that this is the new state-of-the-art vision language model, multimodal model, right? I guess GPT-4-0 is supposedly better than GPT-4, but is it better than GPT-4-V? So if Chameleon is better than GPT-4-V, it might even be better than GPT-4-0, but we don't really know yet. So we're going to say potentially state-of-the-art multimodal model. Okay. The interesting thing about Chameleon is that it's going to be trained from scratch on an end-to-end fashion on an interleaved mixture of all modalities, right? And they need to train this from scratch because they're going to be using this early fusion approach where all modalities are projected into a shared representation space from the start, allowing for seamless reasoning and generation across modalities. Okay, so let's take a pause there to think about what does this actually mean. So what this means is that this model is taking these different modalities. Here you have the modality of\\n\\ntwo papers here, that's going to be text and images. So it's doing text and images at the same time okay let's start with the abstract here chameleon is a I'm gonna actually skim through it a little bit too because I realized that reading these abstracts entirely is a kind of a bit of a waste of time so chameleon is a family of early fusion token based mixed modal models so that's a a lot of different adjectives. But we're going to learn what all of those mean in the paper below. It does a bunch of things such as visual question answering image captioning, text generation, image generation and long form mixed modal generation mixed modal generation is going to be the kind of cool new functionality that this has, right? Where in visual question answering, you get maybe a picture, a question, and then you answer with text. Image captioning, you're answering with text. Text generation, you're answering with text. Image generation, you're generating images. But this mixed modal\\n\\nmodel to a better one. Okay. So when you're doing this type of gluing vision language model where you're gluing a vision encoder to a language model, the most important thing is that this language model here, which has been pre-trained, is very strong. Right? You also want the vision encoder to be pretty strong, but what they discovered is that if you switch out this language model from a Lama 1.7b to a Mistral 7b, you're going to get a 5% improvement in score. So the most important thing when you're doing this kind of gluing approach is to make this language model big, but you don't need to give a shit about that if you're training them from scratch. So you don't have that problem here because you're not using a pre-trained model. Let's look at the pre-training for the chameleon model. Okay. The chameleon model is pre-trained in two separate stages. In the first stage, you use a data mixture consisting of the following very large data sets. So you have a combination of the\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ff53a05-7499-44ee-84d9-0835dd31dd8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_grader.invoke({\"question\": question, \"document\": docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceebc0c1-3850-4f22-bff6-c458aadbd7fa",
   "metadata": {},
   "source": [
    "### 2. The Answer Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1668ba5-26c2-437e-9321-686dd5e87244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assistant for question-answering \n",
    "    tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer,\n",
    "    just say that you don't know. Try and answer exhaustively but only with information provided by the context below.\n",
    "    Do not make up any assumptions yourself.<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26859748-2961-4947-b3e5-1809a16eb556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Based on the given context, I would say that yes, there is a difference between a multimodal model and a generic Large Language Model (LLM) that handles graphics and text.\n",
      "\n",
      "A multimodal model is specifically designed to handle multiple input modalities such as images, videos, audio, or text, whereas a generic LLM is primarily focused on processing and generating human language (text). While some LLMs may be able to process simple visual inputs like images, they are not typically designed to handle complex multimedia data like videos or graphics.\n",
      "\n",
      "Multimodal models, on the other hand, are trained on large datasets that combine different types of data, allowing them to learn representations that can capture relationships across modalities. This enables them to perform tasks like image captioning, visual question answering, or video summarization, which require integrating information from multiple sources.\n",
      "\n",
      "In contrast, a generic LLM might struggle with these tasks due to its limited training data and lack of explicit design for handling non-textual inputs. However, it's worth noting that some recent advancements in natural language processing have led to the development of multimodal transformers that can integrate both textual and visual information, blurring the lines between traditional LLMs and multimodal models. Nonetheless, the primary distinction remains that multimodal models are explicitly designed to handle diverse input modalities, while generic LLMs focus mainly on text-based applications.\n"
     ]
    }
   ],
   "source": [
    "question = \"Is there a difference between a multimodal model and generic LLM that handles graphics and text?\"\n",
    "context = db.similarity_search_with_relevance_scores(question, k=3)\n",
    "docs = format_docs(context)\n",
    "generation = rag_chain.invoke({\"question\": question, \"context\": docs})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b26ee5c-78d9-4117-b4e8-17bcba8352d5",
   "metadata": {},
   "source": [
    "### 3. Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65872170-cf45-40a1-9370-4302928eeb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search = GoogleSearchAPIWrapper()\n",
    "search = TavilySearchResults(k=3)\n",
    "# search.run(\"Is there a difference in responsive html5 ad and takeover skin?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72061d85-3634-4cec-a761-4ad80021c38b",
   "metadata": {},
   "source": [
    "### 4. Answer Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d3e8315-7677-466a-814d-e9d32ed108ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assessing whether an \n",
    "    answer is useful to resolve a question. Give a binary score 'yes' or 'no' to indicate whether the answer is \n",
    "    useful to resolve a question. Provide the binary score as a JSON with a single key 'score' and no preamble or explanation.\n",
    "     <|eot_id|><|start_header_id|>user<|end_header_id|> Here is the answer:\n",
    "    \\n ------- \\n\n",
    "    {generation} \n",
    "    \\n ------- \\n\n",
    "    Here is the question: {question} <|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\",\n",
    "    input_variables=[\"generation\", \"question\"],\n",
    ")\n",
    "# test it\n",
    "question = \"What kind of ads can a CMP generate? Do I need to use my own designers?\"\n",
    "generation = \"\"\"According to the given context, a Creative Management Platform (CMP) can generate various types of ads, \n",
    "including:\n",
    "\n",
    "* Tailored ad banners\n",
    "* Advertisements showcasing the distinctiveness of one's brand, incorporating brand colors, logo, and slogan.\n",
    "\n",
    "As for whether you need to use your own designers, it seems that CMPs can assist in creating ads, setting them up, \n",
    "and even allowing users to add dynamic data, images, illustrations, videos, etc., which implies that CMPs may not \n",
    "require the involvement of in-house designers. However, this does not necessarily mean that having professional \n",
    "designers would not be beneficial or necessary, especially if you want to customize the designs further or have \n",
    "specific requirements.\n",
    "\"\"\"\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a103f3f8-54bf-440f-ad85-d9ee04004ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_grader.invoke({\"generation\": generation, \"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfc230a-690d-45c5-89a6-ec3afbbcf41c",
   "metadata": {},
   "source": [
    "## The Graph State Class Handles the State\n",
    "\n",
    "While the class holds the state, we define functions as nodes that perform operations on this state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76ea1ba3-8970-4bf0-b537-ea8ca797740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the Graph\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5dde1c-943e-4bf0-9dd3-f61223121ee1",
   "metadata": {},
   "source": [
    "### Define the Nodes and Edges of the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ed8d2ecb-8fc9-4292-ba07-a410131778b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_docs(docs, confidence = 0.85):\n",
    "    return \"\\n\\n\".join(doc[0].page_content for doc in docs if doc[1] > confidence)\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieves documents from the database\n",
    "    Args:\n",
    "        state (GraphState): The current state of the graph\n",
    "    Returns:\n",
    "        new key in the state containing the documents\n",
    "    \"\"\"\n",
    "    documents = db.similarity_search_with_relevance_scores(state[\"question\"], k=3)\n",
    "    question = state[\"question\"]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_combined(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    This function would first combine all documents into one single context\n",
    "    and grade the full text in one go.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): The resulting context document and updated web_search state\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    web_search = \"No\"\n",
    "    # Combine all documents\n",
    "    combined_docs = combine_docs(documents, confidence = 0.8)\n",
    "    # initialise score as a dictionary\n",
    "    score = {}\n",
    "    if combined_docs.strip() == \"\":\n",
    "        score = {\"score\": \"No\"}\n",
    "    else:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": combined_docs})\n",
    "    \n",
    "    if score[\"score\"].lower() == \"yes\":\n",
    "        print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "        documents = Document(page_content=combined_docs)\n",
    "    # Document not relevant\n",
    "    else:\n",
    "        print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "        # We do not return the document\n",
    "        # We set a flag to indicate that we want to run web search\n",
    "        web_search = \"Yes\"\n",
    "    return {\"documents\": documents, \"question\": question, \"web_search\": web_search}\n",
    "    \n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    If any document is not relevant, we will set a flag to run web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Filtered out irrelevant documents and updated web_search state\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d[0].page_content} # 0 is the content, 1 is the confidence\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        # Document relevant\n",
    "        if grade.lower() == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        # Document not relevant\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            # We do not include the document in filtered_docs\n",
    "            # We set a flag to indicate that we want to run web search\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based based on the question\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Appended web results to documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = search.run(question)\n",
    "    print(\"---SEARCH DOCS---\", docs)\n",
    "    \n",
    "    web_results = Document(page_content=docs)\n",
    "    # if documents is not None:\n",
    "    #     documents.append(web_results)\n",
    "    # else:\n",
    "    #     documents = [web_results]\n",
    "    documents = [web_results]\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "# Conditional\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or add web search\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: SOME DOCUMENTS ARE NOT RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n",
    "        )\n",
    "        return \"websearch\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_answer(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: returns a json with yes/ no\n",
    "    \"\"\"\n",
    "    print(\"---CHECKING ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "    score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "    return score[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4298d-cf73-4dd1-bc29-e3be2ece397d",
   "metadata": {},
   "source": [
    "### Add the Nodes to the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0a41f3f-7ec2-4fe8-985f-2bda0d156eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"websearch\", web_search)  # web search\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_combined)  # grade combined\n",
    "workflow.add_node(\"generate\", generate)  # generatae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bd26d1-38b5-449f-9afb-fe8876781843",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "\n",
    "use this diagram for reference. Note that this is the first stage simplified without hallucination grader and initial router. These can be added next.\n",
    "![Graph Agents Diagram](./data/images/graph_agents.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87c55d20-d926-47de-8fb2-29a78ad772bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"websearch\": \"websearch\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"websearch\", \"generate\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_answer,\n",
    "    {\n",
    "        \"yes\": END,\n",
    "        \"no\": \"websearch\",\n",
    "    },\n",
    ")\n",
    "# workflow.add_edge(\"generate\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08c81b01-99cb-4622-9174-cf8d55994cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "---GENERATE---\n",
      "---CHECKING ANSWER---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'key': 'retrieve',\n",
       "  'value': {'question': 'Are there any open sourced multimodal models that we can try?',\n",
       "   'documents': [(Document(page_content=\"papers on multimodal models. Multimodal models are all the rage right now because the two biggest players in the AI race, OpenAI and Google, released multimodal models, right? So OpenAI released GPT-4.0, but a version of it that we haven't really gotten access to, which is this kind of like multimodal version. And then Google released Project Astra, which is basically the same thing, right? You can stream video and audio to the model and the model outputs audio, right? Streams out audio. So we wanted to kind of figure out what is the current kind of state of the art? What's the landscape in multimodal models right now? So one thing that we looked at is the open source world, right? Hugging face kind of represents the open source community. You don't have a ton of money. You can't be doing giant training runs. So whenever people build multimodal models in the open source world, usually they're building what are called VLMs, which are very basic multimodal models, right? They're only\", metadata={'source': './data/sound/text_full.txt', 'start_index': 86379}),\n",
       "     0.8915551570633271),\n",
       "    (Document(page_content=\"that I want to show you guys. And I want to show you guys this one. This is the Google multimodal paper, right? And why is this one cooler? This one is cooler because even though these guys call themselves mixed modal and multimodal, they're not, there's just two modalities here, right? There's just images and text. But this one is images in the form of video, audio, and text. Okay, so now this is actually more what we're talking about when we're talking about GPT-4.0 and Google's Astra, which they announced this week, right? That's just, that's beyond just a vision language model or something that just consumes images and text and outputs text or something that consumes images and text and outputs images and text. This is something that is consuming audio as well. So this is, you're talking to the model, right? How are they doing this? So there's a lot of really cool tricks. This paper is very dense, but I'm going to try to focus on the important points here. Video and audio are\", metadata={'source': './data/sound/text_full.txt', 'start_index': 39208}),\n",
       "     0.8285354408061156),\n",
       "    (Document(page_content=\"This is from Facebook AI Research, aka the meta team Zuck, if you want to think of it that way, 16th May 2024. The other paper we're going to be looking at is What Matters When Building Vision Language Models. This is from Hugging Face. So this is kind of like the open source multimodal model contingent. This is 3rd May 2024, just a little bit earlier than the Facebook paper. And then the last paper that we're going to be looking at is going to be Mirasol 3B, a multimodal autoregressive model for time-aligned and contextual modalities. This is Google, DeepMind, Google Research, same people, 3rd April 2024. So this paper has actually been published a couple times. It was first put on Archive last year, and then they've been kind of publishing updates. But these are the three papers we're going to be looking at, kind of representing the three different, I would say, leaders in this space. You got Google, you got Facebook AI Research, and you got open source represented by Hugging Face.\", metadata={'source': './data/sound/text_full.txt', 'start_index': 1602}),\n",
       "     0.8281000954711344)]}},\n",
       " {'key': 'grade_documents',\n",
       "  'value': {'question': 'Are there any open sourced multimodal models that we can try?',\n",
       "   'web_search': 'No',\n",
       "   'documents': Document(page_content=\"papers on multimodal models. Multimodal models are all the rage right now because the two biggest players in the AI race, OpenAI and Google, released multimodal models, right? So OpenAI released GPT-4.0, but a version of it that we haven't really gotten access to, which is this kind of like multimodal version. And then Google released Project Astra, which is basically the same thing, right? You can stream video and audio to the model and the model outputs audio, right? Streams out audio. So we wanted to kind of figure out what is the current kind of state of the art? What's the landscape in multimodal models right now? So one thing that we looked at is the open source world, right? Hugging face kind of represents the open source community. You don't have a ton of money. You can't be doing giant training runs. So whenever people build multimodal models in the open source world, usually they're building what are called VLMs, which are very basic multimodal models, right? They're only\\n\\nthat I want to show you guys. And I want to show you guys this one. This is the Google multimodal paper, right? And why is this one cooler? This one is cooler because even though these guys call themselves mixed modal and multimodal, they're not, there's just two modalities here, right? There's just images and text. But this one is images in the form of video, audio, and text. Okay, so now this is actually more what we're talking about when we're talking about GPT-4.0 and Google's Astra, which they announced this week, right? That's just, that's beyond just a vision language model or something that just consumes images and text and outputs text or something that consumes images and text and outputs images and text. This is something that is consuming audio as well. So this is, you're talking to the model, right? How are they doing this? So there's a lot of really cool tricks. This paper is very dense, but I'm going to try to focus on the important points here. Video and audio are\\n\\nThis is from Facebook AI Research, aka the meta team Zuck, if you want to think of it that way, 16th May 2024. The other paper we're going to be looking at is What Matters When Building Vision Language Models. This is from Hugging Face. So this is kind of like the open source multimodal model contingent. This is 3rd May 2024, just a little bit earlier than the Facebook paper. And then the last paper that we're going to be looking at is going to be Mirasol 3B, a multimodal autoregressive model for time-aligned and contextual modalities. This is Google, DeepMind, Google Research, same people, 3rd April 2024. So this paper has actually been published a couple times. It was first put on Archive last year, and then they've been kind of publishing updates. But these are the three papers we're going to be looking at, kind of representing the three different, I would say, leaders in this space. You got Google, you got Facebook AI Research, and you got open source represented by Hugging Face.\")}},\n",
       " {'key': 'generate',\n",
       "  'value': {'question': 'Are there any open sourced multimodal models that we can try?',\n",
       "   'generation': '\\n\\nAccording to the given context, yes, there are open-sourced multimodal models that can be tried. Specifically mentioned are:\\n\\n1. \"What Matters When Building Vision Language Models\" from Hugging Face (published on May 3, 2024)\\n2. Mirasol 3B, a multimodal autoregressive model for time-aligned and contextual modalities from Google, DeepMind, and Google Research (first published on Archive last year and updated since)\\n\\nThese models represent the open-source community\\'s efforts in developing multimodal models. However, please note that the context does not provide direct links or access to these models.',\n",
       "   'documents': Document(page_content=\"papers on multimodal models. Multimodal models are all the rage right now because the two biggest players in the AI race, OpenAI and Google, released multimodal models, right? So OpenAI released GPT-4.0, but a version of it that we haven't really gotten access to, which is this kind of like multimodal version. And then Google released Project Astra, which is basically the same thing, right? You can stream video and audio to the model and the model outputs audio, right? Streams out audio. So we wanted to kind of figure out what is the current kind of state of the art? What's the landscape in multimodal models right now? So one thing that we looked at is the open source world, right? Hugging face kind of represents the open source community. You don't have a ton of money. You can't be doing giant training runs. So whenever people build multimodal models in the open source world, usually they're building what are called VLMs, which are very basic multimodal models, right? They're only\\n\\nthat I want to show you guys. And I want to show you guys this one. This is the Google multimodal paper, right? And why is this one cooler? This one is cooler because even though these guys call themselves mixed modal and multimodal, they're not, there's just two modalities here, right? There's just images and text. But this one is images in the form of video, audio, and text. Okay, so now this is actually more what we're talking about when we're talking about GPT-4.0 and Google's Astra, which they announced this week, right? That's just, that's beyond just a vision language model or something that just consumes images and text and outputs text or something that consumes images and text and outputs images and text. This is something that is consuming audio as well. So this is, you're talking to the model, right? How are they doing this? So there's a lot of really cool tricks. This paper is very dense, but I'm going to try to focus on the important points here. Video and audio are\\n\\nThis is from Facebook AI Research, aka the meta team Zuck, if you want to think of it that way, 16th May 2024. The other paper we're going to be looking at is What Matters When Building Vision Language Models. This is from Hugging Face. So this is kind of like the open source multimodal model contingent. This is 3rd May 2024, just a little bit earlier than the Facebook paper. And then the last paper that we're going to be looking at is going to be Mirasol 3B, a multimodal autoregressive model for time-aligned and contextual modalities. This is Google, DeepMind, Google Research, same people, 3rd April 2024. So this paper has actually been published a couple times. It was first put on Archive last year, and then they've been kind of publishing updates. But these are the three papers we're going to be looking at, kind of representing the three different, I would say, leaders in this space. You got Google, you got Facebook AI Research, and you got open source represented by Hugging Face.\")}}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = workflow.compile()\n",
    "inputs = {\"question\": \"Are there any open sourced multimodal models that we can try?\"}\n",
    "output = app.stream(inputs)\n",
    "response = []\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        response.append({\"key\": key, \"value\": value})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f68cf7df-2cda-4da1-8e19-ad916334880f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "According to the given context, yes, there are open-sourced multimodal models that can be tried. Specifically mentioned are:\n",
      "\n",
      "1. \"What Matters When Building Vision Language Models\" from Hugging Face (published on May 3, 2024)\n",
      "2. Mirasol 3B, a multimodal autoregressive model for time-aligned and contextual modalities from Google, DeepMind, and Google Research (first published on Archive last year and updated since)\n",
      "\n",
      "These models represent the open-source community's efforts in developing multimodal models. However, please note that the context does not provide direct links or access to these models.\n"
     ]
    }
   ],
   "source": [
    "print(output['generate']['generation'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
