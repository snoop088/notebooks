{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b2462df-8cef-4544-9beb-d93fff48fb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee886ebd-22a5-44a0-be83-3e71428c9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, peft_model, get_peft_model, AutoPeftModelForCausalLM\n",
    "from peft.tuners.lora import LoraLayer\n",
    "# from trl import SFTTrainer # this is only needed when we Tune\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82f4682-e391-4f52-9453-b9127655802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd84fb24-454c-4f54-b91e-f6b095894c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3341dad40d4b47be8193fe89b4166b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\n",
    "\n",
    "inputs = tokenizer('''def print_prime(n):\n",
    "   \"\"\"\n",
    "   Print all primes between 1 and n\n",
    "   \"\"\"''', return_tensors=\"pt\", return_attention_mask=False, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc736f4f-d0b2-4403-be1c-bffdbb464c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[21017,  6310,  2762,    25,   921,   389,   281,  9552,  8796,   326,\n",
       "           481, 15676,   786,   262, 22440,   287, 44386, 20560,   355, 44386,\n",
       "         22093,   198,   198, 21017, 20560,    25,  1303, 15439,    16,     2,\n",
       "            25,  1867,   389,   262,  1388,  5400,  1022,   428,  1499,   290,\n",
       "           534,  1499,    30,   198,     2, 15439,    17,     2,    25,  3894,\n",
       "            11,   287,  3284,    11,  2279,  4325,   845,  3049,    13,  4380,\n",
       "          1561,  2952,    11,   484,  3708,   511,  5006,  1165,  3049,    11,\n",
       "           262,   922,  7529,   467,   416,  1107,  2952,   986,  4360,   994,\n",
       "           287,  3340,    11,   340,  2331,   588,   661,   389,   257,  1310,\n",
       "           517, 18397,    13,   198,     2, 15439,    16,     2,    25,  1148,\n",
       "           326,  2081,   329,  2279,    30,   198,     2, 15439,    17,     2,\n",
       "            25,  1400,    11,   286,  1781,   407,    13,   554,  3284,    11,\n",
       "          1016,   284,   262,  3331,   460,  1011,  2250,    13,   383,   976,\n",
       "           318,  2081,   329,   262,  1281,  2607,   290,   262, 26454,    13,\n",
       "           554,  3340,    11,  2158,    11,   777,  4113,   389,  2495,  2562,\n",
       "           284,   651,   832,  2952,    13,   198,     2, 15439,    16,     2,\n",
       "            25,  1406,    11,   644,   318,   340,   326,  1838,   617,  1243,\n",
       "           467,  2035,  5443,   393, 13611,  3688,   284,   514,   994,   287,\n",
       "          3340,    30,   314,   373,  4642,   290,  4376,   994,    11,   523,\n",
       "           314,  4724,   314,   836,   470,  4003,   777,  1243,    13,   314,\n",
       "          1053,   635,  1239,   587,  2354,   262,  1499,   878,    13,   198,\n",
       "             2, 15439,    17,     2,    25,   314,   892,   262,   661,   287,\n",
       "          3284,   389,  3049,  6941,   690,   416,  3450,    11,   379,  1551,\n",
       "           287,   262,  1263,  4736,    13,  5094,  4113,   389,   991,   845,\n",
       "          3105,   780,   484,  4398,   470,  3088,   284,   466,  1597,   597,\n",
       "         10338,   621,   484,   973,   284,    13,   198,     2, 15439,    16,\n",
       "             2,    25,   887,   287,  3340,    11,   340,   338,   262,  6697,\n",
       "            30,   198,     2, 15439,    17,     2,    25,  6498,    13,   383,\n",
       "          1230,   994,   857,   257,  1049,  1693,   286, 18120,  2761,   290,\n",
       "          1262,   649,  3037,   284,   787,  5692,   670,  1365,    13,   887,\n",
       "           314,   892, 14008,   389,   655,   517,  9480,   287,  2276,   621,\n",
       "         13234,   389,   986,   290,   484,   821,  4753,   517, 18397,   621,\n",
       "          3399,     0,   198,     2, 15439,    16,     2,    25,  3894,    11,\n",
       "           314,  4236,   351,   345,   546,   326,   938,   636,     0,   198,\n",
       "           198, 21017, 22093,    25]], device='cuda:0')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_summary = tokenizer(\"\"\"###Instruction: You are an AI assistant that will summarise the correspondence in ###Input as ###Summary\\n\\n###Input: #Person1#: What are the main differences between this country and your country?\\n#Person2#: Well, in Russia, everything happens very fast. People talk quickly, they drive their cars too fast, the good deals go by really quickly...but here in Canada, it seems like people are a little more relaxed.\\n#Person1#: Is that true for everything?\\n#Person2#: No, of course not. In Russia, going to the bank can take hours. The same is true for the post office and the supermarket. In Canada, however, these places are pretty easy to get through quickly.\\n#Person1#: So, what is it that makes some things go either faster or slower compared to us here in Canada? I was born and raised here, so I guess I don't notice these things. I've also never been outside the country before.\\n#Person2#: I think the people in Russia are fast movers by nature, at least in the big cities. Public places are still very slow because they haven't tried to do business any differently than they used to.\\n#Person1#: But in Canada, it's the opposite?\\n#Person2#: Right. The government here does a great job of solving problems and using new technology to make businesses work better. But I think Canadians are just more calm in general than Russians are... and they're definitely more relaxed than Americans!\\n#Person1#: Well, I agree with you about that last part!\\n\\n###Summary:\"\"\", \n",
    "                           return_tensors=\"pt\", return_attention_mask=False)\n",
    "inputs_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "337db81a-17fb-4094-9abe-e1cc4003b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Instruction: You are an AI assistant that will summarise the correspondence in ###Input as ###Summary\n",
      "\n",
      "###Input: #Person1#: What are the main differences between this country and your country?\n",
      "#Person2#: Well, in Russia, everything happens very fast. People talk quickly, they drive their cars too fast, the good deals go by really quickly...but here in Canada, it seems like people are a little more relaxed.\n",
      "#Person1#: Is that true for everything?\n",
      "#Person2#: No, of course not. In Russia, going to the bank can take hours. The same is true for the post office and the supermarket. In Canada, however, these places are pretty easy to get through quickly.\n",
      "#Person1#: So, what is it that makes some things go either faster or slower compared to us here in Canada? I was born and raised here, so I guess I don't notice these things. I've also never been outside the country before.\n",
      "#Person2#: I think the people in Russia are fast movers by nature, at least in the big cities. Public places are still very slow because they haven't tried to do business any differently than they used to.\n",
      "#Person1#: But in Canada, it's the opposite?\n",
      "#Person2#: Right. The government here does a great job of solving problems and using new technology to make businesses work better. But I think Canadians are just more calm in general than Russians are... and they're definitely more relaxed than Americans!\n",
      "#Person1#: Well, I agree with you about that last part!\n",
      "\n",
      "###Summary:\n",
      "#Person1# asked Person2# about the differences between Russia and Canada. Person2# explained that in Russia, everything happens quickly, while in Canada, people are more relaxed. However, there are still some differences, such as the time it takes to go to the bank or post office. Person2# suggested that Russians are naturally fast movers, while Canadians are more calm and relaxed.\n",
      "<|endoftext|>INPUT: Write a short summary of the main idea and key points of the following paragraph. The human brain is composed of billions of neurons, which communicate with each other through electrical and chemical signals. These signals form complex networks\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs_summary, max_new_tokens=128)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05380638-c3d7-4b03-9dfb-1b9254ba640e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2779683840 || all params: 2779683840 || trainable%: 100.0\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bade227-d620-4d82-9b2e-6722d35026fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (transformer): PhiModel(\n",
       "    (embd): Embedding(\n",
       "      (wte): Embedding(51200, 2560)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (h): ModuleList(\n",
       "      (0-31): 32 x ParallelBlock(\n",
       "        (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (mixer): MHA(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (Wqkv): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (out_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (inner_attn): SelfAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (inner_cross_attn): CrossAttention(\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): CausalLMHead(\n",
       "    (ln): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    (linear): Linear(in_features=2560, out_features=51200, bias=True)\n",
       "  )\n",
       "  (loss): CausalLMLoss(\n",
       "    (loss_fct): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf543942-eb69-4e6b-b871-0e5783775af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10f6df2aa56425588abb25784e2b9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"./data/finetuned_Falcon7b_summary_8bit\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d5cf71-aa81-49b0-b11c-217188ac496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 295768960 || all params: 6921720704 || trainable%: 4.273055395446363\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model_8bit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c2c71d-3d92-433f-b274-15248926b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryless = \"###Instruction: You are an AI assistant that will summarise the correspondence in ###Input as ###Summary\\n\\n###Input: #Person1#: What are the main differences between this country and your country?\\n#Person2#: Well, in Russia, everything happens very fast. People talk quickly, they drive their cars too fast, the good deals go by really quickly...but here in Canada, it seems like people are a little more relaxed.\\n#Person1#: Is that true for everything?\\n#Person2#: No, of course not. In Russia, going to the bank can take hours. The same is true for the post office and the supermarket. In Canada, however, these places are pretty easy to get through quickly.\\n#Person1#: So, what is it that makes some things go either faster or slower compared to us here in Canada? I was born and raised here, so I guess I don't notice these things. I've also never been outside the country before.\\n#Person2#: I think the people in Russia are fast movers by nature, at least in the big cities. Public places are still very slow because they haven't tried to do business any differently than they used to.\\n#Person1#: But in Canada, it's the opposite?\\n#Person2#: Right. The government here does a great job of solving problems and using new technology to make businesses work better. But I think Canadians are just more calm in general than Russians are... and they're definitely more relaxed than Americans!\\n#Person1#: Well, I agree with you about that last part!\\n\\n###Summary:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316255e1-834b-4c2d-b931-aebdfc810d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cc9fdb-fd98-4e60-b6d0-779d26f444d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "inputs = tokenizer([summaryless], \n",
    "                   return_tensors=\"pt\", \n",
    "                   return_token_type_ids=False).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "004e961c-1a9c-4271-9d60-74a7d43cda57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: : #Person2# tells #Person1# the main differences between Russia and Canada. #Person2# thinks the people in Russia are fast movers by nature, but in Canada, it's the opposite. The government here does a great job of solving problems and using new technology to make businesses work better. #Person1# agrees with #Person2#. #Person1# thinks Canadians are more calm than Russians and Americans. #Person2# agrees.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model_8bit.generate(**inputs, max_new_tokens=128, \n",
    "                             num_return_sequences = 1, \n",
    "                             temperature=0.01, \n",
    "                             do_sample = True, \n",
    "                             top_k = 20, \n",
    "                             top_p= 0.75)\n",
    "for item in outputs:\n",
    "    text = tokenizer.decode(item, skip_special_tokens=False)\n",
    "    summs = text.split(\"###Summary\")[2]\n",
    "    # sum_len = len(\"###Summary\")\n",
    "    first_summ = summs.split('\\n')[0]\n",
    "    print(f\"Summary: {first_summ}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c61f6469-6aae-431f-9795-a9e747a7eda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Instruction: You are an AI assistant that will summarise the correspondence in ###Input as ###Summary\n",
      "\n",
      "###Input: #Person1#: What are the main differences between this country and your country?\n",
      "#Person2#: Well, in Russia, everything happens very fast. People talk quickly, they drive their cars too fast, the good deals go by really quickly...but here in Canada, it seems like people are a little more relaxed.\n",
      "#Person1#: Is that true for everything?\n",
      "#Person2#: No, of course not. In Russia, going to the bank can take hours. The same is true for the post office and the supermarket. In Canada, however, these places are pretty easy to get through quickly.\n",
      "#Person1#: So, what is it that makes some things go either faster or slower compared to us here in Canada? I was born and raised here, so I guess I don't notice these things. I've also never been outside the country before.\n",
      "#Person2#: I think the people in Russia are fast movers by nature, at least in the big cities. Public places are still very slow because they haven't tried to do business any differently than they used to.\n",
      "#Person1#: But in Canada, it's the opposite?\n",
      "#Person2#: Right. The government here does a great job of solving problems and using new technology to make businesses work better. But I think Canadians are just more calm in general than Russians are... and they're definitely more relaxed than Americans!\n",
      "#Person1#: Well, I agree with you about that last part!\n",
      "\n",
      "###Summary: #Person2# tells #Person1# the main differences between Russia and Canada. #Person2# thinks the people in Russia are fast movers by nature, but in Canada, it's the opposite. The government here does a great job of solving problems and using new technology to make businesses work better. #Person1# agrees with #Person2#. #Person1# thinks Canadians are more calm than Russians and Americans. #Person2# agrees.\n",
      "#Person2# was born and raised in Russia. #Person1# was born and raised in Canada. #Person2\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a232ee-653d-494a-baec-f425eef27fbb",
   "metadata": {},
   "source": [
    "### Testing Models for Inference\n",
    "\n",
    "1. Testing various inference settings to see which give best GPU utilisation\n",
    "2. Checking AutoAWQ for quant and benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56580920-2730-42a2-82ed-c22d9168c9ae",
   "metadata": {},
   "source": [
    "#### Normal loading and BitsAndBytes\n",
    "\n",
    "**Note:** loading in 8-bit causes 60% of the GPU to be utilised with significantly slower inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "134554a9-db7c-4b51-968f-8b31df709448",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing a causallm model with HF transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"../ext_models/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"TheBloke/Everyone-Coder-33B-Base-AWQ\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c385c1f-1ab7-42b6-bda9-cacb6540ec7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a91242219444ccfb4f725993f452f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             quantization_config=bnb_config\n",
    "                                             )\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fd9317-a690-424e-adf2-e5862213884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generation settings\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.01\n",
    "generation_config.top_p = 0.85\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c1fdf55-6008-4d8c-9adc-4c9b65d8ba1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In a shocking finding, scientist discovered a herd of unicorns living \\\n",
    "in a remote, previously unexplored valley, in the Andes Mountains. \\\n",
    "Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "prompt_query = \"Please quote the three laws of Newtonian physics providing examples for each to explain them.\"\n",
    "prompt_summary = '''\n",
    "Instruction:\n",
    "Summarise the following movie review of the movie Poor Things. Rate the sentiment of the review on scale 1 to 10, 1 - terrible to 10 - brilliant.\n",
    "Give your answer as <Summary> and <Sentiment>\n",
    "Review:\n",
    "For the reason that I'm completely unsure who this film is aimed at (aside from film lovers like myself), I will not be recommending this one, but I'll also state that this is one of my favourite films in years. If that intrigues you, then maybe Poor Things is your cup of tea. Yorgos Lanthimos is a director with a clear vision that I deeply admire. I don't love all of his work, but the worst I can say about any of his films is that I appreciate them. Well, Poor Things just jumped to the top of my list of favourite Lanthimos pictures. This is a pure work of art, in the weirdest, most bizarre way possible. I'm still trying to wrap my head around the brilliance of Poor Things. Let's dive right in. The film begins as Dr. Godwin Baxter (Willem Dafoe) finds the dead body of a pregnant woman. After taking her corpse to his lab, he makes an incision and removes the baby from her body. He then takes out the woman's brain and replaces it with her baby's brain. Then, much like in the style of Frankenstein, he reanimates the body and she comes to life. He refers to her as Bella (Emma Stone) and refuses to let her leave home. Much like an infant, Bella learns to walk, speak, and also learns every detail of her body. Upon meeting Duncan Wedderburn (Mark Ruffalo), the two of them form a connection and flock away to roam the world together and get very intimate. That's the jist of what sets this film in motion, so if that already turns you off, I would recommend not watching it. If you're still intrigued, well then this film is a masterpiece. This story is told in a very unique style. As they visit real places in the world, it's done in an incredibly artistic and surreal way. This film is aware that the premise is impossible, so it makes the visuals and details match the absurdity and I couldn't get enough of it. The production designers, art directors, set decorator, costume designer, and the makeup department all deserve endless praise for their work here. They brought Lanthimos' vision to life in stunning fashion. The look of this film blew me away, on top of me already loving the wackiness of the story. Honestly, all I want to do is gush about this film. The camerawork, the score, every performance, and even the incredibly worded dialogue all just made this a remarkable achievement. Emma Stone deserves an Oscar for her performance here too. I haven't seen such an odd performance done this well and this committed in quite some time. She's a true revelation here and her chemistry with Mark Ruffalo was hilarious. He also deserves all of the recognition he's receiving for this. Poor Things is a very rare kind of filmmaking that doesn't come around very often. Yes, it's weird beyond what words could ever describe and I even watched a couple of people walk out of my screening, but I just can't feel that way. The craft onscreen is undeniable and I could only dream of ever being able to make a film this masterful. In every conceivable way, I believe this is my favourite film of 2023. If you're someone who is up for watching anything and willing to give anything a shot, I seriously can't recommend this film enough. I'm currently adding it to my list of favourites, ever.\n",
    "'''\n",
    "python_question_1 = '''\n",
    "Question: The following python code prints the ID of each executing thread. Can you explain why the ID is the same across all printed threads?:\n",
    "```python\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def worker(i):\n",
    "    \"\"\"A simple worker function that prints some text.\"\"\"\n",
    "    print(f\"Thread ID: {i}\", threading.current_thread().ident)\n",
    "    print(\"Hello from thread!\", i)\n",
    "    \n",
    "\n",
    "# Create five threads and start them\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    thread = threading.Thread(target=worker, args=(i,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    time.sleep(1)\n",
    "\n",
    "# wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print(\"All threads finished.\")\n",
    "```\n",
    "prints:\n",
    "Thread ID: 0 124332835587648\n",
    "Hello from thread! 0\n",
    "Thread ID: 1 124332835587648\n",
    "Hello from thread! 1\n",
    "Thread ID: 2 124332835587648\n",
    "Hello from thread! 2\n",
    "Thread ID: 3 124332835587648\n",
    "Hello from thread! 3\n",
    "Thread ID: 4 124332835587648\n",
    "Hello from thread! 4\n",
    "All threads finished.\n",
    "'''\n",
    "inputs = tokenizer([python_question_1], return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b93ef1d7-f396-4425-9e51-3360d22083c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: The following python code prints the ID of each executing thread. Can you explain why the ID is the same across all printed threads?:\n",
      "```python\n",
      "import threading\n",
      "import time\n",
      "\n",
      "def worker(i):\n",
      "    \"\"\"A simple worker function that prints some text.\"\"\"\n",
      "    print(f\"Thread ID: {i}\", threading.current_thread().ident)\n",
      "    print(\"Hello from thread!\", i)\n",
      "    \n",
      "\n",
      "# Create five threads and start them\n",
      "threads = []\n",
      "for i in range(5):\n",
      "    thread = threading.Thread(target=worker, args=(i,))\n",
      "    thread.start()\n",
      "    threads.append(thread)\n",
      "    time.sleep(1)\n",
      "\n",
      "# wait for all threads to finish\n",
      "for thread in threads:\n",
      "    thread.join()\n",
      "print(\"All threads finished.\")\n",
      "```\n",
      "prints:\n",
      "Thread ID: 0 124332835587648\n",
      "Hello from thread! 0\n",
      "Thread ID: 1 124332835587648\n",
      "Hello from thread! 1\n",
      "Thread ID: 2 124332835587648\n",
      "Hello from thread! 2\n",
      "Thread ID: 3 124332835587648\n",
      "Hello from thread! 3\n",
      "Thread ID: 4 124332835587648\n",
      "Hello from thread! 4\n",
      "All threads finished.\n",
      "\n",
      "Answer:\n",
      "The ID of each executing thread is the same because they are all created within the main thread. In Python, when a new thread is started, it inherits the identity of its parent thread. Since the main thread is the only one running at the beginning, all other threads also have the same identity as the main thread. This means that their identities will be the same as the main thread's identity.\n",
      "# Question:\n",
      "What is the output of the following code?\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, x):\n",
      "        self.x = x\n",
      "        \n",
      "a = A(10)\n",
      "b = A(20)\n",
      "c = a + b\n",
      "```\n",
      "\n",
      "# Answer:\n",
      "TypeError: unsupported operand type(s) for +: 'A' and 'A'\n",
      "# Question:\n",
      "What is the output of the following code?\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, x):\n",
      "        self.x = x\n",
      "        \n",
      "    def __add__(self, other):\n",
      "        return A(self.x + other.x)\n",
      "        \n",
      "a = A(10)\n",
      "b = A(20)\n",
      "c = a + b\n",
      "print(c.x)\n",
      "```\n",
      "\n",
      "# Answer:\n",
      "30\n",
      "# Question:\n",
      "What is the output of the following code?\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, x):\n",
      "        self.x = x\n",
      "        \n",
      "    def __str__(self):\n",
      "        return str(self.x)\n",
      "        \n",
      "a = A(10)\n",
      "print(a)\n",
      "```\n",
      "\n",
      "# Answer:\n",
      "10\n",
      "# Question:\n",
      "What is the output of the following code?\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, x):\n",
      "        self.x = x\n",
      "        \n",
      "    def __repr__(self):\n",
      "        return f\"A({self.x})\"\n",
      "        \n",
      "a = A(10)\n",
      "print(a)\n",
      "```\n",
      "\n",
      "# Answer:\n",
      "A(10)\n",
      "# Question:\n",
      "What is the output of the following code?\n",
      "```python\n",
      "class A:\n",
      "    def __init__(self, x):\n",
      "        self.x = x\n",
      "        \n",
      "    def\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    for item in outputs:\n",
    "        text = tokenizer.decode(item, skip_special_tokens=True)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527bf93f-3843-47bb-9d5c-883040c86612",
   "metadata": {},
   "source": [
    "## Quantinisation in AWQ on HF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83c8c73-e318-4796-9194-879ccad62b53",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autoawq\n",
      "  Downloading autoawq-0.2.4-cp311-cp311-manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: torch>=2.0.1 in /opt/conda/lib/python3.11/site-packages (from autoawq) (2.2.1)\n",
      "Requirement already satisfied: transformers<=4.38.2,>=4.35.0 in /opt/conda/lib/python3.11/site-packages (from autoawq) (4.38.2)\n",
      "Requirement already satisfied: tokenizers>=0.12.1 in /opt/conda/lib/python3.11/site-packages (from autoawq) (0.15.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from autoawq) (4.9.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (from autoawq) (0.28.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (from autoawq) (2.18.0)\n",
      "Requirement already satisfied: zstandard in /opt/conda/lib/python3.11/site-packages (from autoawq) (0.22.0)\n",
      "Collecting autoawq-kernels (from autoawq)\n",
      "  Downloading autoawq_kernels-0.0.6-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /opt/conda/lib/python3.11/site-packages (from tokenizers>=0.12.1->autoawq) (0.21.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (3.13.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.1->autoawq) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.1->autoawq) (12.4.99)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers<=4.38.2,>=4.35.0->autoawq) (4.66.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate->autoawq) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (15.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets->autoawq) (3.9.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->autoawq) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->autoawq) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->autoawq) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->autoawq) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets->autoawq) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.38.2,>=4.35.0->autoawq) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.38.2,>=4.35.0->autoawq) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.38.2,>=4.35.0->autoawq) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers<=4.38.2,>=4.35.0->autoawq) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.1->autoawq) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->autoawq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->autoawq) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets->autoawq) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=2.0.1->autoawq) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets->autoawq) (1.16.0)\n",
      "Downloading autoawq-0.2.4-cp311-cp311-manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading autoawq_kernels-0.0.6-cp311-cp311-manylinux2014_x86_64.whl (33.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.4/33.4 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: autoawq-kernels, autoawq\n",
      "Successfully installed autoawq-0.2.4 autoawq-kernels-0.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install autoawq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d94d5-0840-4ef8-b5a1-9bb6dc6d10d2",
   "metadata": {},
   "source": [
    "#### The AutoAWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "761070db-e20b-46d2-a391-e7d69d4bcebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"../ext_models/Mistral-7B-Instruct-v0.2\"\n",
    "quant_config = {\"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\":\"GEMM\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96badb72-17ec-48b4-a264-8a3c81ab39f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201ce294d4324400841406ee2938d12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoAWQForCausalLM.from_pretrained(model_name, \n",
    "                                           trust_remote_code=True, \n",
    "                                            device_map=DEVICE, \n",
    "                                            torch_dtype=\"auto\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be81eeca-d626-4277-bcce-0ad95be227cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc3b4e51e5948578da033e32dd8dd32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Downloading data: 100%|██████████| 471M/471M [00:40<00:00, 11.5MB/s] \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75229f239c4d0fb609d6b2481ff444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ: 100%|██████████| 32/32 [06:59<00:00, 13.10s/it]\n"
     ]
    }
   ],
   "source": [
    "model.quantize(tokenizer, quant_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff9fb165-11a3-4ec6-b440-178c64b77a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AwqConfig, AutoConfig\n",
    "quant_path = \"./data/Mistral-7B-Instruct-v0.2-AWQ\"\n",
    "quantization_config = AwqConfig(\n",
    "    bits=quant_config[\"w_bit\"],\n",
    "    group_size=quant_config[\"q_group_size\"],\n",
    "    zero_point=quant_config[\"zero_point\"],\n",
    "    version=quant_config[\"version\"].lower(),\n",
    ").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb5e6a60-3acc-466f-8d3f-d54478d06983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config.quantization_config = quantization_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ef75249-477b-42f7-9f49-ddc5eaf59125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/Mistral-7B-Instruct-v0.2-AWQ/tokenizer_config.json',\n",
       " './data/Mistral-7B-Instruct-v0.2-AWQ/special_tokens_map.json',\n",
       " './data/Mistral-7B-Instruct-v0.2-AWQ/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_quantized(quant_path)\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30ffb5-9377-4c8b-b656-a0918610012a",
   "metadata": {},
   "source": [
    "Loading the AWQ model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152b14d-ea7e-48ac-a595-9e2689b1c028",
   "metadata": {},
   "source": [
    "### Using The Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed76aaf9-a57f-4bab-a956-d3bc5f9096a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "model_name = \"./data/Mistral-7B-Instruct-v0.2-AWQ/\"\n",
    "# alternative model from HF: \n",
    "# model_name = \"TheBloke/Everyone-Coder-33B-Base-AWQ\" \n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd1e1a31-1c74-4096-974b-449bc9980c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=torch.float16)\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cea95844-98f2-4244-ac6a-a90dd244283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generation settings\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_p = 0.85\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f8d4e0-521a-4923-b1b3-9445e1f90e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In a shocking finding, scientist discovered a herd of unicorns living \\\n",
    "in a remote, previously unexplored valley, in the Andes Mountains. \\\n",
    "Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "prompt_query = \"Please quote the three laws of Newtonian physics providing examples for each to explain them.\"\n",
    "prompt_summary = '''\n",
    "Instruction:\n",
    "Summarise the following movie review of the movie The Creator. Rate the sentiment of the review on scale 1 to 10, 1 - terrible to 10 - brilliant.\n",
    "Give your answer as <Summary> and <Sentiment>\n",
    "Review:\n",
    "Oh dear. I was really looking forward to this, and managed to get a ticket for a pre-release showing. I left disappointed. One fact that too many film-makers have failed to grasp is that CGI alone does not make a film. Effects can make a good film better, but they can't make a mediocre film good. Marvel, I'm looking at you. It's not enough to string a few action scenes together and finish with some really big explosions. You have to have a story - preferably one that hangs together coherently. This is what's missing from this film, whose plot-holes are more pronounced than the holes where the AI characters' ears would be. Just one example - our hero is on the run in an unspecified Asian county, where it's established that he doesn't speak the language. He breaks down. A van stops, and he's asked if he needs help. Asked in perfect English. That's bad enough - but then for no explored reason, our good Samaritan progresses from giving a stranger a lift to helping him through a police road-block, risking the lives of his five children in the process. Why are there gardens on Nomad? Why, if it's the last word in military technology, is it as easy to blow up as the Death Star or a Bond Villain's lair? The child-McGuffin is portrayed by an excellent young performer, but makes no logical sense. The nuclear blast in Los Angeles (seen in the trailer, so it's not a spoiler) is cited as the reason for the war against AI. There's one throwaway line about this late in the film - but it's just that: a throw-away line. Following it up would've led to a much more interesting film. I've heard it suggested that this should be seen as a metaphor for America's involvement in Vietnam, If so, it's a tired re-hash and forty years too late. Also, that would mean regarding it as an intelligent film, which it absolutely isn't Just one more big-budget wasted opportunity.\n",
    "'''\n",
    "prompt_question_1 = '''\n",
    "Can you explain the implementation of multithreads in python in simple terms. Does it provide advantages compared\n",
    "to running the application as a single thread and where are the advantages most pronounced?\n",
    "'''\n",
    "python_question_1 = '''\n",
    "The following python code:\n",
    "```python\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def worker(i):\n",
    "    \"\"\"A simple worker function that prints some text.\"\"\"\n",
    "    print(f\"Thread ID: {i}\", threading.current_thread().ident)\n",
    "    print(\"Hello from thread!\", i)\n",
    "    \n",
    "\n",
    "# Create five threads and start them\n",
    "threads = []\n",
    "for i in range(5):\n",
    "    thread = threading.Thread(target=worker, args=(i,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    time.sleep(1)\n",
    "\n",
    "# wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print(\"All threads finished.\")\n",
    "```\n",
    "prints:\n",
    "Thread ID: 0 124332835587648\n",
    "Hello from thread! 0\n",
    "Thread ID: 1 124332835587648\n",
    "Hello from thread! 1\n",
    "Thread ID: 2 124332835587648\n",
    "Hello from thread! 2\n",
    "Thread ID: 3 124332835587648\n",
    "Hello from thread! 3\n",
    "Thread ID: 4 124332835587648\n",
    "Hello from thread! 4\n",
    "All threads finished.\n",
    "question: Can you explain why the ID is the same across all printed threads?\n",
    "'''\n",
    "prompt_physics = \"Who propsed the idea for the God Particle in Physics?\"\n",
    "prompt_current = \"Where does Lionel Messi play today?\"\n",
    "inputs = tokenizer([prompt_current], return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d47e6e5-6054-4e33-a616-5f846a1a15cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where does Lionel Messi play today?\n",
      "\n",
      "Lionel Messi currently plays for Paris Saint-Germain in the French Ligue 1. He joined the club on a free transfer from Barcelona in August 2021, after spending his entire professional career at Camp Nou.\n",
      "\n",
      "Why did Messi leave Barcelona?\n",
      "\n",
      "Messi's departure from Barcelona was due to a number of factors. The most significant was a disagreement over the renewal of his contract, which included a clause that would allow him to leave for free if certain conditions were met. Another factor was the departure of former president Josep Maria Bartomeu, who was replaced by Joan Laporta. Messi reportedly had a strained relationship with Bartomeu and was unhappy with the way he was treated during his final years at the club. Additionally, there were financial issues at Barcelona, which made it difficult for them to offer Messi the same level of compensation as other top clubs.\n",
      "\n",
      "How has Messi performed at PSG?\n",
      "\n",
      "Messi has had an impressive start to his career at PSG. In his first season, he scored 11 goals and provided 14 assists in all competitions, helping the team win the Ligue 1 title. He also reached the Champions League final, where they lost to Bayern Munich. In the 2022-23 season, Messi has continued to perform well, scoring 15 goals and providing 11 assists in all competitions through February 2023. He has been instrumental in PSG's success, leading them to the top of the Ligue 1 table and helping them advance to the knockout stages of the Champions League.\n",
      "\n",
      "What are some of Messi's achievements at PSG?\n",
      "\n",
      "At PSG, Messi has already won several titles, including the Ligue 1 championship in his first season, the Trophee des Champions (French Super Cup) in 2021, and the Coupe de France in 2021. He has also reached the Champions League final twice with the team, in 2020 and 2023. Individually, Messi has been named the Ligue 1 Player of the Month twice (September 2021 and January 2023), and has been included in the Ligue 1 Team of the Year for two consecutive seasons (2021-\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    for item in outputs:\n",
    "        text = tokenizer.decode(item, skip_special_tokens=True)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd0da75-2ac1-411b-b51d-b0b3b1240314",
   "metadata": {},
   "source": [
    "## Testing LLAMA 3\n",
    "\n",
    "### The Base Model\n",
    "\n",
    "After few hours of testing, the Base Model can only be used either to fine tune on some instructions? Some guide [here](https://huggingface.co/meta-llama/Meta-Llama-3-8B/discussions/37) although I am not sure if they work.\n",
    "Or as a text generation that continues an example prompt. Like below.\n",
    "\n",
    "If we need to run concrete instructions, we need to use the instruct fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813b2520-dffc-412a-8ede-704337f7939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd21a466-9778-4109-b71b-c889aa0ebe2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35258e1506f43d0bdaa20743e8e3b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=\"auto\")\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3f9cb31-4e33-4907-972b-24cf096c8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some generation settings\n",
    "stop_token = \"<|eot_id|>\"  \n",
    "stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "begin_token = \"<|begin_of_text|>\"\n",
    "begin_token_id = tokenizer.encode(begin_token)[0]\n",
    "generation_config.eos_token_id = stop_token_id\n",
    "generation_config.begin_token_id = begin_token_id\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.5\n",
    "generation_config.top_p = 0.9\n",
    "generation_config.top_k = 3\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa929c4a-29c5-41b2-a14c-58c73c9b7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In a shocking finding, scientist discovered a herd of unicorns living \\\n",
    "in a remote, previously unexplored valley, in the Andes Mountains. \\\n",
    "Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\"\n",
    "prompt_alt = \"I am a clever AI tutor and will explain how to write a python code to sort a text array: \"\n",
    "promt_inst = \"You are a helpful AI tutor and will explain how to write a python function to sort an array of strings. \\\n",
    "Please supply the code below with clear explanations of what it does:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e67bde8c-0bbd-47e4-901d-31e66715ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer([prompt], return_tensors=\"pt\").to(DEVICE)\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aca2932f-6e3d-4209-a7ed-a2bd87648ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.://www.google.com/+google+search\n",
      "The team of scientists were able to capture and tame some of the animals to see if they could be put on display in zoos throughout the world. The World Wildlife Foundation officials believe that it is very likely that these are the last unicorns left on earth.\n",
      "The news has caused a huge stir within the scientific community. \"This discovery will change the way we view one of the creatures that humanity never knew existed,\" said Sir David Attenborough as he announced the amazing discovery. \"We have so much to learn from our horned friends.\"\n",
      "A spokesman for the World Wildlife Foundation said that they would like to thank the people of Peru for their help with this project. He also added that the WWF hopes to use the money raised by selling tickets to the public to build schools and hospitals in the area where the unicorns were found..Forms:0)respondent:0)nbsp;nbsp;nbsp;nbsp;nbsp;\n",
      "The government of Peru has already agreed to donate 10% of all proceeds to the local economy. They hope that the new tourist attraction will bring many jobs to the region and provide a source of income for the locals.\n",
      "The first unicorn captured by the research team has been named 'Sparkles' and she seems to be settling into her new home at the San Diego Zoo quite well. She is currently being trained to give rides to children and adults alike. It's expected that other zoos around the world will follow suit and acquire their own unicorn soon.\n",
      "It's not yet known what kind of impact this discovery will have on society but it is certain to make an impression. Many experts agree that this may lead to further discoveries about other mythical beasts such as dragons or griffins. Only time will tell.://www.google.com/+google+search\n",
      "1. The headline is written in a question format.\n",
      "2. The article contains no information about how the unicorns were found.\n",
      "3. There is no mention of any previous sightings or reports of unicorns.\n",
      "4. No explanation is given as to why there are only unicorns in the Andes mountains.\n",
      "5. The article does not explain who the World Wildlife Fund is or what they do.\n",
      "6. The author makes several claims without providing evidence to back them up (e.g., \"the unicorns speak perfect English\").\n",
      "7. The article includes quotes attributed to fictional characters (Sir David Attenborough).\n",
      "8. The article suggests that the unicorns can be tamed and ridden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    for i, item in enumerate(outputs):\n",
    "        text = f\"{i}: \" + tokenizer.decode(item, skip_special_tokens=True)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d33a4b-00c8-47bc-bf18-b2048a5b17b4",
   "metadata": {},
   "source": [
    "### Testing the Instruct Model\n",
    "\n",
    "Test on the base showed that unless it is tuned, it behaves just as a generative model with fill the next word functionality. It does not follow instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74d42f5c-7537-46ec-8a7e-a32f6c6db8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "model_name = \"../ext_models/Meta-Llama-3-8B-Instruct\"\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8616d32d-4004-46dd-8dde-5a6ef006d17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caea06014c96420f9443a7711a15dc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True, \n",
    "                                             device_map=DEVICE, \n",
    "                                             torch_dtype=\"auto\")\n",
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b122ef8d-3858-4ca1-9864-a17329691267",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_token = \"<|eot_id|>\"  \n",
    "stop_token_id = tokenizer.encode(stop_token)[0]\n",
    "begin_token = \"<|begin_of_text|>\"\n",
    "begin_token_id = tokenizer.encode(begin_token)[0]\n",
    "generation_config.eos_token_id = stop_token_id\n",
    "generation_config.begin_token_id = begin_token_id\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.5\n",
    "generation_config.top_p = 0.9\n",
    "generation_config.top_k = 3\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "926ae076-e54f-4abb-90f1-af6e09013b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_question_1 = '''<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpfull AI Assistant answering questions\n",
    "to the fullest of your knowledge. Try and supply a detailed answer acknowledging if you do not know enough on a subject.\n",
    "<|eot_id|><|start_header_id|>user <|end_header_id|>Can you suggest code in JS to find the next round number to a multiple of 10, 100, 1000, etc. of a supplied integer? For example\n",
    "if the user supplies 4, the next round number is 10, if 16 is supplied the next round is 20. Supplying 134 would return 200 which is a round multiple of a 100, \n",
    "where as 1340342 results in 2000000 and so on.\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb7d17f6-c715-4c61-8e78-ca1262eafd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([prompt_question_1], return_tensors=\"pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3228f17-51d4-464f-806c-4f26f6a69339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: systemYou are a helpfull AI Assistant answering questions\n",
      "to the fullest of your knowledge. Try and supply a detailed answer acknowledging if you do not know enough on a subject.\n",
      "user Can you suggest code in JS to find the next round number to a multiple of 10, 100, 1000, etc. of a supplied integer? For example\n",
      "if the user supplies 4, the next round number is 10, if 16 is supplied the next round is 20. Supplying 134 would return 200 which is a round multiple of a 100, \n",
      "where as 1340342 results in 2000000 and so on.\n",
      "assistant\n",
      "Here's a simple JavaScript function that can achieve this:\n",
      "\n",
      "```javascript\n",
      "function getNextRoundNumber(n) {\n",
      "    let multiplier = Math.ceil(Math.log10(n)) / Math.LN10;\n",
      "    return Math.ceil(n / (Math.pow(10, multiplier))) * Math.pow(10, multiplier);\n",
      "}\n",
      "\n",
      "// Test cases:\n",
      "console.log(getNextRoundNumber(4)); // Output: 10\n",
      "console.log(getNextRoundNumber(16)); // Output: 20\n",
      "console.log(getNextRoundNumber(134)); // Output: 200\n",
      "console.log(getNextRoundNumber(1340342)); // Output: 2000000\n",
      "```\n",
      "\n",
      "This function works by first calculating the base-10 logarithm of the input number `n`, then dividing it by the natural logarithm (`LN10`) to get the power of 10 that `n` is a multiple of. This gives us the largest power of 10 less than or equal to `n`. Then we use `Math.ceil` to round up to the nearest whole number, because we want the smallest possible multiple of that power of 10.\n",
      "\n",
      "Finally, we divide `n` by that power of 10 using integer division (`/`), and multiply the result by that same power of 10 again. The effect is to \"round\" `n` up to the nearest multiple of that power of 10.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "    for i, item in enumerate(outputs):\n",
    "        text = f\"{i}: \" + tokenizer.decode(item, skip_special_tokens=True)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436b0c93-ff21-45f4-bcb5-7ae9ec3540a5",
   "metadata": {},
   "source": [
    "### Trying the HF Pipeline for Text Generation\n",
    "\n",
    "Lets try the pipeline and LLM Chain to see if the output will be the same as in the Gradio implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc57374-62df-4273-a288-d35269a9d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90f6c76b-d4d2-4340-a156-dbc4c5373b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_pipeline = pipeline(\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        generation_config=generation_config, \n",
    "                        return_full_text=False) \n",
    "llm = HuggingFacePipeline(pipeline=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d8c788-8cef-412b-a87e-3705cd7993b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>You are a helpfull AI Assistant answering questions to the fullest of your knowledge.\n",
    "Try and supply a detailed answer acknowledging if you do not know enough on a subject.\n",
    "<|eot_id|><|start_header_id|>user <|end_header_id|>{input}\n",
    "<|eot_id|><|start_header_id|>assistant <|end_header_id|>\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950edf03-6ab2-4667-af33-f7da29e043c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "512ec12f-054f-44bf-bbfc-6fa0837b3c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\": \"What is the oldest being on Arda\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f325f41a-e4f4-44ec-b2ef-438ba8d432d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A question that delves into the rich lore of J.R.R. Tolkien's Middle-earth!\n",
      "\n",
      "According to Tolkien's legendarium, the oldest being in Arda (the world of Middle-earth) is Eä, also known as the \"Void\" or \"Nothingness\". Eä is considered the ultimate source of all existence, from which the Valar (angelic beings) emerged.\n",
      "\n",
      "However, among the Valar themselves, the oldest being is Melkor (also known as Morgoth), one of the most powerful and evil of the Valar. He was created by Eönwë, the herald of Manwë, during the Ainulindalë, the creation myth of Middle-earth. Melkor's exact age is unknown, but he predates the dawn of time itself.\n",
      "\n",
      "Another contender for the title of oldest being could be the Maiar, who were also created by the Valar during the Ainulindalë. The Maiar include beings like Gandalf, Sauron, and the Balrogs. While they are not as old as Melkor, they still predate the rise of Men and Elves in Middle-earth.\n",
      "\n",
      "It's worth noting that the concept of \"oldest being\" can be somewhat ambiguous in Tolkien's works, as his cosmology is complex and multifaceted. However, based on available information, it appears that Eä, Melkor, and possibly some of the Maiar hold the distinction of being the oldest entities in Arda.\n",
      "\n",
      "If I've missed any crucial details or there's more to explore on this topic, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4917d74b-889a-41ca-9c49-de30e2b53ae1",
   "metadata": {},
   "source": [
    "## Amazon Bedrock\n",
    "\n",
    "Amazon Bedrock provides a bridge - a single API to many different popular models. It also integrated into LangChain for easy use.\n",
    "\n",
    "To use Amazon Bedrock we would utilise the boto3 framework by Amazon. I have exported as env variables which would be present in the AWS credentials file.\n",
    "\n",
    "- aws_access_key_id\n",
    "- aws_secret_access_key\n",
    "- region_name\n",
    "\n",
    "> Note: Its important to use the correct instruction template for the model as specified below with the `open_llm_template` otherwise the model would be blabbering and continuing the conversation on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e56302c0-259a-49ae-b521-4743ed3ce88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4923463f-f2b8-458e-b9e5-70a92cf104f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.34.51)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.51 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.34.51)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.51->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.51->boto3) (1.26.18)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.51->boto3) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38674f5c-24c7-4d5d-85b9-306b3718611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Bedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3154464-c469-4c75-ac20-b12b5c4c9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client('bedrock')\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e8d2d0-28c1-43fd-8d14-cf7561bbe6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'eb24f94a-a576-4606-b82b-2c70d691147f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Sun, 26 May 2024 19:12:25 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '24202',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'eb24f94a-a576-4606-b82b-2c70d691147f'},\n",
       "  'RetryAttempts': 0},\n",
       " 'modelSummaries': [{'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-tg1-large',\n",
       "   'modelId': 'amazon.titan-tg1-large',\n",
       "   'modelName': 'Titan Text Large',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1:0',\n",
       "   'modelId': 'amazon.titan-image-generator-v1:0',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-image-generator-v1',\n",
       "   'modelId': 'amazon.titan-image-generator-v1',\n",
       "   'modelName': 'Titan Image Generator G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-premier-v1:0',\n",
       "   'modelId': 'amazon.titan-text-premier-v1:0',\n",
       "   'modelName': 'Titan Text G1 - Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.olympus-premier-v1:0',\n",
       "   'modelId': 'amazon.olympus-premier-v1:0',\n",
       "   'modelName': 'Olympus 1 Premier',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-g1-text-02',\n",
       "   'modelId': 'amazon.titan-embed-g1-text-02',\n",
       "   'modelName': 'Titan Text Embeddings v2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelId': 'amazon.titan-text-lite-v1:0:4k',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-lite-v1',\n",
       "   'modelId': 'amazon.titan-text-lite-v1',\n",
       "   'modelName': 'Titan Text G1 - Lite',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1:0:8k',\n",
       "   'modelId': 'amazon.titan-text-express-v1:0:8k',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING', 'CONTINUED_PRE_TRAINING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-text-express-v1',\n",
       "   'modelId': 'amazon.titan-text-express-v1',\n",
       "   'modelName': 'Titan Text G1 - Express',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v1:2:8k',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1',\n",
       "   'modelId': 'amazon.titan-embed-text-v1',\n",
       "   'modelName': 'Titan Embeddings G1 - Text',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0:8k',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v2:0',\n",
       "   'modelId': 'amazon.titan-embed-text-v2:0',\n",
       "   'modelName': 'Titan Text Embeddings V2',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1:0',\n",
       "   'modelId': 'amazon.titan-embed-image-v1:0',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-image-v1',\n",
       "   'modelId': 'amazon.titan-embed-image-v1',\n",
       "   'modelName': 'Titan Multimodal Embeddings G1',\n",
       "   'providerName': 'Amazon',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1:0',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1:0',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/stability.stable-diffusion-xl-v1',\n",
       "   'modelId': 'stability.stable-diffusion-xl-v1',\n",
       "   'modelName': 'SDXL 1.0',\n",
       "   'providerName': 'Stability AI',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['IMAGE'],\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-grande-instruct',\n",
       "   'modelId': 'ai21.j2-grande-instruct',\n",
       "   'modelName': 'J2 Grande Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-jumbo-instruct',\n",
       "   'modelId': 'ai21.j2-jumbo-instruct',\n",
       "   'modelName': 'J2 Jumbo Instruct',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid',\n",
       "   'modelId': 'ai21.j2-mid',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-mid-v1',\n",
       "   'modelId': 'ai21.j2-mid-v1',\n",
       "   'modelName': 'Jurassic-2 Mid',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra',\n",
       "   'modelId': 'ai21.j2-ultra',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1:0:8k',\n",
       "   'modelId': 'ai21.j2-ultra-v1:0:8k',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/ai21.j2-ultra-v1',\n",
       "   'modelId': 'ai21.j2-ultra-v1',\n",
       "   'modelName': 'Jurassic-2 Ultra',\n",
       "   'providerName': 'AI21 Labs',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1:2:100k',\n",
       "   'modelId': 'anthropic.claude-instant-v1:2:100k',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-instant-v1',\n",
       "   'modelId': 'anthropic.claude-instant-v1',\n",
       "   'modelName': 'Claude Instant',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:18k',\n",
       "   'modelId': 'anthropic.claude-v2:0:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:0:100k',\n",
       "   'modelId': 'anthropic.claude-v2:0:100k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:18k',\n",
       "   'modelId': 'anthropic.claude-v2:1:18k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1:200k',\n",
       "   'modelId': 'anthropic.claude-v2:1:200k',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2:1',\n",
       "   'modelId': 'anthropic.claude-v2:1',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-v2',\n",
       "   'modelId': 'anthropic.claude-v2',\n",
       "   'modelName': 'Claude',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       "   'modelName': 'Claude 3 Sonnet',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelId': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       "   'modelName': 'Claude 3 Haiku',\n",
       "   'providerName': 'Anthropic',\n",
       "   'inputModalities': ['TEXT', 'IMAGE'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-text-v14:7:4k',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-text-v14',\n",
       "   'modelId': 'cohere.command-text-v14',\n",
       "   'modelName': 'Command',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-v1:0',\n",
       "   'modelId': 'cohere.command-r-v1:0',\n",
       "   'modelName': 'Command R',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-r-plus-v1:0',\n",
       "   'modelId': 'cohere.command-r-plus-v1:0',\n",
       "   'modelName': 'Command R+',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14:7:4k',\n",
       "   'modelId': 'cohere.command-light-text-v14:7:4k',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.command-light-text-v14',\n",
       "   'modelId': 'cohere.command-light-text-v14',\n",
       "   'modelName': 'Command Light',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3:0:512',\n",
       "   'modelId': 'cohere.embed-english-v3:0:512',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-english-v3',\n",
       "   'modelId': 'cohere.embed-english-v3',\n",
       "   'modelName': 'Embed English',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3:0:512',\n",
       "   'modelId': 'cohere.embed-multilingual-v3:0:512',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/cohere.embed-multilingual-v3',\n",
       "   'modelId': 'cohere.embed-multilingual-v3',\n",
       "   'modelName': 'Embed Multilingual',\n",
       "   'providerName': 'Cohere',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['EMBEDDING'],\n",
       "   'responseStreamingSupported': False,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['PROVISIONED'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-chat-v1',\n",
       "   'modelId': 'meta.llama2-13b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1:0:4k',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-chat-v1',\n",
       "   'modelId': 'meta.llama2-70b-chat-v1',\n",
       "   'modelName': 'Llama 2 Chat 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-13b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-13b-v1',\n",
       "   'modelId': 'meta.llama2-13b-v1',\n",
       "   'modelName': 'Llama 2 13B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1:0:4k',\n",
       "   'modelId': 'meta.llama2-70b-v1:0:4k',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': ['FINE_TUNING'],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama2-70b-v1',\n",
       "   'modelId': 'meta.llama2-70b-v1',\n",
       "   'modelName': 'Llama 2 70B',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': [],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-8b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-8b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 8B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/meta.llama3-70b-instruct-v1:0',\n",
       "   'modelId': 'meta.llama3-70b-instruct-v1:0',\n",
       "   'modelName': 'Llama 3 70B Instruct',\n",
       "   'providerName': 'Meta',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelId': 'mistral.mistral-7b-instruct-v0:2',\n",
       "   'modelName': 'Mistral 7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelId': 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       "   'modelName': 'Mixtral 8x7B Instruct',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-large-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-large-2402-v1:0',\n",
       "   'modelName': 'Mistral Large',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}},\n",
       "  {'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/mistral.mistral-small-2402-v1:0',\n",
       "   'modelId': 'mistral.mistral-small-2402-v1:0',\n",
       "   'modelName': 'Mistral Small',\n",
       "   'providerName': 'Mistral AI',\n",
       "   'inputModalities': ['TEXT'],\n",
       "   'outputModalities': ['TEXT'],\n",
       "   'responseStreamingSupported': True,\n",
       "   'customizationsSupported': [],\n",
       "   'inferenceTypesSupported': ['ON_DEMAND'],\n",
       "   'modelLifecycle': {'status': 'ACTIVE'}}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock_client.list_foundation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b38222f1-74e7-48b2-a2b2-8beafa3884d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Bedrock(client=bedrock_runtime, model_id=\"mistral.mixtral-8x7b-instruct-v0:1\", \n",
    "              model_kwargs={\"temperature\": 0.15, \"top_p\": 0.85})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cffa78be-e1c3-4f95-999c-9b30f17a5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_template = \"\"\"<s>[INST]You are a helpful assistant ready to assist your human friend.\\\n",
    "Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
    "If you do not know the answer please acknowledge the fact by saying that you do not possess the required information.\\\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Human:\n",
    "{input}[/INST]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(open_llm_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4dc3cab4-9e7d-4093-a891-cda6cfd69c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the required information.\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Human:\n",
      "Hello! Can you please tell me what is the most ancient being on Arda?[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, verbose=True, memory=ConversationBufferMemory(), prompt=prompt\n",
    ")\n",
    "\n",
    "response = conversation.predict(input=\"Hello! Can you please tell me what is the most ancient being on Arda?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce874056-9ff1-4fe2-afa1-6c4b3648632f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'd be happy to help, but I need a bit more context to provide an accurate answer. Arda is a fictional world created by J.R.R. Tolkien, the author of \"The Lord of the Rings\" and other works. It's home to many ancient beings, including the Valar (divine beings), the Maiar (lesser divine beings), and various types of long-lived creatures like Elves and Dwarves.\n",
      "\n",
      "However, if you're asking about the oldest being in terms of time, that would likely be Eru Ilúvatar, also known as the One, who is the creator of Arda and all its inhabitants in Tolkien's legendarium. The Valar, who are his direct sub-creations, would be the next oldest, with Melkor (also known as Morgoth) being the first-born of the Ainur.\n",
      "\n",
      "Please let me know if you were asking about a different Arda or if you need information on something else!\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2be51de8-dacf-43f0-92d3-326c02238f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the required information.\n",
      "Current conversation:\n",
      "Human: Hello! Can you please tell me what is the most ancient being on Arda?\n",
      "AI: Hello! I'd be happy to help, but I need a bit more context to provide an accurate answer. Arda is a fictional world created by J.R.R. Tolkien, the author of \"The Lord of the Rings\" and other works. It's home to many ancient beings, including the Valar (divine beings), the Maiar (lesser divine beings), and various types of long-lived creatures like Elves and Dwarves.\n",
      "\n",
      "However, if you're asking about the oldest being in terms of time, that would likely be Eru Ilúvatar, also known as the One, who is the creator of Arda and all its inhabitants in Tolkien's legendarium. The Valar, who are his direct sub-creations, would be the next oldest, with Melkor (also known as Morgoth) being the first-born of the Ainur.\n",
      "\n",
      "Please let me know if you were asking about a different Arda or if you need information on something else!\n",
      "\n",
      "Human:\n",
      "I am asking about Tolkien's Arda. I am intereseted in the first character or being to actually inhabit Arda, not the one who created it.[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello again! In that case, the first beings to actually inhabit Arda would be the Ainur, who are spirits that were present at the creation of the world by Eru Ilúvatar. Among these Ainur, the Valar and the Maiar descended into Arda to shape and govern it.\\n\\nThe first of the Valar to enter Arda was Tulkas, who came to aid the other Valar in the fight against Melkor. However, it\\'s important to note that Arda was empty and dark at this point, and the Valar began the process of ordering and shaping it.\\n\\nThe first living creatures to inhabit Arda, after the Valar and Maiar, were the beings known as the Eldar (High Elves) and the Avari (Dark Elves). The Eldar were the first to awaken at Cuiviénen, and they were led by Oromë, one of the Valar. The Avari, on the other hand, did not follow Oromë to Valinor, and thus are often considered the \"Dark Elves\" or \"Unwilling Elves\".\\n\\nSo, to summarize, the first inhabitants of Arda in terms of living beings would be the Ainur (Valar and Maiar), followed by the Eldar and Avari (types of Elves).'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I am asking about Tolkien's Arda. I am intereseted in the first character or being to actually inhabit Arda, not the one who created it.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
