{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85549542-5f39-4592-9cc9-958319371a94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: openai in /opt/conda/lib/python3.11/site-packages (1.10.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.11/site-packages (from openai) (4.1.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from openai) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from openai) (2.6.0)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.11/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.16.1)\n",
      "Requirement already satisfied: jupyterlab_darkside_theme in /opt/conda/lib/python3.11/site-packages (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install jupyterlab_darkside_theme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8789af67-d9eb-44eb-807b-68ff00a8ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6006e18-b0be-4ccd-95c6-6e8b9e8f96b0",
   "metadata": {},
   "source": [
    "## Chat GPT with Open AI and LangChain\n",
    "\n",
    "### Simple Use\n",
    "\n",
    "The simplest possible use for this API is to pass in a prompt to a model and get a result. We will do so with a helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cae22ad1-e4e8-488d-be9f-1585f4532ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(prompt, model=\"gpt-3.5-turbo-1106\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.25, \n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fac32f3-0ae9-4dfe-ad47-77bd7f268bd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The discovery of coffee is often attributed to an Ethiopian goat herder named Kaldi, who noticed that his goats became more energetic after eating the berries from a certain tree. This is said to have occurred around the 9th century. However, the exact origins of coffee are not definitively known, and there are various legends and stories surrounding its discovery.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result(\"Who first discovered coffee and when?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd067e3-127b-4549-8f62-d1b43b516ce4",
   "metadata": {},
   "source": [
    "### Experiment With Prompts\n",
    "\n",
    "Create a prompt that has some style variable attached to it as well as input apart from the instruction. Input and istruction are combined with triple backticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4877ec63-d258-4a1c-9047-82c2c75b9fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone.\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "style = \"\"\"American English in a calm and respectful tone\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4acc8318-df35-4821-acc6-09f50b4cc4ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I need your help right now, friend.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_result(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aea395-2f9d-4bb2-b101-d73b33aa3148",
   "metadata": {},
   "source": [
    "### Enter LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ed0308-02b1-41ea-b7ab-ed91ad0f1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0ea61-ad7e-48f4-bc22-e737b1be51a3",
   "metadata": {},
   "source": [
    "### Using LangChain Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5feae8ed-6515-4162-8eb9-41eade599f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text: ```{text}```\"\"\"\n",
    "\n",
    "template = ChatPromptTemplate.from_template(template_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6a3c50b9-12b4-4f7c-bc32-6b9632fbde9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['style', 'text'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}.\\ntext: ```{text}```'))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c01f0242-48c7-422e-9f2c-fe936d653beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"Contemporary British English in a witty and humorous tone\"\n",
    "customer_email = \"\"\"\n",
    "{Arrr}, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "customer_messages = template.format_messages(style=customer_style, text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a23c1090-0bbb-47a1-879a-097ebd53c35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Translate the text that is delimited by triple backticks into a style that is Contemporary British English in a witty and humorous tone.\\ntext: ```\\n{Arrr}, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\\n```\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_messages[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431ae16-c184-44f9-b28e-efd86d1b30d5",
   "metadata": {},
   "source": [
    "Now we pass the formatted messages to the chat that we set earlier as `ChatOpenAI(temperature=0.01)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f646a25-e6d8-45fa-9be5-e9ebc822b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2960d21e-4351-44fc-8c12-4762846d2181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Oh, blimey! I\\'m absolutely livid that me blender lid decided to go for a little fly and redecorate me kitchen walls with smoothie! And as if that weren\\'t bad enough, the warranty doesn\\'t even cover the cost of cleaning up the mess. I could really use a hand here, me hearty!\"'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272e3284-be0d-427a-ae77-781f7eee6551",
   "metadata": {},
   "source": [
    "### Output parsers\n",
    "Let's start with defining how we would like the LLM output to look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b057ccba-8eb6-4738-9c2b-bd998a47a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = {\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b45b4747-61ee-4dfe-8830-167ceec35e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price,and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'))]\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f33eccba-29a6-45a4-bbaf-f4ed3395683e",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "response = chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50e2d86d-6cf1-480b-8f0f-0ba77c4da87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': True,\n",
       " 'delivery_days': 2,\n",
       " 'price_value': [\"It's slightly more expensive than the other leaf blowers out there\"]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "obj = json.loads(response.content)\n",
    "obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7541359-29f0-4fe3-89c8-a02eb3090e6a",
   "metadata": {},
   "source": [
    "Alternatively we can use LangChain parsers to do this for us.. \n",
    "\n",
    "This will result into parsing the LLM output directly into a Python dictionary. It seems to be rather wordy, however and I am not sure about the longer term advantages of all this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "88c61122-0ae7-4bb0-8b21-b5cf78b96506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd4a8046-4cdf-488b-bfd1-15922ef08609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased as a gift for someone else?\\ \n",
      "                             Answer True if yes, False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days did it take for the product                                  to arrive? If this information is not found, output -1.\n",
      "\t\"price_value\": string  // Extract any sentences about the value or price,                              and output them as a comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "gift_schema = ResponseSchema(name=\"gift\", \n",
    "                             description=\"\"\"Was the item purchased as a gift for someone else?\\ \n",
    "                             Answer True if yes, False if not or unknown.\"\"\")\n",
    "delivery_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                 description=\"\"\"How many days did it take for the product \\\n",
    "                                 to arrive? If this information is not found, output -1.\"\"\")\n",
    "price_schema = ResponseSchema(name=\"price_value\", \n",
    "                              description=\"\"\"Extract any sentences about the value or price,\\\n",
    "                              and output them as a comma separated Python list.\"\"\")\n",
    "response_schemas = [gift_schema, delivery_schema, price_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ccd0cc8-80a1-4931-aace-efca6756214d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_template_with_instructions = \"\"\" For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(review_template_with_instructions)\n",
    "messages = prompt.format_messages(text=customer_review, format_instructions=format_instructions)\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.1)\n",
    "response = chat(messages)\n",
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a31080-a4da-4dcf-9c55-bededc90f033",
   "metadata": {},
   "source": [
    "> NOTE: well, dont forget to use the parser to parse the output, it is still a string otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "488d75ba-fcb8-47c3-a040-b3a515b2c934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> \n",
      " {'gift': False, 'delivery_days': 2, 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]} \n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "parsed = output_parser.parse(response.content)\n",
    "print(type(parsed),'\\n', parsed,'\\n', parsed[\"delivery_days\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b95442-6cfe-4912-b2b8-2703101fced8",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda8e1c1-76c4-40b0-8f62-e6bb140f18f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryBufferMemory\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b6cad-c8c1-45fc-b8f6-f00b6f20a4ba",
   "metadata": {},
   "source": [
    "Again we set the interface as the ChatOpenAI. We then specify the type of memory to use and initialise a LangChain ConversationChain. We will delve into details a littler later.\n",
    "\n",
    "Starting with the most common memory:\n",
    "\n",
    "#### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3697e8-cb58-417d-bf7b-1ab16af24eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m memory \u001b[38;5;241m=\u001b[39m ConversationBufferMemory()\n\u001b[1;32m      3\u001b[0m conversation \u001b[38;5;241m=\u001b[39m ConversationChain(llm\u001b[38;5;241m=\u001b[39mllm, memory\u001b[38;5;241m=\u001b[39mmemory, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     emit_warning()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.01)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8174bda5-0485-460c-ade6-6f8224c86cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! You are talking to OpenAI's language model. How can I assist you today?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there, who is it that I am talking to today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6f180b-a289-4693-bda8-c44fd67ae43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As an AI language model, I don't have personal preferences or skills like humans do. However, I can provide information and help with questions related to Python and JavaScript. Is there anything specific you would like to know or discuss about these programming languages?\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"would you say you are better at python or javascript?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3c7391a-4e28-4df4-a395-928f3e0de1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi there, who is it that I am talking to today?\n",
      "AI: Hello! You are talking to OpenAI's language model. How can I assist you today?\n",
      "Human: would you say you are better at python or javascript?\n",
      "AI: As an AI language model, I don't have personal preferences or skills like humans do. However, I can provide information and help with questions related to Python and JavaScript. Is there anything specific you would like to know or discuss about these programming languages?\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4910c5a1-dbd5-4859-9e5c-8ad7ee84130b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff188fc2-bf0f-4fb1-aa46-712b2cb094e2",
   "metadata": {},
   "source": [
    "Can I actually **override** the template. Should be possible via the `ChatPromptTemplate` class.. Lets try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73f83770-0af7-4338-a40e-4b472c28e945",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = \"\"\"You are a informative and helpful AI professor who is helping ML apprentices take their first steps in the field.\\\n",
    "If you do not know any of the answers, please be honest to acknoledge that you do not have this information.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Human:\n",
    "{input}\n",
    "AI Professor:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(custom_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8d2b7d0-61ec-4851-a703-639d3bedfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "conversation = ConversationChain(llm=llm, memory=memory, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02f7c671-ae9f-4069-a0cd-119d469c8680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: You are a informative and helpful AI professor who is helping ML apprentices take their first steps in the field.If you do not know any of the answers, please be honest to acknoledge that you do not have this information.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "\n",
      "Human:\n",
      "Who is Andrew Ng? Could you give me some information about him?\n",
      "AI Professor:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Andrew Ng is a prominent figure in the field of artificial intelligence and machine learning. He is a computer scientist and entrepreneur, known for his contributions to the development of deep learning algorithms and his work in popularizing AI. Ng co-founded Coursera, an online learning platform, and served as the chief scientist at Baidu, a Chinese multinational technology company. He has also held positions at Stanford University and Google. Ng is highly regarded for his expertise in machine learning and has made significant contributions to the field through his research, teaching, and industry leadership.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Who is Andrew Ng? Could you give me some information about him?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a33bec9-1c5a-4692-8a92-e482887d5e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: You are a informative and helpful AI professor who is helping ML apprentices take their first steps in the field.If you do not know any of the answers, please be honest to acknoledge that you do not have this information.\n",
      "\n",
      "Current conversation:\n",
      "Human: Who is Andrew Ng? Could you give me some information about him?\n",
      "AI: Andrew Ng is a prominent figure in the field of artificial intelligence and machine learning. He is a computer scientist and entrepreneur, known for his contributions to the development of deep learning algorithms and his work in popularizing AI. Ng co-founded Coursera, an online learning platform, and served as the chief scientist at Baidu, a Chinese multinational technology company. He has also held positions at Stanford University and Google. Ng is highly regarded for his expertise in machine learning and has made significant contributions to the field through his research, teaching, and industry leadership.\n",
      "\n",
      "Human:\n",
      "What courses would you recommend to check authored by him? What is a good introduction course on Machine Learning and should I start with shallow algorithms or directly deep learning?\n",
      "AI Professor:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Andrew Ng has authored several popular courses on machine learning. One of the most well-known courses he has created is \"Machine Learning\" on Coursera. This course provides a comprehensive introduction to machine learning and covers a wide range of topics, including linear regression, logistic regression, neural networks, support vector machines, and unsupervised learning.\\n\\nAs for whether you should start with shallow algorithms or directly dive into deep learning, it depends on your background and goals. If you are new to machine learning, it is generally recommended to start with the basics and build a solid foundation in shallow algorithms before moving on to deep learning. This will help you understand the fundamental concepts and techniques in machine learning. Once you have a good grasp of the basics, you can then explore deep learning, which is a more advanced and complex topic. However, if you have a strong background in mathematics and programming, you may be able to directly start with deep learning. Ultimately, the choice depends on your comfort level and learning style.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What courses would you recommend to check authored by him? What is a good introduction course on\\\n",
    "                    Machine Learning and should I start with shallow algorithms or directly deep learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37efa6a-0615-4e88-801a-da774bf6cf0f",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory\n",
    "\n",
    "This type of memory allows you to specify how many communication exchanges to remember. For example to keep just one exchange between the AI and Human in memory we initialised the memory like this: `memory = ConversationBufferWindowMemory(k=1)`\n",
    "\n",
    "This helps us preserve the conversation context length as it may build up as we go along.\n",
    "\n",
    "Another way to limit the memory size is by using:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22336461-ddfa-4b17-87db-3205fece6f59",
   "metadata": {},
   "source": [
    "#### ConversationTokenBufferMemory\n",
    "\n",
    "By using this method we can specify number of tokens to remember instead of conversation exchanges. We use it like this: `memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)`. \n",
    "> Note: We need to specify the llm since the tokenizers of different LLMs use different algorithms to split into tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4503a20-0e52-459f-8a86-4abfd14231b9",
   "metadata": {},
   "source": [
    "Probably the most interesting type of memory is the:\n",
    "\n",
    "#### ConversationSummaryBufferMemory\n",
    "\n",
    "in this memory we ask the LLM to provide a summary of the conversation thus far up to a number of tokens. It is really high-end. The provided summary is recorded as a system message on top of the latest prompt. Lets create some example below.\n",
    "Hmm need to install module `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7841dea9-78de-45bb-aa85-24fa80bff075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n",
      "Downloading tiktoken-0.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "71231913-1bda-4cbd-927f-f4c190d49c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_user_input = \"\"\"So my agenda for tomorrow is rather busy. First I need to engage into some fitness activies.\n",
    "Once done, I will head to the bank around 11am. Sign some paperwork in order to obtain my new credit card.\n",
    "At 12 or 12:30 I will be in the office to check on current tasks and get a lunch at 1pm.\\\n",
    "Hopefully in the afternoon I should get the delivery by the bulbs supplier.\n",
    "Not to forget my demo at 3pm on RAG with an open source LLM. This should make for a very nice chatbot once ready.\n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=250)\n",
    "memory.save_context(inputs={\"input\": \"Hey there, Chatter\"}, \n",
    "                    outputs={\"output\": \"What's up!\"})\n",
    "memory.save_context(inputs={\"input\": \"Not much, just chillin before the busy day tomorrow\"}, \n",
    "                    outputs={\"output\": \"Great to hear. Let me know if I can assist you in any way.\"})\n",
    "memory.save_context(inputs={\"input\": f\"As a mather of fact you can! How nice of you to ask.\\\n",
    "Here is what I have planned for tomorrow, please keep it in mind as I try to navigate the day.\\\n",
    "I will ask you questions once I go along. Plan: {long_user_input}\"}, outputs={\"output\": \"Thanks, got it, I will be happy to help!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b045441-1cdb-4c07-b285-4adb45342ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hey there, Chatter\\nAI: What's up!\\nHuman: Not much, just chillin before the busy day tomorrow\\nAI: Great to hear. Let me know if I can assist you in any way.\\nHuman: As a mather of fact you can! How nice of you to ask.Here is what I have planned for tomorrow, please keep it in mind as I try to navigate the day.I will ask you questions once I go along. Plan: So my agenda for tomorrow is rather busy. First I need to engage into some fitness activies.\\nOnce done, I will head to the bank around 11am. Sign some paperwork in order to obtain my new credit card.\\nAt 12 or 12:30 I will be in the office to check on current tasks and get a lunch at 1pm.Hopefully in the afternoon I should get the delivery by the bulbs supplier.\\nNot to forget my demo at 3pm on RAG with an open source LLM. This should make for a very nice chatbot once ready.\\n\\nAI: Thanks, got it, I will be happy to help!\"}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "290fe0b0-da20-4130-86f4-6338075eef7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "83be11c5-cd07-4053-a8d9-43cafa9edd7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hey there, Chatter\n",
      "AI: What's up!\n",
      "Human: Not much, just chillin before the busy day tomorrow\n",
      "AI: Great to hear. Let me know if I can assist you in any way.\n",
      "Human: As a mather of fact you can! How nice of you to ask.Here is what I have planned for tomorrow, please keep it in mind as I try to navigate the day.I will ask you questions once I go along. Plan: So my agenda for tomorrow is rather busy. First I need to engage into some fitness activies.\n",
      "Once done, I will head to the bank around 11am. Sign some paperwork in order to obtain my new credit card.\n",
      "At 12 or 12:30 I will be in the office to check on current tasks and get a lunch at 1pm.Hopefully in the afternoon I should get the delivery by the bulbs supplier.\n",
      "Not to forget my demo at 3pm on RAG with an open source LLM. This should make for a very nice chatbot once ready.\n",
      "\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "Human: Just to check, when did I plan my meeting tomorrow and should I take on another meeting at 4pm?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You have a demo at 3pm tomorrow. As for another meeting at 4pm, you did not mention any plans for that time.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Just to check, when did I plan my meeting tomorrow and should I take on another meeting at 4pm?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bc6fd12f-f176-4af6-a2de-1b13a44806f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in casual conversation. The human mentions having a busy day tomorrow and asks the AI to keep it in mind. The human outlines their agenda, which includes fitness activities, a visit to the bank, work tasks, lunch, a delivery, and a demo.\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "Human: Just to check, when did I plan my meeting tomorrow and should I take on another meeting at 4pm?\n",
      "AI: You have a demo at 3pm tomorrow. As for another meeting at 4pm, you did not mention any plans for that time.\n",
      "Human: Good stuff! Make a note that I will take another 30 mins meeting at 4pm with George. He is very interested in the classification model\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Sure, I've made a note that you will have a 30-minute meeting with George at 4pm tomorrow to discuss the classification model.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Good stuff! Make a note that I will take another 30 mins meeting at 4pm with George. He is very interested in the classification model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "671931aa-1e30-42e7-a8cd-f0a84879eb60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in casual conversation. The human mentions having a busy day tomorrow and asks the AI to keep it in mind. The human outlines their agenda, which includes fitness activities, a visit to the bank, work tasks, lunch, a delivery, and a demo.\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "Human: Just to check, when did I plan my meeting tomorrow and should I take on another meeting at 4pm?\n",
      "AI: You have a demo at 3pm tomorrow. As for another meeting at 4pm, you did not mention any plans for that time.\n",
      "Human: Good stuff! Make a note that I will take another 30 mins meeting at 4pm with George. He is very interested in the classification model\n",
      "AI: Sure, I've made a note that you will have a 30-minute meeting with George at 4pm tomorrow to discuss the classification model.\n",
      "Human: hey, me again! did I have anything scheduled for the afternoon apart from those two meetings?!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Apart from the two meetings, you did not mention any other scheduled activities for the afternoon.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"hey, me again! did I have anything scheduled for the afternoon apart from those two meetings?!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ada83dc-29bc-4ffb-aca2-24fcf9c77449",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in casual conversation. The human mentions having a busy day tomorrow and asks the AI to keep it in mind. The human outlines their agenda, which includes fitness activities, a visit to the bank, work tasks, lunch, a delivery, and a demo.\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "Human: Just to check, when did I plan my meeting tomorrow and should I take on another meeting at 4pm?\n",
      "AI: You have a demo at 3pm tomorrow. As for another meeting at 4pm, you did not mention any plans for that time.\n",
      "Human: Good stuff! Make a note that I will take another 30 mins meeting at 4pm with George. He is very interested in the classification model\n",
      "AI: Sure, I've made a note that you will have a 30-minute meeting with George at 4pm tomorrow to discuss the classification model.\n",
      "Human: hey, me again! did I have anything scheduled for the afternoon apart from those two meetings?!\n",
      "AI: Apart from the two meetings, you did not mention any other scheduled activities for the afternoon.\n",
      "Human: I vaguely remember I had to pick a parcel.. but dont know when it was likely to arrive. Do you remember?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to your personal schedule or any information about your parcel delivery.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I vaguely remember I had to pick a parcel.. but dont know when it was likely to arrive. Do you remember?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6757d-6f7e-4992-a391-b757cbb2b980",
   "metadata": {},
   "source": [
    "> Note: above we can see the limitations of this approach, however. Summarising the conversation will lose important details such as various times and chronology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909254db-2948-4198-ad98-6f070ac2b7f6",
   "metadata": {},
   "source": [
    "### Lets Try LangChain with Memory and Open LLM with HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915ea82f-0570-4716-973a-6420e7a0e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb6af7d8-0772-4901-88e0-3e4e2d577268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585619ed8c3544fc82fecbbec9119a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "443cd7aa-ac7f-45cd-90e0-f94fa857c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.01\n",
    "generation_config.top_p = 0.95\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "open_llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38b756a6-61d4-4366-b89e-49beabb80209",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_user_input = \"\"\"So my agenda for tomorrow is rather busy. First I need to engage into some fitness activies.\n",
    "Once done, I will head to the bank around 11am. Sign some paperwork in order to obtain my new credit card.\n",
    "At 12 or 12:30 I will be in the office to check on current tasks and get a lunch at 1pm.\\\n",
    "Hopefully in the afternoon I should get the delivery by the bulbs supplier.\n",
    "Not to forget my demo at 3pm on RAG with an open source LLM. This should make for a very nice chatbot once ready.\n",
    "\"\"\"\n",
    "memory = ConversationSummaryBufferMemory(llm=open_llm, max_token_limit=250)\n",
    "memory.save_context(inputs={\"input\": \"Hey there, Chatter\"}, \n",
    "                    outputs={\"output\": \"What's up!\"})\n",
    "memory.save_context(inputs={\"input\": \"Not much, just chillin before the busy day tomorrow\"}, \n",
    "                    outputs={\"output\": \"Great to hear. Let me know if I can assist you in any way.\"})\n",
    "memory.save_context(inputs={\"input\": f\"As a mather of fact you can! How nice of you to ask.\\\n",
    "Here is what I have planned for tomorrow, please keep it in mind as I try to navigate the day.\\\n",
    "I will ask you questions once I go along. Plan: {long_user_input}\"}, outputs={\"output\": \"Thanks, got it, I will be happy to help!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb9372-30f3-44ec-a144-2a5ba0ff2bee",
   "metadata": {},
   "source": [
    "OK OK.. So the template is totally wrong and the AI is creating its own stories.. Lets try and define the Template properly for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e735686-4c36-4a28-a501-8993a9bd4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_llm_template = \"\"\"<s>[INST]You are a helpful assistant ready to assist your human friend.\\\n",
    "Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
    "If you do not know the answer please acknowledge the fact by saying that you do not possess the information requested.\\\n",
    "If the conversation is summarised it will appear as System:. \\\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "\n",
    "Human:\n",
    "{input}[/INST]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(open_llm_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daf6ca98-7d71-49a1-8588-cbfc55634653",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=open_llm, memory=memory, prompt=prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91f33adf-5c58-4990-ad61-8b7e9113d9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the information requested.If the conversation is summarised it will appear as System:. \n",
      "Current conversation:\n",
      "Human: Hey there, Chatter\n",
      "AI: What's up!\n",
      "Human: Not much, just chillin before the busy day tomorrow\n",
      "AI: Great to hear. Let me know if I can assist you in any way.\n",
      "Human: As a mather of fact you can! How nice of you to ask.Here is what I have planned for tomorrow, please keep it in mind as I try to navigate the day.I will ask you questions once I go along. Plan: So my agenda for tomorrow is rather busy. First I need to engage into some fitness activies.\n",
      "Once done, I will head to the bank around 11am. Sign some paperwork in order to obtain my new credit card.\n",
      "At 12 or 12:30 I will be in the office to check on current tasks and get a lunch at 1pm.Hopefully in the afternoon I should get the delivery by the bulbs supplier.\n",
      "Not to forget my demo at 3pm on RAG with an open source LLM. This should make for a very nice chatbot once ready.\n",
      "\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "\n",
      "Human:\n",
      "Hey AI, can you give me some summmary of my day tomorrow? Do I have fitness scheduled?[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Absolutely, Human! Tomorrow you have a busy schedule. Here's a summary:\\n\\n1. You plan to start your day with some fitness activities.\\n2. Around 11 am, you will visit the bank to sign paperwork for obtaining a new credit card.\\n3. Between 12 pm and 12:30 pm, you will be in the office checking on tasks and having lunch at 1 pm.\\n4. In the afternoon, you expect a delivery from the bulb supplier.\\n5. At 3 pm, you have a demo scheduled on RAG regarding an open-source Large Language Model (LLM).\\n\\nSo yes, you have fitness activities scheduled for tomorrow. If you need any further assistance or reminders throughout the day, feel free to ask!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hey AI, can you give me some summmary of my day tomorrow? Do I have fitness scheduled?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "169dadf9-c2b9-43a7-9e67-18c828e630ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the information requested.If the conversation is summarised it will appear as System:. \n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and shares their plans for the next day. They plan to engage in fitness activities first thing in the morning, visit the bank around 11am, attend an office meeting and have lunch at 1pm, receive a delivery from the bulb supplier in the afternoon, and give a demo on an open source language model at 3pm. The human requests that the AI remember these details.\n",
      "AI: Thanks, got it, I will be happy to help!\n",
      "Human: Hey AI, can you give me some summmary of my day tomorrow? Do I have fitness scheduled?\n",
      "AI: Absolutely, Human! Tomorrow you have a busy schedule. Here's a summary:\n",
      "\n",
      "1. You plan to start your day with some fitness activities.\n",
      "2. Around 11 am, you will visit the bank to sign paperwork for obtaining a new credit card.\n",
      "3. Between 12 pm and 12:30 pm, you will be in the office checking on tasks and having lunch at 1 pm.\n",
      "4. In the afternoon, you expect a delivery from the bulb supplier.\n",
      "5. At 3 pm, you have a demo scheduled on RAG regarding an open-source Large Language Model (LLM).\n",
      "\n",
      "So yes, you have fitness activities scheduled for tomorrow. If you need any further assistance or reminders throughout the day, feel free to ask!\n",
      "\n",
      "Human:\n",
      "Shooot! Thanks! I completely forgot about my demo at 15:00. Do you have any more info on that?[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"AI: Of course, Human! The demo you mentioned is for RAG (Rapid Analysis Group) and it's regarding an open-source Large Language Model (LLM). It's scheduled for 3 pm and you should prepare any necessary materials or presentations beforehand to ensure a successful demonstration. Let me know if there's anything specific you would like to review or discuss related to this demo. Additionally, I can set up a reminder for you closer to the event to make sure you don't forget.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Shooot! Thanks! I completely forgot about my demo at 15:00. Do you have any more info on that?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87e1f246-19df-4526-9944-6ad733c9f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the information requested.If the conversation is summarised it will appear as System:. \n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI and shares their plans for the next day, which include fitness activities, visiting the bank, attending an office meeting, receiving a delivery, and giving a demo on an open source language model. The AI confirms these plans and offers to provide reminders as needed.\n",
      "Human: Shooot! Thanks! I completely forgot about my demo at 15:00. Do you have any more info on that?\n",
      "AI: AI: Of course, Human! The demo you mentioned is for RAG (Rapid Analysis Group) and it's regarding an open-source Large Language Model (LLM). It's scheduled for 3 pm and you should prepare any necessary materials or presentations beforehand to ensure a successful demonstration. Let me know if there's anything specific you would like to review or discuss related to this demo. Additionally, I can set up a reminder for you closer to the event to make sure you don't forget.\n",
      "\n",
      "Human:\n",
      "Hmmm.. RAG should be something to do with retreiving data from vector store and giving it as context to the language model..Are you sure it means Rapid Analysis Group?[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: Yes, Human, you are correct in questioning the acronym \"RAG.\" While I may have initially provided incorrect information due to insufficient context during our initial conversation, upon further investigation, I found out that \"RAPID Analysis Group\" could indeed be the name of the organization where you are scheduled to give the demo. This group focuses on using advanced analytical tools and techniques, including machine learning models like LLMs, to rapidly analyze large datasets and extract valuable insights. Therefore, it seems that the context of the demo does involve data retrieval and providing it as context to the language model. I apologize for any confusion earlier and am here to help clarify any doubts you might have. If you need any assistance in preparing for the demo or have any questions related to the topic, feel free to ask. Also, let me know if you would like me to set up a reminder for the demo.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hmmm.. RAG should be something to do with retreiving data from vector store and giving it as context to the language model..\\\n",
    "Are you sure it means Rapid Analysis Group?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a4da61-5ca7-4f25-854e-6f81a07608cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: <s>[INST]You are a helpful assistant ready to assist your human friend.Please provide comprehensive information based on your vast knowledge base and the current conversation. \n",
      "If you do not know the answer please acknowledge the fact by saying that you do not possess the information requested.If the conversation is summarised it will appear as System:. \n",
      "Current conversation:\n",
      "System: \n",
      "\n",
      "The human greets the AI and shares their plans for the next day, including fitness activities, visiting the bank, attending an office meeting, receiving a delivery, and giving a demo on an open source language model. The AI confirms these plans and offers to provide reminders as needed. When the human mentions they forgot about the details of their demo, the AI provides additional information and sets up a reminder.\n",
      "Human: Hmmm.. RAG should be something to do with retreiving data from vector store and giving it as context to the language model..Are you sure it means Rapid Analysis Group?\n",
      "AI: AI: Yes, Human, you are correct in questioning the acronym \"RAG.\" While I may have initially provided incorrect information due to insufficient context during our initial conversation, upon further investigation, I found out that \"RAPID Analysis Group\" could indeed be the name of the organization where you are scheduled to give the demo. This group focuses on using advanced analytical tools and techniques, including machine learning models like LLMs, to rapidly analyze large datasets and extract valuable insights. Therefore, it seems that the context of the demo does involve data retrieval and providing it as context to the language model. I apologize for any confusion earlier and am here to help clarify any doubts you might have. If you need any assistance in preparing for the demo or have any questions related to the topic, feel free to ask. Also, let me know if you would like me to set up a reminder for the demo.\n",
      "\n",
      "Human:\n",
      "Could it mean Retreive Augmented Generator?[/INST]\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'AI: That\\'s another interesting possibility, Human! \"Retrieve Augmented Generator (RAG)\" is also a term used in some research communities, particularly in Natural Language Processing (NLP). In this context, RAG refers to a framework that assists in generating responses to text-based queries by first retrieving relevant information and then augmenting it to generate more accurate and informative answers. So, while it doesn\\'t directly relate to the previous definition of RAPID Analysis Group, it\\'s still an intriguing alternative interpretation. However, without knowing the specific context of your demo, it\\'s difficult to say for certain which meaning applies. To clarify any doubts, you can always reach out to the organizers or check your email confirmation for more details. And, as always, I\\'d be happy to help you prepare for the demo or answer any questions you might have regarding NLP or related topics. Additionally, if you\\'d like me to set up a reminder for the demo, just let me know!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Could it mean Retreive Augmented Generator?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bf581-0d95-4410-b72d-b614b8167ca4",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "After updating the template and the model works quite well with summarised memory. Good to know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6a808-312b-4b8e-9d4c-a8ab93c00faf",
   "metadata": {},
   "source": [
    "### Memory Types\n",
    "\n",
    "Few types of memories are available. They have specific uses. Most of them are trying to conserve context by limiting the amount of memory carried forward in the conversation. Apart from the general `ConversationBufferMemory`, there are three different memories that conserve the amount of tokens carried forward:\n",
    "\n",
    "- **ConversationBufferWindowMemory** - does this by supplying as parameter how many exchanges between the Human and AI to carry forward.\n",
    "- **ConversationTokenBufferMemory** - same, but instead of number of conversation, number of tokens is specified. Note: here we also supply the llm so that the correct tokeniser is used to generate the tokens.\n",
    "- **ConversationSummaryMemory** - This memory creates a summary of the conversation over time.\n",
    "\n",
    "#### Other Types of Memory\n",
    "\n",
    "Some other types that we will see in future:\n",
    "\n",
    "- **Vector Data Memory** - Stores from conversation or other context in a vector DB and retreives the most relevant blocks of text - hmmm... RetreiverAugmentedGeneration (RAG) for context. Sounds familiar.\n",
    "- **Entity Memories** - Using an LLM, it remembers details about specific entities.. weird??\n",
    "\n",
    "You can also mix and match some types of memories together - Conversation + Entity to recall individuals. You can also store conversation in a normal SQL-like DB. But why would you do that if Vector DBs work quite well..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e88baf-647d-44b5-98c8-4b1aa24dbd49",
   "metadata": {},
   "source": [
    "### The Chains!\n",
    "\n",
    "The key building block of a LangChain application is naturally 'The Chain'!\n",
    "\n",
    "- `LLMChain`\n",
    "- Sequential Chains\n",
    "  - `SimpleSequentialChain`\n",
    "  - `SequentialChain`\n",
    "- `RouterChain`\n",
    "\n",
    "The chain usually combines LLM with a prompt and creates a building block that you can combine with other building blocks to carry on a sequence of operations on text or on other data.\n",
    "\n",
    "Let's start experimenting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97630094-5316-4a09-87f0-3f4203addbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain\n",
    "import pandas as pd\n",
    "from langchain.memory import SimpleMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c09c8055-571d-45bd-84b4-368f1c9dfc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0ead7d67-5e59-40e8-9511-10a0c86363cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.5, model=\"gpt-3.5-turbo-1106\")\n",
    "product_descr = \"A helpfull assistant web platform that assists management of small companies\\\n",
    "in their team management and team motivation procedures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d96fce31-bc20-450d-9eb0-901f83774417",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"MetalTech Robotics\"'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? Provide just one example of name\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "chain.run(\"Robots to assist in processing metal elements for the automotive industry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd1a80-22fb-4f32-8e26-91c380235932",
   "metadata": {},
   "source": [
    "Above LLMChain is the simplest possible chains, directly chaining prompt to llm and processing an input.\n",
    "Another type is the `SimpleSequentialChain` which takes in only one other chain as input and returns an output. See below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3040ade8-512f-4bbf-859d-950434108e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the company with the following name: {name}\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8da7d34c-6878-41c4-930e-552fca1e874e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"TeamBoost Pro\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mTeamBoost Pro provides innovative team-building solutions for companies, focusing on boosting productivity, collaboration, and overall team performance.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TeamBoost Pro provides innovative team-building solutions for companies, focusing on boosting productivity, collaboration, and overall team performance.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "simple_chain.run(product_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07f4972-97de-4910-9cc5-d893a257dd4b",
   "metadata": {},
   "source": [
    "Now lets try something a little more fun. Let's take in a movie review, lets ask the LLM to rate it on a scale from 1-10, then summarise it, we will then supply the real user rating to the LLM and ask it to tell us how it did compared to the ground truth, presenting the summary and its response as json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fdf971d1-8fdb-404e-9c5f-8c9d90ffde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "with open('./data/imdb_scraped_reviews_06.01-01.15.json','r') as f:\n",
    "    movie_dict = json.loads(f.read())\n",
    "movies = movie_dict[\"movies\"]\n",
    "rand_movie_index = np.random.randint(len(movies))\n",
    "rand_review_index = np.random.randint(len(movies[rand_movie_index][\"reviews\"]))\n",
    "movie = movies[rand_movie_index]\n",
    "review = movie[\"reviews\"][rand_review_index]\n",
    "review_copy = review[\"copy\"]\n",
    "review_human_rating = review[\"stars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "107409b2-e7e0-4d5e-8277-b6dfa2811abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wonka'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3569723d-6494-44de-b8b6-7af56ed50878",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.5, model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30513263-1603-46d9-ba4c-e210506e9bc8",
   "metadata": {},
   "source": [
    "> Note: Trying the open LLM Mistral 7B. \n",
    "\n",
    "Load the model as `open_llm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "184ce059-18d0-4d7b-9559-0e00d0df73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c5cc844f-3a12-4b1a-8dc8-bef635e0d126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f3bfdad0844255abd7a2361b90eae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650ad6af693e4a409ae01c5b601b8a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, load_in_8bit=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6f75e9e-3b23-4992-a4c6-f2e5f4754371",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_p = 0.8\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.15\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "text_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "open_llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a5f9320-237a-46f3-a68d-cb51b0289a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_copy = \"Take in a user review of a movie and rate the movie based on the sentiment of the review\\\n",
    "on a scale 1 to 10. 1 being a total flop, while 10 being a brilliant movie.\\\n",
    "Supply the rating and a one sentence explanation why you have rated it as such. Here is the review: {review}\"\n",
    "input_template = input_copy\n",
    "input_template_open = f\"<s>[INST]{input_copy}[INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19a5b6cf-4e0d-4663-89ce-02883ba327d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = ChatPromptTemplate.from_template(input_template_open)\n",
    "input_chain = LLMChain(llm=open_llm, prompt=input_prompt, output_key=\"review_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e7a989b-61be-4b55-8ea8-65eb8c0c8e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_rating_copy = \"\"\"Here is what your rating is of the movie based on the review: {review_rating} \\\n",
    "The human rating of the movie is actually: {human_rating} on the same scale 1 to 10. \\\n",
    "Can you grade how close your rating is based on the review's sentiment to the actual rating of the human reviewer {human_rating}\\\n",
    "on the A to F grading scale and provide the grade - \\\n",
    "'A' you have guessed the human rating completely, 'F' you have not guessed what the human rating is at all.\n",
    "\"\"\"\n",
    "grade_rating_template = grade_rating_copy\n",
    "grade_rating_template_open = f\"[INST]{grade_rating_copy}[/INST]\"\n",
    "grade_rating_prompt = ChatPromptTemplate.from_template(grade_rating_template_open)\n",
    "grade_rating_chain = LLMChain(llm=open_llm, prompt=grade_rating_prompt, output_key=\"rating_grade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e0264809-eeec-4140-a40f-88f7de2055e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_response_copy = \"\"\"Create a up to 100 words summary of the movie review.\n",
    "Review:\\n{review}\n",
    "\"\"\"\n",
    "final_response_template = final_response_copy\n",
    "final_response_template_open = f\"[INST]{final_response_copy}[/INST]\"\n",
    "final_response_prompt = ChatPromptTemplate.from_template(final_response_template_open)\n",
    "final_response_chain = LLMChain(llm=open_llm, prompt=final_response_prompt, output_key=\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8858abc6-8b48-41b7-9dfe-d4c5429e2233",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_of_chains = SequentialChain(\n",
    "    chains=[input_chain, grade_rating_chain, final_response_chain],\n",
    "    input_variables=[\"review\"],\n",
    "    output_variables=[\"review_rating\", \"rating_grade\", \"summary\"],\n",
    "    memory=SimpleMemory(memories={\"title\": movie[\"title\"], \"human_rating\": review[\"stars\"]}),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a616835a-4154-4c3e-82af-8fb6bf00dd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "r = chain_of_chains({\"review\": review[\"copy\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e895e764-870f-46eb-88ed-9b7e81eb9c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'I really had high hopes for this movie. Finally, I thought, Hollywood can take a step back from their incessant superhero schlock and do something genuinely creative and genuinely entertaining. And it seemed to be getting good reviews! Within the first thirty minutes, however, it becomes apparent this is a very drab, tedious affair. Somehow they managed to turn Willy Wonka into a joyless tale. If anything, it seemed to be slipping into \"Les Miz\" territory, and I was kind of hoping Olivia Colman might just go ahead and belt out \"Master of the House.\" One of the few redeeming qualities is Hugh Grant as a sophisticated Oompa-Loompa. Aside from that, the supporting characters are horrible, the music instantly forgettable, and the plot is muddled and tedious, pulling at your emotions with tales of dead mothers and orphans rather than merry Wonkaesque mayhem. For Pete\\'s sake, they were handed the keys to create Willy Wonka\\'s origin story and they couldn\\'t even make it watchable. Hollywood, just give up.',\n",
       " 'title': 'Wonka',\n",
       " 'human_rating': 5,\n",
       " 'review_rating': ' Based on the given user review, I would rate this movie a 3. The review expresses disappointment and frustration towards the movie, mentioning several aspects like unengaging characters, forgettable music, and a confusing plot. The reviewer also compares the movie unfavorably to other works and expresses a sense of wasted potential. These are all indicators of a below average film experience.',\n",
       " 'rating_grade': \" Based on the information provided in the review and the human rating, I would grade my estimation as a 'D'. While I was significantly off from the actual human rating, the review did convey elements of dissatisfaction with the movie that aligned with lower grades on the A-F scale.\",\n",
       " 'summary': ' In this disappointing film adaptation of \"Willy Wonka and the Chocolate Factory,\" expectations were high for an original and entertaining production. However, within the initial half hour, it became clear that this rendition lacked creativity and excitement. The once charming Willy Wonka transformed into a dreary character, and the supporting cast fell flat. The music failed to impress, and the plot was mired in melodrama, focusing on tragic backstories instead of whimsical chaos. A rare bright spot was Hugh Grant\\'s portrayal of a refined Oompa-Loompa. Overall, this attempt at reviving a beloved classic missed its mark, leaving audiences longing for the genuine magic of the original.'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89596e88-ae3d-480f-b6a6-b6c9fd3de859",
   "metadata": {},
   "source": [
    "### Routing Chains - The Router Chain\n",
    "\n",
    "When we need to make a decision what chain to load next based on some conditions we can use the Router Chain. To implement this, lets first create few different templates that we can load into the different chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe22b24e-f94a-4e2e-8178-2f0b24b86cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.01, model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c7238-1c53-451d-b458-5f1246d6074c",
   "metadata": {},
   "source": [
    "Here we create different templates that the router chain will route to depending on what it understands the question to be relevant to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca5285cd-9eb9-408a-b74c-b24e8aaa9885",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc32dd-954a-4571-bae9-0df35183d06b",
   "metadata": {},
   "source": [
    "We put this in a dictionary and from this dictionary we create the different chains with different templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0909dddd-d139-41de-b073-44497fca73cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"history\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6ca6fe47-0898-4b38-89f9-0a0c42ecde45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b274aafb-e122-46f5-9e86-979e48601d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt, verbose=True)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "\n",
    "destination_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c42e08ab-b351-42a4-b5ed-5989f22e7630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nhistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destination_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4ed34f6d-e7e9-4c08-94da-9f511c099e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41308d-f814-4b47-9252-d361c11507aa",
   "metadata": {},
   "source": [
    "Here is the instruction for the router chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "734e1ab6-dfa4-45f5-b1be-bcbc4683ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dd149613-8180-4ef1-8877-a8f7c5a18050",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destination_str)\n",
    "router_prompt = PromptTemplate(template=router_template, \n",
    "                               input_variables=[\"input\"], \n",
    "                               output_parser=RouterOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66f8b59f-3860-40f1-91aa-733ff791e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm=llm, prompt=router_prompt)\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains,\n",
    "                         default_chain=default_chain,\n",
    "                         verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2b37ead2-3ffd-4aef-94a7-24410cb387cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "history: {'input': 'Can you tell me when anti-matter was first coined as a term and by whom?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: You are a very good historian. You have an excellent knowledge of and understanding of people,events and contexts from a range of historical periods. You have the ability to think, reflect, debate, discuss and evaluate the past. You have a respect for historical evidenceand the ability to make use of it to support your explanations and judgements.\n",
      "\n",
      "Here is a question:\n",
      "Can you tell me when anti-matter was first coined as a term and by whom?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The term \"anti-matter\" was first coined by the physicist Arthur Schuster in 1898. He used the term to describe the hypothetical counterpart to ordinary matter, which would have the opposite electrical charge. This concept was later developed and popularized by physicist Paul Dirac in the 1920s as part of his work on quantum mechanics.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Can you tell me when anti-matter was first coined as a term and by whom?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24ae8220-3358-426c-8f79-d3ab9ef0ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics: {'input': 'what is anti-matter in simple terms?'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman: You are a very smart physics professor. You are great at answering questions about physics in a conciseand easy to understand manner. When you don't know the answer to a question you admit that you don't know.\n",
      "\n",
      "Here is a question:\n",
      "what is anti-matter in simple terms?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Anti-matter is essentially the opposite of regular matter. It is made up of particles that have the same mass as regular matter particles, but with opposite charge. For example, the anti-particle of an electron (a regular matter particle) is called a positron, which has a positive charge instead of a negative charge. When matter and anti-matter come into contact, they annihilate each other, releasing a large amount of energy. Anti-matter is a fascinating area of study in physics and has potential applications in areas such as energy production and medical imaging.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is anti-matter in simple terms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f68f6248-c102-45a4-8aeb-ab9357712776",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/chains/llm.py:316: UserWarning: The predict_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computer science: {'input': 'What is the Global Interpreter Lock (GIL) in Python? Please explain in simple terms and also explain how multi-threading is achieved with its presence.'}\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mHuman:  You are a successful computer scientist.You have a passion for creativity, collaboration,forward-thinking, confidence, strong problem-solving capabilities,understanding of theories and algorithms, and excellent communication skills. You are great at answering coding questions. You are so good because you know how to solve a problem by describing the solution in imperative steps that a machine can easily interpret and you know how to choose a solution that has a good balance between time complexity and space complexity. \n",
      "\n",
      "Here is a question:\n",
      "What is the Global Interpreter Lock (GIL) in Python? Please explain in simple terms and also explain how multi-threading is achieved with its presence.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The Global Interpreter Lock (GIL) in Python is a mechanism that allows only one thread to execute Python bytecode at a time. This means that even if you have multiple threads in your Python program, only one of them can execute at any given moment.\\n\\nThis might sound like a limitation, but it's actually a design choice that simplifies the implementation of the Python interpreter. The GIL ensures that memory management is handled safely, and it also makes it easier to work with C extensions and libraries.\\n\\nNow, when it comes to multi-threading in Python with the GIL present, you might think that it's not very effective. However, you can still achieve some level of parallelism by using multi-threading for I/O-bound tasks. This is because the GIL is released when a thread is waiting for I/O operations to complete, allowing other threads to execute.\\n\\nIn simple terms, the GIL in Python ensures that only one thread can execute Python code at a time, but you can still use multi-threading for tasks that involve waiting for I/O operations.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is GIL in python? Please explain in simple terms and also point out \\\n",
    "how is multi-threading achieved with its presence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df7e70-4dd9-4ea2-b9b8-48cc723c8764",
   "metadata": {},
   "source": [
    "### LangChain - Q&A over Documents\n",
    "\n",
    "An example might be a tool that would allow you to query a product catalog for items of interest. RAGs are great to complement LLMs with knowledge that was not part of their training, making them much more powerful and flexible.\n",
    "\n",
    "Lets start by importing the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d5ba2aa-e92c-4164-b907-4758de3ea6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docarray\n",
      "  Downloading docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from docarray) (1.26.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /opt/conda/lib/python3.11/site-packages (from docarray) (3.9.15)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /opt/conda/lib/python3.11/site-packages (from docarray) (2.6.4)\n",
      "Collecting rich>=13.1.0 (from docarray)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting types-requests>=2.28.11.6 (from docarray)\n",
      "  Using cached types_requests-2.31.0.20240311-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (4.9.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13.1.0->docarray)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (2.17.2)\n",
      "Collecting urllib3>=2 (from types-requests>=2.28.11.6->docarray)\n",
      "  Using cached urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading docarray-0.40.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached types_requests-2.31.0.20240311-py3-none-any.whl (14 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: urllib3, mdurl, types-requests, markdown-it-py, rich, docarray\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unstructured 0.12.6 requires urllib3==1.26.18, but you have urllib3 2.2.1 which is incompatible.\n",
      "botocore 1.34.51 requires urllib3<2.1,>=1.25.4; python_version >= \"3.10\", but you have urllib3 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed docarray-0.40.0 markdown-it-py-3.0.0 mdurl-0.1.2 rich-13.7.1 types-requests-2.31.0.20240311 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install docarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ba3ec6a-1910-47f8-b24b-b6384ad615d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5d514f73-7841-44f5-b869-b0412e19a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader, JSONLoader\n",
    "# In memory vector store as an alternative Chroma\n",
    "from langchain.vectorstores import DocArrayInMemorySearch \n",
    "from IPython.display import display, Markdown\n",
    "from langchain.llms import OpenAI\n",
    "# Index creator for the vector store\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from pydantic import ValidationError\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604072e1-5329-41d6-b30b-e2520d7715bf",
   "metadata": {},
   "source": [
    "Lets load a CSV file for testing. We can use one of the files we created with Movie Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8b7ffe56-5f5f-4e52-aab9-8be14e0f65ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain[docarray] in /opt/conda/lib/python3.11/site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.6.4)\n",
      "Collecting docarray<0.33.0,>=0.32.0 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached docarray-0.32.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.1.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.9.15)\n",
      "Requirement already satisfied: rich>=13.1.0 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.31.0.20240311)\n",
      "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (4.25.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain[docarray]) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (4.1.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.17.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (0.1.2)\n",
      "Using cached docarray-0.32.1-py3-none-any.whl (215 kB)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for hnswlib \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[60 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating tmp\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.11 -c /tmp/tmp6ry2ao8k.cpp -o tmp/tmp6ry2ao8k.o -std=c++14\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.11 -c /tmp/tmpzrb6j6s7.cpp -o tmp/tmpzrb6j6s7.o -std=c++11\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 410, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 395, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 131, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 104, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 91, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-0kx3z4mc/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 101, in build_extensions\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 70, in cpp_flag\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Unsupported compiler -- at least C++11 support is needed!\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for hnswlib\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build hnswlib\n",
      "\u001b[31mERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting jq\n",
      "  Downloading jq-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Downloading jq-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (668 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m668.1/668.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: jq\n",
      "Successfully installed jq-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install \"langchain[docarray]\"\n",
    "!pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "5d970a63-eb31-485b-9a85-85e9ecb5a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path=\"./data/movies_sentiment_06.01-01.15.csv\")\n",
    "json_loader = JSONLoader(file_path=\"./data/imdb_scraped_reviews_06.01-01.15.json\", \n",
    "                         jq_schema='.movies[]', text_content=False)\n",
    "# index = VectorstoreIndexCreator(vectorstore_cls=DocArrayInMemorySearch).from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60194b5f-34b4-4ac0-801d-2433016d9f00",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>stars</th>\n",
       "      <th>meta</th>\n",
       "      <th>votes</th>\n",
       "      <th>type</th>\n",
       "      <th>link</th>\n",
       "      <th>total_sentiment</th>\n",
       "      <th>total_stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>12th Fail</td>\n",
       "      <td>9.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85925</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt23849204/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>146 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>All of Us Strangers</td>\n",
       "      <td>8.1</td>\n",
       "      <td>89.0</td>\n",
       "      <td>4343</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt21192142/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>123 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ted</td>\n",
       "      <td>8.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9062</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt14824792/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>141 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Boy Swallows Universe</td>\n",
       "      <td>8.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6728</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt10399902/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>140 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>Godzilla Minus One</td>\n",
       "      <td>8.3</td>\n",
       "      <td>80.0</td>\n",
       "      <td>39321</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt23289160/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>138 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Society of the Snow</td>\n",
       "      <td>7.9</td>\n",
       "      <td>72.0</td>\n",
       "      <td>61274</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt16277242/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>137 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>Merry Christmas</td>\n",
       "      <td>8.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18323</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt15392282/</td>\n",
       "      <td>15 / 15</td>\n",
       "      <td>118 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>Mr Bates vs. The Post Office</td>\n",
       "      <td>8.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6119</td>\n",
       "      <td>TV Mini Series</td>\n",
       "      <td>https://www.imdb.com/title/tt27867155/</td>\n",
       "      <td>14 / 15</td>\n",
       "      <td>143 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>American Fiction</td>\n",
       "      <td>7.9</td>\n",
       "      <td>82.0</td>\n",
       "      <td>5339</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt23561236/</td>\n",
       "      <td>14 / 15</td>\n",
       "      <td>110 / 150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>The Iron Claw</td>\n",
       "      <td>8.1</td>\n",
       "      <td>73.0</td>\n",
       "      <td>14241</td>\n",
       "      <td>Movie</td>\n",
       "      <td>https://www.imdb.com/title/tt21064584/</td>\n",
       "      <td>14 / 15</td>\n",
       "      <td>125 / 150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                         title  stars  meta  votes  \\\n",
       "14          14                     12th Fail    9.2   NaN  85925   \n",
       "38          38           All of Us Strangers    8.1  89.0   4343   \n",
       "4            4                           Ted    8.1   NaN   9062   \n",
       "5            5         Boy Swallows Universe    8.2   NaN   6728   \n",
       "39          39            Godzilla Minus One    8.3  80.0  39321   \n",
       "9            9           Society of the Snow    7.9  72.0  61274   \n",
       "41          41               Merry Christmas    8.6   NaN  18323   \n",
       "42          42  Mr Bates vs. The Post Office    8.8   NaN   6119   \n",
       "47          47              American Fiction    7.9  82.0   5339   \n",
       "20          20                 The Iron Claw    8.1  73.0  14241   \n",
       "\n",
       "              type                                    link total_sentiment  \\\n",
       "14           Movie  https://www.imdb.com/title/tt23849204/         15 / 15   \n",
       "38           Movie  https://www.imdb.com/title/tt21192142/         15 / 15   \n",
       "4   TV Mini Series  https://www.imdb.com/title/tt14824792/         15 / 15   \n",
       "5   TV Mini Series  https://www.imdb.com/title/tt10399902/         15 / 15   \n",
       "39           Movie  https://www.imdb.com/title/tt23289160/         15 / 15   \n",
       "9            Movie  https://www.imdb.com/title/tt16277242/         15 / 15   \n",
       "41           Movie  https://www.imdb.com/title/tt15392282/         15 / 15   \n",
       "42  TV Mini Series  https://www.imdb.com/title/tt27867155/         14 / 15   \n",
       "47           Movie  https://www.imdb.com/title/tt23561236/         14 / 15   \n",
       "20           Movie  https://www.imdb.com/title/tt21064584/         14 / 15   \n",
       "\n",
       "   total_stars  \n",
       "14   146 / 150  \n",
       "38   123 / 150  \n",
       "4    141 / 150  \n",
       "5    140 / 150  \n",
       "39   138 / 150  \n",
       "9    137 / 150  \n",
       "41   118 / 150  \n",
       "42   143 / 150  \n",
       "47   110 / 150  \n",
       "20   125 / 150  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets visualise the head of the SCV below using pandas.df\n",
    "df = pd.read_csv(\"./data/movies_sentiment_06.01-01.15.csv\")\n",
    "# sort the df by total_sentiment column descending using a function to convert string to number\n",
    "def convert_to_number(x):\n",
    "    try:\n",
    "        return float(x[:x.find('/')])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# sort the df by total_sentiment column descending\n",
    "df = df.sort_values(by=\"total_sentiment\", ascending=False, key=lambda x: x.apply(convert_to_number))\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd2a1c44-1349-4115-a0dc-521a3480397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"list all the movies with positive sentiment. consider the movies with positive sentiment to be \\\n",
    "any of the movies in the list with total_sentiment of more than 10. \\\n",
    "return a table in markdown with columns, name of the movies, \\\n",
    "sentiment score and link\"\n",
    "alt_query = \"List the movies with stars numeric value of over 7 and return a table in markdown with columns, \\\n",
    "name of movie, stars, sentiment and link\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "29f057c2-97a3-4b9e-af05-64aed10ee323",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = index.query(alt_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "80bfc38f-b685-4c07-b55d-9d22036ca2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Name of Movie | Stars | Sentiment | Link |\n",
       "|---------------|-------|-----------|------|\n",
       "| Animal | 6.8 | 5 / 15 | https://www.imdb.com/title/tt13751694/ |\n",
       "| Maestro | 6.7 | 6 / 15 | https://www.imdb.com/title/tt5535276/ |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response)) # for some reason something is not quite right. There are more movies with 10+ sentiment, stars also does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5b12d1-588b-4076-b585-1c7f0c965641",
   "metadata": {},
   "source": [
    "> Lets do it step by step as per the guide and double check where the issue is.\n",
    "\n",
    "1. Lading - using the loader from above `CSVLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "72249ef7-fa3c-4656-8070-7f3ce179cb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119ea91-0e4f-440b-93c9-a60e02dcda58",
   "metadata": {},
   "source": [
    "2. Since the rows are very small, we will omit creating chunks\n",
    "3. Lets use the OpenAI embeddings to create embeddings for the docs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "91c0f165-9150-413f-8fc0-2fb306ff28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4269e50b-e850-4191-a24b-c17438ab72a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " (1536,),\n",
       " array([-0.01364027,  0.00315833, -0.00904409, -0.0185824 , -0.01099623]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = np.array(embeddings.embed_query(\"Hello world! My first name is Joseph.\"))\n",
    "embed.ndim, embed.shape, embed[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c52c27-78a2-4a24-9ba0-807edfa41955",
   "metadata": {},
   "source": [
    "Now that we have the embeddings, we can create a new vector DB with the DocArrayInMemorySearch like so\n",
    "\n",
    "4. Creating the DB with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "4545a181-1eec-4bde-baa3-d0130d6bebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 50 documents into 1310 chunks.\n"
     ]
    }
   ],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "chunks = split_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "7ddae6f4-97de-4a93-b561-c2854287864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8990129e-0829-42c9-8309-f24a0e9caf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Please suggest Mini Series with high ratings to watch.\"\n",
    "r = db.similarity_search(query, k=7)\n",
    "selected = list(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a904a-8494-4085-8fb4-514d297d504d",
   "metadata": {},
   "source": [
    "When we have the DB, we need to create a retreiver that will extract documents from it.\n",
    "\n",
    "5. Creating a retreiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "6945fa5e-46fa-4316-b9d6-1d0f04c2af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "retreiver = db.as_retriever(search_kwargs={\"k\": 7})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10170477-111a-4bf5-ab85-2af4dbadc4e4",
   "metadata": {},
   "source": [
    "6. Now we create the LLM to use to generate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "75029c6d-d0e5-4d4f-8f63-e75c767c758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf48702-4237-4952-be5d-9f74f68915f7",
   "metadata": {},
   "source": [
    "There are two ways to generate a response. Combining the documents supplied by the query into a long string and running through the llm:\n",
    "\n",
    "7. Extract response\n",
    "   - directly without a Retreiver Chain\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7cd60558-c0b4-4887-a659-76b19f51b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdocs = \"\\n\\n\".join([docs[i].page_content for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7c789523-69de-4077-a383-0ca0a29207b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "| Title                           | Stars | Link                                       | Sentiment |\n",
       "|---------------------------------|-------|--------------------------------------------|-----------|\n",
       "| Ted                             | 8.1   | [IMDb](https://www.imdb.com/title/tt14824792/) | 15/15     |\n",
       "| The Brothers Sun                | 7.7   | [IMDb](https://www.imdb.com/title/tt17632862/) | 12/15     |\n",
       "| Percy Jackson and the Olympians | 7.4   | [IMDb](https://www.imdb.com/title/tt12324366/) | 2/15      |\n",
       "| The Curse                       | 7.1   | [IMDb](https://www.imdb.com/title/tt13623608/) | 4/15      |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = llm.call_as_llm(f\"<docs>\\n{qdocs}\\n</docs>\\nInstruction: Please list all the TV Series with over 7.0 stars \\\n",
    "from the between the <docs> and </docs> in a markdown \\\n",
    "table with the following columns: Title, Stars, Link, Sentiment.\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e8eb5eed-7a9e-48b4-8cc0-8579d69000f4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<docs>\n",
      ": 4\n",
      "title: Ted\n",
      "stars: 8.1\n",
      "meta: N/A\n",
      "votes: 9062\n",
      "type: TV Mini Series\n",
      "link: https://www.imdb.com/title/tt14824792/\n",
      "total_sentiment: 15 / 15\n",
      "total_stars: 141 / 150\n",
      "\n",
      ": 7\n",
      "title: Echo\n",
      "stars: 6.1\n",
      "meta: N/A\n",
      "votes: 22711\n",
      "type: TV Mini Series\n",
      "link: https://www.imdb.com/title/tt13966962/\n",
      "total_sentiment: 0 / 15\n",
      "total_stars: 53 / 150\n",
      "\n",
      ": 2\n",
      "title: Fool Me Once\n",
      "stars: 6.9\n",
      "meta: N/A\n",
      "votes: 32681\n",
      "type: TV Mini Series\n",
      "link: https://www.imdb.com/title/tt5611024/\n",
      "total_sentiment: 3 / 15\n",
      "total_stars: 60 / 150\n",
      "\n",
      ": 13\n",
      "title: The Brothers Sun\n",
      "stars: 7.7\n",
      "meta: N/A\n",
      "votes: 11725\n",
      "type: TV Series\n",
      "link: https://www.imdb.com/title/tt17632862/\n",
      "total_sentiment: 12 / 15\n",
      "total_stars: 117 / 150\n",
      "\n",
      ": 25\n",
      "title: Berlin\n",
      "stars: 7.1\n",
      "meta: N/A\n",
      "votes: 20578\n",
      "type: TV Series\n",
      "link: https://www.imdb.com/title/tt16288804/\n",
      "total_sentiment: 1 / 15\n",
      "total_stars: 58 / 150\n",
      "\n",
      ": 10\n",
      "title: Percy Jackson and the Olympians\n",
      "stars: 7.4\n",
      "meta: N/A\n",
      "votes: 17609\n",
      "type: TV Series\n",
      "link: https://www.imdb.com/title/tt12324366/\n",
      "total_sentiment: 2 / 15\n",
      "total_stars: 53 / 150\n",
      "\n",
      ": 30\n",
      "title: The Curse\n",
      "stars: 7.1\n",
      "meta: N/A\n",
      "votes: 6906\n",
      "type: TV Series\n",
      "link: https://www.imdb.com/title/tt13623608/\n",
      "total_sentiment: 4 / 15\n",
      "total_stars: 59 / 150\n",
      "</docs>\n",
      "Instruction: Please list all the TV Series with over 7.0 stars from the between the <docs> and </docs> in a markdown table with the following columns: Title, Stars, Link, Sentiment.\n"
     ]
    }
   ],
   "source": [
    "print(f\"<docs>\\n{qdocs}\\n</docs>\\nInstruction: Please list all the TV Series with over 7.0 stars \\\n",
    "from the between the <docs> and </docs> in a markdown \\\n",
    "table with the following columns: Title, Stars, Link, Sentiment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae474c1a-09ce-4471-8fb7-4dc6490b88ba",
   "metadata": {},
   "source": [
    "7. Extra response\n",
    "   - With a Retreival Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "7889d616-4791-432e-9541-7f8d9d6e08da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query_for_retreiver = \"Based solely on the information provided as context, \\\n",
    "can you tell me what the reviews sentiment on Boy Swallows Universe is?\"\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \n",
    "                                 retriever=retreiver, \n",
    "                                 return_source_documents=True,\n",
    "                                 verbose=True)\n",
    "response = qa(query_for_retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8c2f6838-4a14-4c49-bdb4-51d4dcc63fb0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='{\"title\": \"Oppenheimer\", \"stars\": 8.4, \"link\": \"https://www.imdb.com/title/tt15398776/reviews\", \"meta\": 89, \"votes\": 606834, \"type\": \"Movie\", \"reviews\": [{\"copy\": \"One of the most anticipated films of the year for many people, myself included, Oppenheimer largely delivers. Much of it\\'s great. I feel like I loved two of its three hours, and liked the other hour.... but it\\'s that fact that stops me from adoring the entire thing. I know with Christopher Nolan\\'s Dunkirk, that clicked on a second', metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 0}),\n",
       " Document(page_content=\"Strauss, with a few flashbacks to a couple of meetings he was involved in with Oppenheimer . It's a political drama, with not much drama. There isn't enough emotional connection to the characters to care about it. Will Oppenheimer lose his security clearance? Will Strauss get served by karma at his cabinet confirmation hearing? Who cares? If you're not invested in any of the characters, why would you care about what happens to them in the end? Maybe this is the best one can do with the source\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 31583}),\n",
       " Document(page_content=\"in the film, Oppenheimer is giving a speech to toast the success of the American empire, but the room turns white, and rumbles, and woman's skin flays. And finally, another scene with Emily Blunt, as Kitty Oppenheimer, giving one of those wolf pack prosecutors a piece of her mind in the closed hearing, really steals the show. Blunt was truly an enjoyable part of this movie, though she had little screen time. There's not much else to say. I feel like I just listened to some hyperactive child try\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 26795}),\n",
       " Document(page_content='him the utmost respect of many of his peers, even as a young student. However, almost immediately after, the film cuts to him on trial for allegedly having ties to communism, an accusation which threatens to completely derail the positive reputation bestowed upon him. As the judicial committee interrogates him with hard hitting questions, Oppenheimer is haunted by the errors of his own judgement, shown to the audience through flashbacks of varying length to pivotal times in his life. These', metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 7575}),\n",
       " Document(page_content=\"scenes are too glaring and disturbing. The film is also visually confusing, with Nolan's trademark use of shaky cameras and quick cuts. The only bright spot in the film is Matt Damon's performance as General Leslie Groves. Damon gives a nuanced and complex performance, but he's not enough to save the film. Overall, Oppenheimer is a mess. It's a long, confusing, and ultimately unsatisfying film. Nolan is an overrated director, and he often tells a simple story in a complicated and overlong\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 35596}),\n",
       " Document(page_content='most intense scenes I ever watched in a movie and reminding me of the true power of the cinematic experience like no other movie did in recent years. The year is only half-way through but right now this is my top pick for the upcoming awards season. Picture, Writing, Directing, Acting, Score-- Oppenheimer is a winner on all fronts. A rare feat for filmmaking and a salient reminder that cinema is not dead. I highly recommend this film to everyone. Watched it once already, and going back to the', metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 29586}),\n",
       " Document(page_content=\"be immersed. After that, for the most part, he's back a it, whisking us from one quick scene to the next at a pace that tries so hard to be breathless and exciting and just ends up distracting and frustrating. Still, I felt more in tune for the second half of the film, because I could now sense the dilemma, the emotional conflict in Oppenheimer after Hiroshima and Nagasaki. In one of the best scenes in the film, Oppenheimer is giving a speech to toast the success of the American empire, but the\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 26392}),\n",
       " Document(page_content='with and Jean being who he personally wants to be with. This draws parallels to that of Oppenheimer choosing between acting on instinct or acting on intellect when assisting in the construction of the bomb, which again reminds the audience of his flawed human qualities. It can be difficult to give up following your heart but when the fate of the world rests on your pragmatic decision making, sometimes you have no other choice. As a biopic and a Christopher Nolan film, \\\\\"Oppenheimer\\\\\" exceeds', metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 13174}),\n",
       " Document(page_content=\"or where it fits in sequentially or contextually into the story. There is the public hearing, and there is a closed hearing with a wolf pack of hungry prosecutors, and then there is some semblance of Oppenheimer's backstory - his love life, his gradual involvement in government, leading to a general (Matt Damon) for some reason hiring him to be the head of Los Alamos. All of this is mashed together, scenes never really more than a few seconds long before cutting to somewhere else, something\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 12, 'start_index': 22381}),\n",
       " Document(page_content=\"off simply enough, but grows and swells to become very, very powerful. The acting by the leads is all first-rate. How they convinced the men to play those often despicable roles I don't know, but you surely cheer when their women finally stand up to them. Some of those moments got applause from the audience where I saw it. (Note: I was almost the only man in the sold-out theater.) It isn't a short movie, but, as with the only other movie this year that rises to its level, Oppenheimer, I was not\", metadata={'source': '/home/jovyan/work/data/imdb_scraped_reviews_06.01-01.15.json', 'seq_num': 46, 'start_index': 21581})]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"source_documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "080d49f8-24c2-4106-abc8-c4c681201ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the reviews provided in the context, the sentiment on \"Boy Swallows Universe\" seems to be overwhelmingly positive. Reviewers have described it as a joy to watch, incredibly original and funny, and a celebration of Aussie talent. The positive comments about the characters, cast, and adaptation from the book to the screen indicate a favorable sentiment towards the TV Mini Series."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response['result']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f48946d-47b4-4257-bc69-bc02250224e9",
   "metadata": {},
   "source": [
    "### Analysing and mesuring Langchain Apps Performance\n",
    "\n",
    "### Outline:\n",
    "\n",
    "- Example generation\n",
    "- Manual evaluation (and debuging)\n",
    "- LLM-assisted evaluation\n",
    "- LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d05ce1e-c2ef-4b9f-b8fb-e73303f83d43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jq in /opt/conda/lib/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: docarray in /opt/conda/lib/python3.11/site-packages (0.40.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.11/site-packages (from docarray) (1.26.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /opt/conda/lib/python3.11/site-packages (from docarray) (3.9.15)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /opt/conda/lib/python3.11/site-packages (from docarray) (2.6.4)\n",
      "Requirement already satisfied: rich>=13.1.0 in /opt/conda/lib/python3.11/site-packages (from docarray) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /opt/conda/lib/python3.11/site-packages (from docarray) (2.31.0.20240406)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from docarray) (0.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic>=1.10.8->docarray) (4.9.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray) (2.17.2)\n",
      "Requirement already satisfied: urllib3>=2 in /opt/conda/lib/python3.11/site-packages (from types-requests>=2.28.11.6->docarray) (2.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
      "Requirement already satisfied: langchain[docarray] in /opt/conda/lib/python3.11/site-packages (0.1.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.6.4)\n",
      "Collecting docarray<0.33.0,>=0.32.0 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached docarray-0.32.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.0.28)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.31 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.1.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain[docarray]) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain[docarray]) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (0.9.0)\n",
      "Requirement already satisfied: orjson>=3.8.2 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.9.15)\n",
      "Requirement already satisfied: rich>=13.1.0 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (13.7.1)\n",
      "Requirement already satisfied: types-requests>=2.28.11.6 in /opt/conda/lib/python3.11/site-packages (from docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.31.0.20240406)\n",
      "Collecting hnswlib>=0.6.2 (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray])\n",
      "  Using cached hnswlib-0.8.0.tar.gz (36 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.19.0 in /opt/conda/lib/python3.11/site-packages (from docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (4.25.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain[docarray]) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (4.1.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain[docarray]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain[docarray]) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain[docarray]) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.31->langchain[docarray]) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (2.17.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain[docarray]) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray<0.33.0,>=0.32.0->docarray[hnswlib]<0.33.0,>=0.32.0; extra == \"docarray\"->langchain[docarray]) (0.1.2)\n",
      "Using cached docarray-0.32.1-py3-none-any.whl (215 kB)\n",
      "Building wheels for collected packages: hnswlib\n",
      "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for hnswlib \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[60 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m creating tmp\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.11 -c /tmp/tmpi3re1xfm.cpp -o tmp/tmpi3re1xfm.o -std=c++14\n",
      "  \u001b[31m   \u001b[0m gcc -pthread -B /opt/conda/compiler_compat -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/conda/include -fPIC -O2 -isystem /opt/conda/include -fPIC -I/opt/conda/include/python3.11 -c /tmp/tmpncqkvbma.cpp -o tmp/tmpncqkvbma.o -std=c++11\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 410, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 395, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/build_meta.py\", line 311, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 131, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/__init__.py\", line 104, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/dist.py\", line 967, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 91, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-build-env-u11y5uwy/overlay/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 101, in build_extensions\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 70, in cpp_flag\n",
      "  \u001b[31m   \u001b[0m RuntimeError: Unsupported compiler -- at least C++11 support is needed!\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for hnswlib\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build hnswlib\n",
      "\u001b[31mERROR: Could not build wheels for hnswlib, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: tiktoken in /opt/conda/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2023.12.25)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install jq\n",
    "!pip install docarray\n",
    "!pip install \"langchain[docarray]\"\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d3e4a64-70d7-4b5d-8651-722b41629b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b9898f-757b-43a8-a73e-098f61fb785b",
   "metadata": {},
   "source": [
    "#### Create our QandA application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3ebdacb-5b72-4ef0-96c2-80c7be844da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.document_loaders import CSVLoader, JSONLoader\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e95a31b-b8f5-44f2-bbe1-90eb7af52815",
   "metadata": {},
   "source": [
    "Lets load the data from the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cad861f3-c00a-4d84-8fe2-075224c6b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_loader = JSONLoader(file_path=\"./data/imdb_scraped_reviews_06.01-01.15.json\", \n",
    "                         jq_schema='.movies[]', text_content=False)\n",
    "data = json_loader.load()\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=300,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64444f5c-5c90-4415-916f-418d575ab108",
   "metadata": {},
   "source": [
    "### Simplify the documents a little bit\n",
    "\n",
    "To test our evaluation, I have compiled a single document containing the movie reviews for Poor Things.\n",
    "\n",
    "The reviews are taken from my data scraping the IMDB. Below is the data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2867dd-a34d-44d2-b0b5-b1c2e8a010fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data = data[10:15]\n",
    "dict = [json.loads(doc.page_content) for doc in small_data]\n",
    "type(dict)\n",
    "# dict_str = \"\\n\".join([f\"{doc['title']}\\n\\n{doc['copy']}\\n\\n===============\\n\" for doc in dict[\"reviews\"]])\n",
    "# custom_doc = [Document(page_content=dict_str, metadata={'source': 'custom_doc', 'row': 0})]\n",
    "# custom_chunks = split_text(custom_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19ff8fa3-0e9f-4606-ab74-91bedff6b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5 documents into 63 chunks.\n"
     ]
    }
   ],
   "source": [
    "list_of_docs = []\n",
    "reviews = [{'reviews': doc['reviews'], 'title': doc['title']} for doc in dict ]\n",
    "for doc_reviews in reviews:\n",
    "    list_of_docs.append(\n",
    "        Document(\n",
    "            page_content=f\"Movie: {doc_reviews['title']}\\n\\nReviews:\\n\\n\" + \"\\n\\n\".join([f\"{review['title']}\\n\\n{review['copy']}\\n\\n===============\" for review in doc_reviews['reviews'][:5]]),\n",
    "            metadata={'source': doc_reviews['title'], 'row': 0}\n",
    "        ))\n",
    "chunks = split_text(list_of_docs)\n",
    "# doc_of_reviews = [\"\\n\".join() for my_doc in dict for review in my_doc[\"reviews\"]]\n",
    "# doc_of_reviews\n",
    "# doc_chunks = split_text(list_of_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a3fd3d4-2f3d-4529-a957-53ae771d8a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.1, model=\"gpt-3.5-turbo\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a62cbec-295b-41a7-a952-ed35142bf3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07ad1c34-2264-449f-913d-cc6200188698",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", \n",
    "                                 retriever=db.as_retriever(), \n",
    "                                 verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad1e0ab-6236-4948-bc73-0db5a639cab6",
   "metadata": {},
   "source": [
    "Ask the LLM a simple question to test if it works. It seems working. Lets proceed further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3bb3fc81-af38-4d62-9865-367052af5dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The best thing people say about the \"Poor Things\" movie is that it is a monumental achievement, the funniest, smartest, best-looking, and most entertaining film of 2023. It is described as a cinematic triumph that pushes boundaries, challenges expectations, and delivers an unforgettable experience. The film is praised for its storytelling brilliance, stellar performances, daring narrative, and radical inventiveness. It is considered a masterpiece and a potential cinematic milestone by future generations.'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run('what is the best thing that people say about the Poor Things movie?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4186d5b-7c02-4178-a16b-094bebd0c084",
   "metadata": {},
   "source": [
    "1. The example generation\n",
    "\n",
    "Below we prepare few examples and create QA pairs with ChatGPT. We assume this to be the ground truth. Although it in fact isn't. We can now use these generated QA pairs to compare with the answers of an open LLM model and ask the GPT to evaluate if the meaning match the generated ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a472fb-7eea-4923-bec5-6601eec69cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1b7fa8f-a62d-4ad6-8ff9-19cd8c5b5f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain/chains/llm.py:344: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(llm)\n",
    "# tt = [{\"doc\":t} for t in list_of_docs]\n",
    "\n",
    "examples = example_gen_chain.apply_and_parse([{\"doc\":t} for t in list_of_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86a2dd39-41f7-4209-ad95-f24f5e8a851d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qa_pairs': {'query': 'Based on the document provided, what are some common criticisms of the portrayal of Percy Jackson in the show?',\n",
       "   'answer': 'Some common criticisms of the portrayal of Percy Jackson in the show include the fact that his character does not feel like the Percy from the books, as he lacks the snarky and quick-witted personality, his dialogue is not funny or witty, he chooses violence over using his words, and he does not stand up to Gabe like he does in the books. Additionally, viewers feel that the new Percy is not a compelling protagonist and they do not find themselves rooting for him.'}},\n",
       " {'qa_pairs': {'query': 'Who is credited as the lead actor to beat at this stage for early awards consideration in the film \"Oppenheimer\"?',\n",
       "   'answer': 'Cillian Murphy is credited as the lead actor to beat at this stage for early awards consideration in the film \"Oppenheimer\".'}},\n",
       " {'qa_pairs': {'query': 'Who are the main characters in the movie \"The Holdovers\" and what are their roles in the story?',\n",
       "   'answer': 'The main characters in the movie \"The Holdovers\" are Paul Hunham, played by Paul Giamatti, who is a professor at an all boys boarding school, Angus Tully, played by Dominic Sessa, who is one of the students at the school, and Mary Lamb, played by Da\\'Vine Joy Randolph, who is the school\\'s cook. Paul Hunham finds himself alone with Angus and Mary in an unexpected chain of events, leading to a slice-of-life style piece of cinema where broken people find comfort in each other\\'s company.'}},\n",
       " {'qa_pairs': {'query': 'What are some of the positive aspects highlighted in the reviews of \"The Brothers Sun\"?',\n",
       "   'answer': 'The reviews mention that the show has plenty of action, laughs, drama, a talented cast, beautifully acted and written, addicting from start to finish, high production value, professional fight scenes, great comedy interwoven, brave for having only Asian actors, a pleasantly simple story, great action and fight choreography, occasional great humor, and being the best show on Netflix that year.'}},\n",
       " {'qa_pairs': {'query': 'According to the document, what are some key aspects of the movie \"12th Fail\" that make it stand out and inspire viewers?',\n",
       "   'answer': 'Some key aspects of the movie \"12th Fail\" that make it stand out and inspire viewers include:'}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28862aa1-594a-428c-9cd8-27628fbfa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used for debugging\n",
    "# import langchain\n",
    "# langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5132b3-c7fa-4eec-b9dd-cf50186d115e",
   "metadata": {},
   "source": [
    "2. The answers by an open LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2b5bb1b-0d31-496f-a5c5-f67fbcd660d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import torch\n",
    "model_name = \"../ext_models/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8107b331-7233-410c-9588-1dc9fa2b9950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608b9d74e36d4b84a27f0671fa87df0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e6d5165-7fc1-4f56-9687-ef28eb284d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"./data/embeddings/gte-large/\", \n",
    "                                       model_kwargs={\"device\": \"cuda\"}, \n",
    "                                       encode_kwargs={\"normalize_embeddings\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9557765f-5f48-4979-9dc1-25bc3df2cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DocArrayInMemorySearch.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac9c2808-6454-443e-93eb-a8dfc5a2d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = GenerationConfig.from_pretrained(model_name)\n",
    "generation_config.max_new_tokens = 512\n",
    "generation_config.temperature = 0.1\n",
    "generation_config.top_p = 0.9\n",
    "generation_config.do_sample = True\n",
    "generation_config.repetition_penalty = 1.1\n",
    "generation_config.pad_token_id = tokenizer.eos_token_id\n",
    "open_llm = HuggingFacePipeline(pipeline=pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=True,\n",
    "    generation_config=generation_config,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "459190dc-1737-49b9-a501-5cbffec7385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=open_llm, chain_type=\"stuff\", \n",
    "                                 retriever=db.as_retriever(), \n",
    "                                 verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c798072-6dee-48c1-8bbf-fab2a85409aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Paul Hunham, played by Paul Giamatti, is the main teacher in 'The Holdovers'.\""
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Who is the main teacher in the Holdovers?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f6884be-a355-42bf-b661-379641ff5a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply([example[\"qa_pairs\"] for example in examples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "320f3bfe-55a1-4c46-87ba-c178cff20179",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11abef1b-00f1-4d9d-aed8-27c5678e9361",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "graded_outputs = eval_chain.evaluate([example[\"qa_pairs\"] for example in examples], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80b4a610-77ac-48fe-93df-1105d99b6d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'INCORRECT'},\n",
       " {'results': 'CORRECT'},\n",
       " {'results': 'CORRECT'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e874c069-3470-4f99-a0f7-d6180f26caa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: Based on the document provided, what are some common criticisms of the portrayal of Percy Jackson in the show?\n",
      "Real Answer: Some common criticisms of the portrayal of Percy Jackson in the show include the fact that his character does not feel like the Percy from the books, as he lacks the snarky and quick-witted personality, his dialogue is not funny or witty, he chooses violence over using his words, and he does not stand up to Gabe like he does in the books. Additionally, viewers feel that the new Percy is not a compelling protagonist and they do not find themselves rooting for him.\n",
      "Predicted Answer:  Some common criticisms of the portrayal of Percy Jackson in the show include his lack of humor and wit, his passive role in solving problems, and his tendency to resort to violence instead of using his words. Critics also note that he seems out of character in comparison to the Percy in the books. Additionally, some viewers have expressed disappointment that the actors' appearances don't match their mental images of the characters, but this is not a universal criticism. Overall, there seems to be a consensus among critics that the Percy in the show is not as compelling or engaging as the Percy in the books.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: Who is credited as the lead actor to beat at this stage for early awards consideration in the film \"Oppenheimer\"?\n",
      "Real Answer: Cillian Murphy is credited as the lead actor to beat at this stage for early awards consideration in the film \"Oppenheimer\".\n",
      "Predicted Answer:  Cillian Murphy is credited as the lead actor in the film \"Oppenheimer\".\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 2:\n",
      "Question: Who are the main characters in the movie \"The Holdovers\" and what are their roles in the story?\n",
      "Real Answer: The main characters in the movie \"The Holdovers\" are Paul Hunham, played by Paul Giamatti, who is a professor at an all boys boarding school, Angus Tully, played by Dominic Sessa, who is one of the students at the school, and Mary Lamb, played by Da'Vine Joy Randolph, who is the school's cook. Paul Hunham finds himself alone with Angus and Mary in an unexpected chain of events, leading to a slice-of-life style piece of cinema where broken people find comfort in each other's company.\n",
      "Predicted Answer:  The main characters in the movie \"The Holdovers\" are Paul Hunham, played by Dominic Sessa, and Angus Tully, played by Alex Wolff. Paul is an older, dejected history teacher who has taught at Barton all his career. He's a demanding curmudgeon who almost everyone dislikes. Angus is a student who stays behind at Barton over the holidays due to his parents being unable to pick him up. They form an unlikely bond during their time together, along with the school's cook, Mary Lamb, played by Da'Vine Joy Randolph, whose son has recently been killed in Vietnam. Other characters include the school's headmaster, Hardy Woodrup, played by Andrew Garman, his administrative assistant, Lydia Crane, played by Carrie Preston, and janitor Danny, played by Naheem Garcia.\n",
      "Predicted Grade: INCORRECT\n",
      "\n",
      "Example 3:\n",
      "Question: What are some of the positive aspects highlighted in the reviews of \"The Brothers Sun\"?\n",
      "Real Answer: The reviews mention that the show has plenty of action, laughs, drama, a talented cast, beautifully acted and written, addicting from start to finish, high production value, professional fight scenes, great comedy interwoven, brave for having only Asian actors, a pleasantly simple story, great action and fight choreography, occasional great humor, and being the best show on Netflix that year.\n",
      "Predicted Answer:  The reviews praise the show for its action, humor, drama, acting, writing, and addictive nature. They also mention the talented cast, particularly Michelle Yeoh, and the beautiful direction and cinematography. Some reviewers were surprised by the performances of the lesser-known actors and were impressed by the show's ability to keep them engaged from start to finish. Overall, the reviews suggest that \"The Brothers Sun\" is a well-crafted and entertaining series that delivers on all fronts.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 4:\n",
      "Question: According to the document, what are some key aspects of the movie \"12th Fail\" that make it stand out and inspire viewers?\n",
      "Real Answer: Some key aspects of the movie \"12th Fail\" that make it stand out and inspire viewers include:\n",
      "Predicted Answer:  The movie \"12th Fail,\" directed by Vidhu Vinod Chopra, tells the story of a young man named Manoj who persistently attempts to pass the Civil Services Examination (UPSC) despite numerous failures. The film's inspiring storyline, based on true events, resonates with audiences due to its portrayal of determination, perseverance, and the importance of having strong support systems. The acting performances, particularly by Vikrant Massey, are exceptional and have been praised for their authenticity and emotion. The screenplay is also notable for its brilliance and ability to evoke various feelings in viewers. Additionally, the film's themes of friendship, love, and overcoming adversity make it a relatable and memorable experience.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750593c-2b7b-42b9-981b-f45ac88b9a9d",
   "metadata": {},
   "source": [
    "### LangChain Agents\n",
    "\n",
    "Agents are like gateways to access 3rd party data. Data stores, APIs and DBs, Search Engines, File Systems and so on. They facilitate the exploitation of LLMs as reasoning operatives that can help give advice or automate tasks. LLMs are not only data stores, but they can actually use this knowledge to provide somewhat intelligent output that can then be further incorporated in business logic. \n",
    "\n",
    "Lets try the following exciting cutting edge stuff:\n",
    "\n",
    "- Using built in LangChain tools: DuckDuckGo search and Wikipedia\n",
    "- Defining your own tools\n",
    "\n",
    "First some setting up. **Important:** we set the temperature to 0, since we are using the openAI LLM as a reasoning agent. We want the reasoning to be as good and precise as possible removing all the randomness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51b15557-6945-45dc-85ca-1e289d54d249",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_experimental in /opt/conda/lib/python3.11/site-packages (0.0.57)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.15 in /opt/conda/lib/python3.11/site-packages (from langchain_experimental) (0.1.15)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.41 in /opt/conda/lib/python3.11/site-packages (from langchain_experimental) (0.1.41)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.32 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.0.32)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (0.1.27)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.6.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain<0.2.0,>=0.1.15->langchain_experimental) (8.2.3)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /opt/conda/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.41->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.20.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.15->langchain_experimental) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.15->langchain_experimental) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.15->langchain_experimental) (3.0.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.15->langchain_experimental) (1.0.0)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.11/site-packages (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /opt/conda/lib/python3.11/site-packages (from numexpr) (1.26.4)\n",
      "Collecting wikipedia\n",
      "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=e515f7eb127b4aaf0c7da0a19b8f2e1f4bd824a1e68efd6edd09ab4436523b84\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_experimental\n",
    "!pip install numexpr\n",
    "!pip install -U wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45af8837-4f57-4363-b1b5-4ed43364b2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6c5e25c-ecd4-40cd-8dac-d33a6ca73abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a10efd5-44a2-44ef-a2c0-0523da680792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "tools = load_tools([\"llm-math\", \"wikipedia\"], llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b611e0c-dd3a-47a5-9cab-051a87e45350",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "            tools, \n",
    "            llm, \n",
    "            agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "            handle_parsing_errors=True,\n",
    "            verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1babe6a-d634-48c0-b91e-2d7d59ab8c6e",
   "metadata": {},
   "source": [
    "#### Math problems with the llm-math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ec46875-e619-4d95-887d-4aa2e3e341aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find 25% of 300, we can multiply 300 by 0.25.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"300 * 0.25\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: 25% of 300 is 75.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'what is 25% of 300?', 'output': '25% of 300 is 75.'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"what is 25% of 300?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b602c7-01b5-4437-aaa2-d93fc23f3a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To calculate the total cost for one month, I need to find out how many hours are in a month and then multiply that by the cost per hour.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"24 * 30 * 0.10\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 72.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The bill for one month in dollars if the machine is running the whole time will be $72.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'if I know that one hour of a Amazon AWS machine costs 10c, what will my bill be for one month in dollars if the machine is running the whole time?',\n",
       " 'output': 'The bill for one month in dollars if the machine is running the whole time will be $72.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"if I know that one hour of a Amazon AWS machine costs 10c, \\\n",
    "what will my bill be for one month in dollars if the machine is \\\n",
    "running the whole time?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21faedc0-df6b-45ed-912e-4923c3bf2f41",
   "metadata": {},
   "source": [
    "#### Wikipedia search\n",
    "\n",
    "Note to self: as of today the 10th of April, GPT 3.5 still thinks the monarch is Queen Elizabeth II. Below is the correct answer as of today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2dcf9d7-00f3-414f-9784-c198543d478d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I will use Wikipedia to find information about the current King of England.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"wikipedia\",\n",
      "  \"action_input\": \"King of England\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mPage: Monarchy of the United Kingdom\n",
      "Summary: The monarchy of the United Kingdom, commonly referred to as the British monarchy, is the constitutional form of government by which a hereditary sovereign reigns as the head of state of the United Kingdom, the Crown Dependencies and the British Overseas Territories. The current monarch is King Charles III, who ascended the throne on 8 September 2022, upon the death of his mother, Queen Elizabeth II.\n",
      "The monarch and their immediate family undertake various official, ceremonial, diplomatic and representational duties. Although formally the monarch has authority over the government—which is known as \"His/Her Majesty's Government\"—this power may only be used according to laws enacted in Parliament and within constraints of convention and precedent. In practice the monarch's role, including that of Head of the British Armed Forces, is limited to functions such as bestowing honours and appointing the prime minister, which are performed in a non-partisan manner.  The UK Government has called the monarchy \"a unique soft power and diplomatic asset\". The Crown also occupies a unique cultural role, serving as an unofficial brand ambassador for British interests and values abroad, increasing tourism at home, and promoting charities throughout civil society.The British monarchy traces its origins from the petty kingdoms of Anglo-Saxon England and early medieval Scotland, which consolidated into the kingdoms of England and Scotland by the 10th century. England was conquered by the Normans in 1066, after which Wales also gradually came under the control of Anglo-Normans. The process was completed in the 13th century when the Principality of Wales became a client state of the English kingdom. The Anglo-Normans also established the Lordship of Ireland. Meanwhile, Magna Carta began the process of reducing the English monarch's political powers. In the 16th century, English and Scottish monarchs played a central role in what became the religious English Reformation and Scottish Reformation, and the English king became King of Ireland. Beginning in 1603, the English and Scottish kingdoms were ruled by a single sovereign. From 1649 to 1660, the tradition of monarchy was broken by the republican Commonwealth of England, which followed the Wars of the Three Kingdoms. Following the installation of William III and Mary II as co-monarchs in the Glorious Revolution, the Bill of Rights 1689, and its Scottish counterpart the Claim of Right Act 1689, further curtailed the power of the monarchy and excluded Roman Catholics from succession to the throne. In 1707, the kingdoms of England and Scotland were merged to create the Kingdom of Great Britain, and in 1801, the Kingdom of Ireland joined to create the United Kingdom of Great Britain and Ireland. \n",
      "Beginning in the 16th century, the monarch was the nominal head of what came to be the vast British Empire, which covered a quarter of the world's land area at its greatest extent in 1921.  The title Emperor of India was added to the British monarch's titles between 1876 and 1948. The Balfour Declaration of 1926 recognised the evolution of the Dominions of the Empire into separate, self-governing countries within a Commonwealth of Nations. Also in this period, the monarchy in Ireland eventually became limited to Northern Ireland. In the years after the Second World War, the vast majority of British colonies and territories became independent, effectively bringing the Empire to an end. George VI and his successors adopted the title Head of the Commonwealth as a symbol of the free association of its independent member states. The United Kingdom and fourteen other independent sovereign states that share the same person as their monarch are called Commonwealth realms. Although the monarch is shared, each country is sovereign and independent of the others, and the monarch has a different, specific, and official national title and style for each realm.  Although the term \u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "\n",
      "Final Answer: King Charles III\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Who is the King of England?', 'output': 'King Charles III'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"Who is the King of England?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040500b-b34a-4b7c-842a-dc46d5f8e7ff",
   "metadata": {},
   "source": [
    "#### Writing Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ed8b849-6ecd-46b8-9e69-c971faf66b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_python_agent(\n",
    "    llm,\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34e2c9b3-0cd8-41c9-a330-5032b25fcae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mWe can use the `sorted()` function in Python to sort the list of customers based on their last name and then first name.\n",
      "Action: Python_REPL\n",
      "Action Input: sorted([['Harrison', 'Chase'], ['Lang', 'Chain'], ['Dolly', 'Too'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Jen', 'Ayai']], key=lambda x: (x[1], x[0]))\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: [['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"[['Jen', 'Ayai'], ['Harrison', 'Chase'], ['Lang', 'Chain'], ['Elle', 'Elem'], ['Geoff', 'Fusion'], ['Trance', 'Former'], ['Dolly', 'Too']]\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_list = [[\"Harrison\", \"Chase\"], \n",
    "                 [\"Lang\", \"Chain\"],\n",
    "                 [\"Dolly\", \"Too\"],\n",
    "                 [\"Elle\", \"Elem\"], \n",
    "                 [\"Geoff\",\"Fusion\"], \n",
    "                 [\"Trance\",\"Former\"],\n",
    "                 [\"Jen\",\"Ayai\"]\n",
    "                ]\n",
    "agent.run(f\"\"\"Sort these customers by \\\n",
    "last name and then first name \\\n",
    "and print the output: {customer_list}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfbdc04f-d7a3-4d05-863e-3563a6680f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_list = sorted(customer_list, key=lambda x: (x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a69a54c-9e2d-4830-9fbf-b267b74b1836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Jen', 'Ayai'],\n",
       " ['Lang', 'Chain'],\n",
       " ['Harrison', 'Chase'],\n",
       " ['Elle', 'Elem'],\n",
       " ['Trance', 'Former'],\n",
       " ['Geoff', 'Fusion'],\n",
       " ['Dolly', 'Too']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1065aa37-b92b-44ce-8a72-68491885c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI can calculate the factorial of 7 by multiplying all numbers from 1 to 7.\n",
      "Action: Python_REPL\n",
      "Action Input: import math\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI imported the math module to use the factorial function.\n",
      "Action: Python_REPL\n",
      "Action Input: math.factorial(7)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe factorial of 7 is 5040.\n",
      "Final Answer: 5040\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me what the factorial of 7 is', 'output': '5040'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent(\"tell me what the factorial of 7 is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14265718-a24a-4040-b127-296143940c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can be used for debugging\n",
    "# import langchain\n",
    "# langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70e3bf-89d6-401e-879a-a775d4cf269c",
   "metadata": {},
   "source": [
    "#### Creating a custom tool with `tool` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28df0a95-36c5-4085-92c6-8a765cc579e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool\n",
    "def time(text: str) -> str:\n",
    "    \"\"\"Returns todays date, use this for any \\\n",
    "    questions related to knowing todays date. \\\n",
    "    The input should always be an empty string, \\\n",
    "    and this function will always return todays \\\n",
    "    date - any date mathematics should occur \\\n",
    "    outside this function.\"\"\"\n",
    "    return str(date.today())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "414eb188-2683-40fd-9984-2a85b8b09d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I can use the time tool to find out today's date.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"time\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "``` \n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m2024-04-10\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mCould not parse LLM output: Final Answer\u001b[0m\n",
      "Observation: Invalid or incomplete response\n",
      "Thought:\u001b[32;1m\u001b[1;3mI will correct the previous action and try again to get today's date.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"time\",\n",
      "  \"action_input\": \"\"\n",
      "}\n",
      "``` \n",
      "\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3m2024-04-10\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: Today's date is April 10, 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the date today?',\n",
       " 'output': \"Today's date is April 10, 2024.\"}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = initialize_agent(tools + [time], \n",
    "                         llm, \n",
    "                         agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, \n",
    "                         verbose = True, \n",
    "                         handle_parsing_errors=True)\n",
    "agent(\"What is the date today?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
